# fetch_product_info_v3.py  (PUBLIC + MULTI-THREAD, STABLE)
# âœ… ä¿æŒä¸å˜ï¼š
# - camper_fetch_product_info(product_urls_file: Optional[str] = None, login_wait_seconds: int = LOGIN_WAIT_SECONDS)
# - URL list è¯»å–é€»è¾‘
# - process_product_url_with_driver é¡µé¢è·å–/è§£æé€»è¾‘ï¼ˆä»ç„¶ç”¨ driver.get + __NEXT_DATA__ï¼‰
# âœ… åªæ”¹åŠ¨ï¼š
# - Chrome/driver é€»è¾‘ï¼šæ¯çº¿ç¨‹ä¸€ä¸ª driverï¼ˆthread-localï¼‰ï¼Œçº¿ç¨‹å†…å¤ç”¨ï¼Œç»“æŸç»Ÿä¸€ quit
# - å¤šçº¿ç¨‹è°ƒåº¦ï¼šThreadPoolExecutor

import os
import re
import time
import json
import threading
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed

from bs4 import BeautifulSoup
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

from config import CAMPER, SIZE_RANGE_CONFIG
from common_taobao.ingest.txt_writer import format_txt
from common_taobao.core.category_utils import infer_style_category
from common_taobao.core.selenium_utils import get_driver

# =========================
# Config
# =========================
HOME_URL = "https://www.camper.com/en_GB"
PRODUCT_URLS_FILE = CAMPER["LINKS_FILE"]
SAVE_PATH = CAMPER["TXT_DIR"]

MAX_WORKERS = 4  # å»ºè®® 3~5
LOGIN_WAIT_SECONDS = 30  # å‚æ•°å…¼å®¹ä¿ç•™ï¼ˆpublic ç‰ˆä¸ç™»å½•ï¼‰

DEBUG_ENABLED = False
DEBUG_DIR = str(Path(SAVE_PATH).resolve().parent / "debug_camper")
Path(DEBUG_DIR).mkdir(parents=True, exist_ok=True)

os.makedirs(SAVE_PATH, exist_ok=True)

# =========================
# Utils
# =========================
def _safe_float(v) -> float:
    try:
        if v is None:
            return 0.0
        return float(v)
    except Exception:
        return 0.0


def infer_gender_from_url(url: str) -> str:
    u = url.lower()
    if "/women/" in u:
        return "å¥³æ¬¾"
    if "/men/" in u:
        return "ç”·æ¬¾"
    if "/kids/" in u or "/children/" in u:
        return "ç«¥æ¬¾"
    return "æœªçŸ¥"


def is_driver_connection_error(e: Exception) -> bool:
    msg = str(e)
    return (
        "WinError 10061" in msg
        or "Max retries exceeded" in msg
        or "NewConnectionError" in msg
        or "Failed to establish a new connection" in msg
        or ("localhost" in msg and "/session/" in msg)
    )


def dump_debug_page(driver, name: str):
    if not DEBUG_ENABLED:
        return

    safe = re.sub(r"[^\w\-\.]+", "_", name)[:120]
    d = Path(DEBUG_DIR) / safe
    d.mkdir(parents=True, exist_ok=True)

    with open(d / "page.html", "w", encoding="utf-8") as f:
        f.write(driver.page_source)

    soup = BeautifulSoup(driver.page_source, "html.parser")
    tag = soup.find("script", {"id": "__NEXT_DATA__"})
    if tag and tag.string:
        try:
            j = json.loads(tag.string)
            with open(d / "next_data.json", "w", encoding="utf-8") as f:
                json.dump(j, f, indent=2, ensure_ascii=False)
        except Exception as ex:
            with open(d / "next_data_error.txt", "w", encoding="utf-8") as f:
                f.write(str(ex))

    try:
        with open(d / "cookies.json", "w", encoding="utf-8") as f:
            json.dump(driver.get_cookies(), f, indent=2, ensure_ascii=False)
    except Exception:
        pass


def pick_prices_from_product_sheet(product_sheet: dict) -> Tuple[float, float, str]:
    """
    public æ¨¡å¼ä¸‹é€šå¸¸æ²¡æœ‰ voucherPricesï¼Œä¼šè‡ªåŠ¨è½åˆ° public/no_discount
    ï¼ˆä¿æŒä½ åŸæ¥çš„ä»·æ ¼é€»è¾‘ï¼Œä¸åŠ¨ï¼‰
    """
    prices = product_sheet.get("prices") or {}

    def pick_from_voucher_dict(voucher_prices: dict) -> Optional[Tuple[float, float, str]]:
        best = None
        if not isinstance(voucher_prices, dict):
            return None
        for key, vp in voucher_prices.items():
            if not isinstance(vp, dict):
                continue
            v_cur = _safe_float(vp.get("current"))
            v_prev = _safe_float(vp.get("previous"))
            if v_cur > 0 and v_prev > 0 and v_cur < v_prev:
                cand = (v_prev, v_cur, f"voucher:{key}")
                if best is None or (cand[0] - cand[1]) > (best[0] - best[1]):
                    best = cand
        return best

    top = pick_from_voucher_dict(prices.get("voucherPrices") or {})
    if top:
        return top[0], top[1], top[2]

    sizes = product_sheet.get("sizes") or []
    best = None
    for s in sizes:
        if not isinstance(s, dict):
            continue
        cand = pick_from_voucher_dict(s.get("voucherPrices") or {})
        if cand:
            if best is None or (cand[0] - cand[1]) > (best[0] - best[1]):
                best = cand
    if best:
        return best[0], best[1], best[2] + "__from_size"

    cur = _safe_float(prices.get("current"))
    prev = _safe_float(prices.get("previous"))
    if cur > 0 and prev > 0 and cur < prev:
        return prev, cur, "public"

    if cur > 0:
        return cur, cur, "no_discount"

    return 0.0, 0.0, "no_price"


# =========================
# âœ… STABLE CHROME: thread-local driver reuse (å…³é”®ä¿®å¤ç‚¹)
# =========================
drivers_lock = threading.Lock()
_all_drivers = set()
thread_local = threading.local()

def get_thread_driver():
    """
    æ¯ä¸ªçº¿ç¨‹åªåˆ›å»ºä¸€æ¬¡ driverï¼Œå¹¶åœ¨ä»»åŠ¡ç»“æŸç»Ÿä¸€ quit
    è¿™å°±æ˜¯ä½ â€œåŸå§‹ç‰ˆ chrome å¾ˆå¥½â€çš„æ ¸å¿ƒæœºåˆ¶
    """
    if not hasattr(thread_local, "driver") or thread_local.driver is None:
        d = get_driver(name="camper_v3_public_mt", headless=True)
        thread_local.driver = d
        with drivers_lock:
            _all_drivers.add(d)
    return thread_local.driver

def reset_thread_driver():
    """
    çº¿ç¨‹å†… driver æ‰çº¿ï¼ˆ10061ï¼‰æ—¶ï¼Œé‡å»ºä¸€æ¬¡
    """
    try:
        if hasattr(thread_local, "driver") and thread_local.driver is not None:
            try:
                thread_local.driver.quit()
            except Exception:
                pass
    finally:
        thread_local.driver = None

def shutdown_all_drivers():
    with drivers_lock:
        for d in list(_all_drivers):
            try:
                d.quit()
            except Exception:
                pass
        _all_drivers.clear()


# =========================
# Core: parse one url with existing driver  (ä¿æŒä¸å˜è°ƒç”¨æ–¹å¼)
# =========================
def process_product_url_with_driver(driver, product_url: str):
    print(f"\nğŸ” æ­£åœ¨è®¿é—®: {product_url}")
    driver.get(product_url)
    WebDriverWait(driver, 25).until(EC.presence_of_element_located((By.CSS_SELECTOR, "body")))
    time.sleep(1.2)

    dump_debug_page(driver, "PRE__" + product_url[-80:])

    soup = BeautifulSoup(driver.page_source, "html.parser")

    title_tag = soup.find("title")
    product_title = (
        re.sub(r"\s*[-â€“â€”].*", "", title_tag.text.strip())
        if title_tag and title_tag.text
        else "Unknown Title"
    )

    script_tag = soup.find("script", {"id": "__NEXT_DATA__", "type": "application/json"})
    if not script_tag or not script_tag.string:
        dump_debug_page(driver, "NO_NEXT_DATA")
        raise RuntimeError("æœªæ‰¾åˆ° __NEXT_DATA__")

    json_data = json.loads(script_tag.string)
    product_sheet = (
        json_data.get("props", {})
        .get("pageProps", {})
        .get("productSheet")
    )
    if not product_sheet:
        dump_debug_page(driver, "NO_PRODUCT_SHEET")
        raise RuntimeError("æœªæ‰¾åˆ° productSheet")

    data = product_sheet
    product_code = data.get("code", "Unknown_Code")
    dump_debug_page(driver, product_code)

    description = data.get("description", "")
    original_price, discount_price, price_src = pick_prices_from_product_sheet(data)

    color_data = data.get("color", "")
    color = color_data.get("name", "") if isinstance(color_data, dict) else str(color_data)

    features_raw = data.get("features") or []
    feature_texts = []
    for f in features_raw:
        value_html = (f.get("value") or "")
        clean_text = BeautifulSoup(value_html, "html.parser").get_text(strip=True)
        if clean_text:
            feature_texts.append(clean_text)
    feature_str = " | ".join(feature_texts) if feature_texts else "No Data"

    upper_material = "No Data"
    for feature in features_raw:
        name = (feature.get("name") or "").lower()
        if "upper" in name:
            raw_html = feature.get("value") or ""
            upper_material = BeautifulSoup(raw_html, "html.parser").get_text(strip=True)
            break

    size_map = {}
    size_detail = {}
    for s in data.get("sizes", []):
        value = (s.get("value", "") or "").strip()
        available = bool(s.get("available", False))
        quantity = s.get("quantity", 0)
        ean = s.get("ean", "")
        size_map[value] = "æœ‰è´§" if available else "æ— è´§"
        size_detail[value] = {"stock_count": quantity, "ean": ean}

    gender = infer_gender_from_url(product_url)

    standard_sizes = SIZE_RANGE_CONFIG.get("camper", {}).get(gender, [])
    if standard_sizes:
        missing = [x for x in standard_sizes if x not in size_detail]
        for x in missing:
            size_map[x] = "æ— è´§"
            size_detail[x] = {"stock_count": 0, "ean": ""}
        if missing:
            print(f"âš ï¸ {product_code} è¡¥å…¨å°ºç : {', '.join(missing)}")

    style_category = infer_style_category(description)

    info = {
        "Product Code": product_code,
        "Product Name": product_title,
        "Product Description": description,
        "Product Gender": gender,
        "Product Color": color,
        "Product Price": str(original_price),
        "Adjusted Price": str(discount_price),
        "Product Material": upper_material,
        "Style Category": style_category,
        "Feature": feature_str,
        "SizeMap": size_map,
        "SizeDetail": size_detail,
        "Source URL": product_url,
        "Price Source": price_src,
    }

    out_path = Path(SAVE_PATH) / f"{product_code}.txt"
    format_txt(info, out_path, brand="camper")
    print(f"âœ… å®Œæˆ TXT: {out_path.name}  (src={price_src}, P={original_price}, D={discount_price})")


# =========================
# v3 Entry: PUBLIC multi-thread (no login)  âœ… ç­¾åä¸å˜
# =========================
def camper_fetch_product_info(product_urls_file: Optional[str] = None,
                              login_wait_seconds: int = LOGIN_WAIT_SECONDS):
    if product_urls_file is None:
        product_urls_file = PRODUCT_URLS_FILE

    print(f"ğŸ“„ ä½¿ç”¨é“¾æ¥æ–‡ä»¶: {product_urls_file}")
    with open(product_urls_file, "r", encoding="utf-8") as f:
        urls = [line.strip() for line in f if line.strip()]

    print(f"ğŸš€ å¼€å§‹å¤šçº¿ç¨‹æŠ“å–ï¼š{len(urls)} æ¡ï¼ŒMAX_WORKERS={MAX_WORKERS}")

    failed = []

    def worker(url: str):
        # æ¯ä¸ªçº¿ç¨‹å¤ç”¨è‡ªå·±çš„ driverï¼›æ‰çº¿å°±é‡å»ºä¸€æ¬¡å†è¯•
        for attempt in range(2):
            driver = get_thread_driver()
            try:
                process_product_url_with_driver(driver, url)
                return True, url, ""
            except Exception as e:
                if is_driver_connection_error(e) and attempt == 0:
                    print(f"âš ï¸ driver æ‰çº¿(10061)ï¼Œé‡å»ºåé‡è¯•: {url}")
                    reset_thread_driver()
                    time.sleep(0.5)
                    continue
                return False, url, str(e)
        return False, url, "unknown"

    try:
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:
            fut_map = {ex.submit(worker, u): u for u in urls}
            for fut in as_completed(fut_map):
                ok, url, err = fut.result()
                if not ok:
                    print(f"âŒ å¤±è´¥: {url} - {err}")
                    failed.append(url)
    finally:
        shutdown_all_drivers()

    if failed:
        fail_path = Path(SAVE_PATH).resolve().parent / "failed_urls_public_mt.txt"
        with open(fail_path, "w", encoding="utf-8") as f:
            f.write("\n".join(failed))
        print(f"âš ï¸ å¤±è´¥é“¾æ¥å·²è¾“å‡º: {fail_path}")

    print("âœ… å…¨éƒ¨å®Œæˆ")


if __name__ == "__main__":
    camper_fetch_product_info()
