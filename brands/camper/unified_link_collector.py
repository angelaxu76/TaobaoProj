# TODO: å®ç° camper å•†å“é“¾æ¥æŠ“å–é€»è¾‘
import requests
from bs4 import BeautifulSoup
import time
from pathlib import Path
from config import CAMPER  # âœ… æ ¹æ®å“ç‰Œåˆ‡æ¢

# ========= å‚æ•°é…ç½® =========
LINKS_FILE = CAMPER["LINKS_FILE"]
TOTAL_PAGES = 15
WAIT = 1
LINK_PREFIX = "https://www.camper.com"

# âœ… å¤šä¸ªå…¥å£å¯æ·»åŠ åˆ°è¿™é‡Œ
BASE_URLS = [
    "https://www.camper.com/en_GB/women/shoes?sort=default&page={}",  # å¥³å£«è¿åŠ¨é‹
    "https://www.camper.com/en_GB/men/shoes?sort=default&page={}",   # ç”·å£«è¿åŠ¨é‹
    "https://www.camper.com/en_GB/kids/shoes?sort=default&page={}",  # å„¿ç«¥é‹ï¼ˆç¤ºä¾‹ï¼‰
    "https://www.camper.com/en_GB/women/shoes/all_shoes_lab_women?filter.collection=allabw&sort=default&page={}", # LABå¥³é‹ï¼ˆç¤ºä¾‹ï¼‰
    "https://www.camper.com/en_GB/men/shoes/all_shoes_lab_men?filter.collection=allabw&sort=default&page={}"# LABç”·é‹ï¼ˆç¤ºä¾‹ï¼‰
]

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
}

# ========= é“¾æ¥æå–å‡½æ•° =========
def get_links_from_page(url):
    try:
        resp = requests.get(url, headers=HEADERS, timeout=10)
        resp.raise_for_status()
    except Exception as e:
        print(f"âŒ è¯·æ±‚å¤±è´¥: {url}ï¼Œé”™è¯¯: {e}")
        return []

    soup = BeautifulSoup(resp.text, "html.parser")
    product_divs = soup.find_all("div", class_="ant-col grid-item overflow-hidden ant-col-xs-12 ant-col-md-8 ant-col-lg-6 ant-col-xl-6 ant-col-xxl-6")

    links = []
    for div in product_divs:
        a_tag = div.find("a", class_="block")
        if a_tag and a_tag.get("href"):
            href = a_tag["href"]
            if href.startswith("/"):
                href = LINK_PREFIX + href
            links.append(href)

    return links

# ========= ä¸»ç¨‹åº =========
def main():
    all_links = set()

    for base_url in BASE_URLS:
        for page in range(1, TOTAL_PAGES + 1):
            url = base_url.format(page)
            print(f"ğŸŒ æŠ“å–: {url}")
            links = get_links_from_page(url)
            print(f"âœ… ç¬¬ {page} é¡µ: {len(links)} æ¡é“¾æ¥")
            all_links.update(links)
            time.sleep(WAIT)

    LINKS_FILE.parent.mkdir(parents=True, exist_ok=True)
    with open(LINKS_FILE, "w", encoding="utf-8") as f:
        for link in sorted(all_links):
            f.write(link + "\n")

    print(f"\nğŸ‰ å…±æŠ“å–é“¾æ¥: {len(all_links)}ï¼Œå·²ä¿å­˜åˆ°: {LINKS_FILE}")

if __name__ == "__main__":
    main()
