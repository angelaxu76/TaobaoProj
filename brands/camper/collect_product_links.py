# -*- coding: utf-8 -*-
"""
Camper å•†å“é“¾æ¥æŠ“å–ï¼ˆå¢å¼ºç‰ˆï¼‰
å˜åŒ–ç‚¹ï¼š
1ï¼‰è‡ªåŠ¨ç¿»é¡µï¼šä»ç¬¬ 1 é¡µå¼€å§‹ï¼Œåªè¦èƒ½æŠ“åˆ°å•†å“å°±ç»§ç»­ï¼›è¿ç»­ 3 é¡µä¸ºç©ºå°±åˆ‡åˆ°ä¸‹ä¸€ä¸ªç±»åˆ«ã€‚
2ï¼‰æ›´å¥å£®çš„é€‰æ‹©å™¨ï¼šå…¼å®¹å¤šä¸ªå¡ç‰‡ç»“æ„ï¼Œå°½é‡ä» <a> æå–é“¾æ¥ã€‚
3ï¼‰ä¿®å¤ BASE_URLS å°‘é€—å·å¯¼è‡´çš„ format IndexErrorã€‚
4ï¼‰æŠ“åˆ°çš„é“¾æ¥å»é‡ã€æ’åºåè½ç›˜ã€‚
"""
import requests
from bs4 import BeautifulSoup
import time
from pathlib import Path
from config import CAMPER  # âœ… æ ¹æ®å“ç‰Œåˆ‡æ¢
import sys

sys.stdout.reconfigure(encoding='utf-8')

# ========= å‚æ•°é…ç½® =========
LINKS_FILE = CAMPER["LINKS_FILE"]
WAIT = 1.0                      # æ¯é¡µæŠ“å–é—´éš”ï¼ˆç§’ï¼‰
MAX_EMPTY_PAGES = 3             # è¿ç»­å¤šå°‘é¡µæ— æ•°æ®å°±æ¢ç±»ç›®
LINK_PREFIX = "https://www.camper.com"

# âœ… ç±»ç›®å…¥å£ï¼ˆæ³¨æ„æ¯ä¸€è¡Œæœ«å°¾éƒ½æœ‰é€—å·ï¼ï¼‰
BASE_URLS = [
    # æ±‡æ€»é¡µ
    "https://www.camper.com/en_GB/women/shoes?sort=default&page={}",
    "https://www.camper.com/en_GB/men/shoes?sort=default&page={}",
    "https://www.camper.com/en_GB/kids/shoes?sort=default&page={}",

    # LAB/ALL ç³»åˆ—
    "https://www.camper.com/en_GB/women/shoes/all_shoes_lab_women?filter.collection=allabw&sort=default&page={}",
    "https://www.camper.com/en_GB/men/shoes/all_shoes_lab_men?filter.collection=allabm&sort=default&page={}",

    # æ–°å“
    "https://www.camper.com/en_GB/men/shoes/new_collection?filter.collection=neco&sort=default&page={}",
    "https://www.camper.com/en_GB/women/shoes/new_collection?filter.collection=neco&sort=default&page={}",

    # å¥³æ¬¾ç»†åˆ†
    "https://www.camper.com/en_GB/women/shoes/ballerinas?filter.collection=allabw&sort=default&page={}",
    "https://www.camper.com/en_GB/women/shoes/sneakers?filter.collection=allabw&sort=default&page={}",
    "https://www.camper.com/en_GB/women/shoes/formal_shoes?filter.collection=allabw&sort=default&page={}",
    "https://www.camper.com/en_GB/women/shoes/casual?filter.collection=allabw&sort=default&page={}",
    "https://www.camper.com/en_GB/women/shoes/ankle_boots?filter.collection=allabw&sort=default&page={}",
    "https://www.camper.com/en_GB/women/shoes/boots?filter.collection=allabw&sort=default&page={}",

    # ç”·æ¬¾ç»†åˆ†
    "https://www.camper.com/en_GB/men/shoes/sneakers?filter.collection=allabm&sort=default&page={}",
    "https://www.camper.com/en_GB/men/shoes/formal_shoes?filter.collection=allabm&sort=default&page={}",
    "https://www.camper.com/en_GB/men/shoes/casual?filter.collection=allabm&sort=default&page={}",
    "https://www.camper.com/en_GB/men/shoes/ankle_boots?filter.collection=allabm&sort=default&page={}",
]

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
}

# ========= å¸¦é‡è¯•çš„è¯·æ±‚ =========
def fetch_with_retry(url, retries=3, timeout=20):
    for attempt in range(1, retries + 1):
        try:
            resp = requests.get(url, headers=HEADERS, timeout=timeout)
            resp.raise_for_status()
            return resp.text
        except Exception as e:
            print(f"âŒ ç¬¬ {attempt} æ¬¡è¯·æ±‚å¤±è´¥: {url}ï¼Œé”™è¯¯: {e}")
            if attempt < retries:
                time.sleep(2 * attempt)
    return None

# ========= è§£æé¡µé¢æå–é“¾æ¥ =========
def get_links_from_page(url):
    html = fetch_with_retry(url)
    if not html:
        return []

    soup = BeautifulSoup(html, "html.parser")

    links = set()

    # é€‰æ‹©å™¨ 1ï¼šå…¼å®¹å½“å‰ç½‘æ ¼å¡ç‰‡ï¼ˆä½ ç°æœ‰è„šæœ¬ä½¿ç”¨çš„ classï¼‰
    for a_tag in soup.select("div.grid-item a.block[href]"):
        if a_tag and a_tag.get("href"):
            href = a_tag["href"].strip()
            if href.startswith("/"):
                href = LINK_PREFIX + href
            links.add(href)

    # é€‰æ‹©å™¨ 2ï¼šå…œåº•ï¼ŒæŠ“å–æ‰€æœ‰å•†å“å¡ç‰‡ä¸Šçš„ <a>ï¼ˆé¿å… UI ç»†å¾®å˜åŠ¨æ—¶ä¸¢å¤±ï¼‰
    if not links:
        for a in soup.select("a[href*='//www.camper.com/en_']"):
            href = (a.get("href") or "").strip()
            if not href:
                continue
            if ("/women/shoes/" in href or "/men/shoes/" in href or "/kids/shoes/" in href):
                # è¯¦æƒ…é¡µå±‚çº§æ›´æ·±ï¼Œç²—ç•¥è¿‡æ»¤æ‰åˆ—è¡¨/ç­›é€‰é¡µ
                if href.count("/") >= 7:
                    if href.startswith("/"):
                        href = LINK_PREFIX + href
                    links.add(href)

    return list(links)

# ========= ä¸»æµç¨‹ï¼šè‡ªåŠ¨ç¿»é¡µï¼Œè¿ç»­ç©ºé¡µé˜ˆå€¼ååˆ‡ç±»ç›® =========
def camper_get_links():
    all_links = set()

    for base_url in BASE_URLS:
        empty_pages = 0
        page = 1
        print(f"\nâ–¶ï¸ å…¥å£ï¼š{base_url}")
        while True:
            url = base_url.format(page)
            print(f"ğŸŒ æŠ“å–: {url}")
            links = get_links_from_page(url)

            if links:
                print(f"âœ… ç¬¬ {page} é¡µ: {len(links)} æ¡é“¾æ¥")
                before = len(all_links)
                all_links.update(links)
                added = len(all_links) - before
                if added > 0:
                    print(f"   â†³ æ–°å¢ {added} æ¡ï¼ˆå»é‡åç´¯è®¡ {len(all_links)}ï¼‰")
                empty_pages = 0
                page += 1
            else:
                print(f"âš ï¸ ç¬¬ {page} é¡µæ— é“¾æ¥æˆ–æŠ“å–å¤±è´¥")
                empty_pages += 1
                page += 1
                if empty_pages >= MAX_EMPTY_PAGES:
                    print(f"â¹ï¸ è¿ç»­ {MAX_EMPTY_PAGES} é¡µä¸ºç©ºï¼Œåˆ‡æ¢ä¸‹ä¸€ä¸ªå…¥å£")
                    break
            time.sleep(WAIT)

    # è¾“å‡º
    LINKS_FILE.parent.mkdir(parents=True, exist_ok=True)
    with open(LINKS_FILE, "w", encoding="utf-8") as f:
        for link in sorted(all_links):
            f.write(link + "\n")

    print(f"\nğŸ‰ å…±æŠ“å–é“¾æ¥: {len(all_links)}ï¼Œå·²ä¿å­˜åˆ°: {LINKS_FILE}")

if __name__ == "__main__":
    camper_get_links()
