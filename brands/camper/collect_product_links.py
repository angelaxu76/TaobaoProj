# -*- coding: utf-8 -*-
"""
Camper å•†å“é“¾æ¥æŠ“å–ï¼ˆå¢å¼ºç‰ˆ - ä¿®å¤æ— é™ç¿»é¡µï¼‰
ä¿®å¤ç‚¹ï¼ˆé’ˆå¯¹ä½ ç°åœ¨â€œæ¯é¡µå›ºå®š 8 æ¡ï¼Œæ°¸è¿œä¸ç©ºï¼Œåœä¸ä¸‹æ¥â€çš„é—®é¢˜ï¼‰ï¼š
1ï¼‰æ–°å¢åœæ­¢æ¡ä»¶ï¼šè¿ç»­ NO_NEW_LIMIT é¡µâ€œæ— æ–°å¢é“¾æ¥â€ -> åˆ‡æ¢ä¸‹ä¸€ä¸ªå…¥å£
2ï¼‰æ–°å¢åœæ­¢æ¡ä»¶ï¼šé¡µé¢ç­¾åé‡å¤ PAGE_REPEAT_LIMIT æ¬¡ -> åˆ‡æ¢ä¸‹ä¸€ä¸ªå…¥å£
3ï¼‰ä»ä¿ç•™åŸé€»è¾‘ï¼šè¿ç»­ MAX_EMPTY_PAGES é¡µä¸ºç©º -> åˆ‡æ¢ä¸‹ä¸€ä¸ªå…¥å£
4ï¼‰æŠ“åˆ°çš„é“¾æ¥å…¨å±€å»é‡åè½ç›˜
"""
import requests
from bs4 import BeautifulSoup
import time
from pathlib import Path
from config import CAMPER  # âœ… æ ¹æ®å“ç‰Œåˆ‡æ¢
import sys
import re

sys.stdout.reconfigure(encoding='utf-8')

# ========= å‚æ•°é…ç½® =========
LINKS_FILE = CAMPER["LINKS_FILE"]
WAIT = 1.0                      # æ¯é¡µæŠ“å–é—´éš”ï¼ˆç§’ï¼‰
MAX_EMPTY_PAGES = 3             # è¿ç»­å¤šå°‘é¡µæ— æ•°æ®å°±æ¢ç±»ç›®ï¼ˆä¿ç•™åŸé€»è¾‘ï¼‰
LINK_PREFIX = "https://www.camper.com"

# âœ… æ–°å¢ï¼šè¿ç»­æ— æ–°å¢é¡µæ•°é™åˆ¶ï¼ˆè§£å†³â€œæ¯é¡µéƒ½æœ‰8æ¡ä½†éƒ½æ˜¯é‡å¤/æ¨èä½â€ï¼‰
NO_NEW_LIMIT = 2                # è¿ç»­ 2 é¡µæ— æ–°å¢ -> åœæ­¢è¯¥å…¥å£ï¼ˆä½ æƒ³æ›´ä¿å®ˆå¯æ”¹ 3ï¼‰

# âœ… æ–°å¢ï¼šé¡µé¢å†…å®¹é‡å¤é™åˆ¶ï¼ˆåŒä¸€æ‰¹é“¾æ¥åå¤å‡ºç°ï¼‰
PAGE_REPEAT_LIMIT = 1           # é¡µé¢ç­¾åé‡å¤ 1 æ¬¡å³è®¤ä¸ºå¡æ­»ï¼ˆå¯æ”¹ 2 æ›´å®½æ¾ï¼‰

# âœ… ç±»ç›®å…¥å£ï¼ˆæ³¨æ„æ¯ä¸€è¡Œæœ«å°¾éƒ½æœ‰é€—å·ï¼ï¼‰
BASE_URLS = [
    # æ±‡æ€»é¡µ
    "https://www.camper.com/en_GB/men/shoes?sort=default&page={}",
    "https://www.camper.com/en_GB/women/shoes?sort=default&page={}",
    "https://www.camper.com/en_GB/kids/shoes?sort=default&page={}",

    "https://www.camper.com/en_GB/women/shoes/casual?filter.typology=_CST_T01&sort=default&page={}",
    "https://www.camper.com/en_GB/women/shoes?sort=default&page={}",
    "https://www.camper.com/en_GB/women/shoes/ballerinas?filter.typology=_CST_T10&sort=default&page={}",
    "https://www.camper.com/en_GB/women/shoes/boots?filter.typology=T01&sort=default&page={}",
    "https://www.camper.com/en_GB/women/shoes/loafers?filter.typology=_CST_T17&sort=default&page={}",
    "https://www.camper.com/en_GB/women/shoes/sneakers?filter.typology=_CST_T04&sort=default&page={}",
    "https://www.camper.com/en_GB/women/shoes/heels?filter.typology=_CST_T07&sort=default&page={}",
    "https://www.camper.com/en_GB/women/shoes/casual?filter.typology=_CST_T01&sort=default&page={}",
    "https://www.camper.com/en_GB/women/shoes/formal_shoes?filter.typology=_CST_T05&sort=default&page={}",
    "https://www.camperlab.com/en_GB/women/shoes/all_shoes_lab_women?filter.collection=allabw&sort=default&page={}",
    "https://www.camper.com/en_GB/men/shoes/fall_winter?filter.collection=fw&sort=default&page={}",
    "https://www.camperlab.com/en_GB/men/shoes/all_shoes_lab_men?filter.collection=allabm&sort=default&page={}",
    "https://www.camper.com/en_GB/men/shoes/fall_winter?filter.collection=fw&sort=default&page={}",

    "https://www.camper.com/en_GB/men/shoes/ankle_boots?filter.typology=_CST_T09&sort=default&page={}",
    "https://www.camper.com/en_GB/men/shoes/sneakers?filter.typology=_CST_T04&sort=default&page={}",
    "https://www.camper.com/en_GB/men/shoes/formal_shoes?filter.typology=_CST_T05&sort=default&page={}",
    "https://www.camper.com/en_GB/men/shoes/casual?filter.typology=_CST_T01&sort=default&page={}",
    "https://www.camper.com/en_GB/men/shoes/loafers?page={}",

    # LAB/ALL ç³»åˆ—
    "https://www.camper.com/en_GB/women/shoes/all_shoes_lab_women?filter.collection=allabw&sort=default&page={}",
    "https://www.camper.com/en_GB/men/shoes/all_shoes_lab_men?filter.collection=allabm&sort=default&page={}",

    # å¥³æ¬¾ç»†åˆ†
    "https://www.camper.com/en_GB/women/shoes/ballerinas?filter.collection=allabw&sort=default&page={}",
    "https://www.camper.com/en_GB/women/shoes/sneakers?filter.collection=allabw&sort=default&page={}",
    "https://www.camper.com/en_GB/women/shoes/formal_shoes?filter.collection=allabw&sort=default&page={}",
    "https://www.camper.com/en_GB/women/shoes/casual?filter.collection=allabw&sort=default&page={}",
    "https://www.camper.com/en_GB/women/shoes/ankle_boots?filter.collection=allabw&sort=default&page={}",
    "https://www.camper.com/en_GB/women/shoes/boots?filter.collection=allabw&sort=default&page={}",
    "https://www.camper.com/en_GB/women/shoes/heels?filter.collection=allabw&sort=default&page={}",
    "https://www.camper.com/en_GB/women/shoes/loafers?filter.collection=allabw&sort=default&page={}",
    "https://www.camper.com/en_GB/women/shoes/fall_winter?filter.collection=allabw&sort=default&page={}",

    # ç”·æ¬¾ç»†åˆ†
    "https://www.camper.com/en_GB/men/shoes/sneakers?filter.collection=allabm&sort=default&page={}",
    "https://www.camper.com/en_GB/men/shoes/formal_shoes?filter.collection=allabm&sort=default&page={}",
    "https://www.camper.com/en_GB/men/shoes/casual?filter.collection=allabm&sort=default&page={}",
    "https://www.camper.com/en_GB/men/shoes/ankle_boots?filter.collection=allabm&sort=default&page={}",
    "https://www.camper.com/en_GB/men/shoes/loafers?filter.collection=allabw&sort=default&page={}",
    "https://www.camper.com/en_GB/men/shoes/fall_winter?filter.collection=allabw&sort=default&page={}",
]

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
}


def normalize_link(href: str) -> str:
    """
    æŠŠå•†å“é“¾æ¥è§„èŒƒåŒ–ï¼š
    - è¡¥å…¨åŸŸå
    - å»æ‰ ? åé¢çš„æ‰€æœ‰å‚æ•°ï¼ˆåŒ…æ‹¬ sizeã€utm ç­‰ï¼‰
    - å»æ‰æœ«å°¾å¤šä½™çš„ /
    """
    href = (href or "").strip()
    if not href:
        return ""

    if href.startswith("/"):
        href = LINK_PREFIX + href

    href = href.split("?", 1)[0]
    href = href.rstrip("/")
    return href


def parse_pagination_spec(url: str):
    """
    è§£æ URL ä¸­çš„ page å ä½ç¬¦:
    - page={}        -> (base_url, start=1, end=None)   è‡ªåŠ¨æ¨¡å¼
    - page={20}      -> (base_url, start=1, end=20)     1..20
    - page={3-12}    -> (base_url, start=3, end=12)     3..12
    """
    m = re.search(r"page=\{(\d+)(?:-(\d+))?\}", url)
    if m:
        start = int(m.group(1))
        end = int(m.group(2)) if m.group(2) else int(m.group(1))
        base_url = re.sub(r"\{(\d+)(?:-(\d+))?\}", "{}", url)
        return base_url, start, end
    else:
        return url, 1, None


def fetch_with_retry(url, retries=3, timeout=20):
    for attempt in range(1, retries + 1):
        try:
            resp = requests.get(url, headers=HEADERS, timeout=timeout)
            resp.raise_for_status()
            return resp.text
        except Exception as e:
            print(f"âŒ ç¬¬ {attempt} æ¬¡è¯·æ±‚å¤±è´¥: {url}ï¼Œé”™è¯¯: {e}")
            if attempt < retries:
                time.sleep(2 * attempt)
    return None


def get_links_from_page(url):
    html = fetch_with_retry(url)
    if not html:
        return []

    soup = BeautifulSoup(html, "html.parser")
    links = set()

    # é€‰æ‹©å™¨ 1
    for a_tag in soup.select("div.grid-item a.block[href]"):
        href = normalize_link(a_tag.get("href"))
        if href:
            links.add(href)

    # é€‰æ‹©å™¨ 2
    for a in soup.select("a[href*='//www.camper.com/en_']"):
        href = normalize_link(a.get("href"))
        if not href:
            continue
        if ("/women/shoes/" in href or "/men/shoes/" in href or "/kids/shoes/" in href):
            if href.count("/") >= 7:
                links.add(href)

    return list(links)


def _page_signature(links):
    """æŠŠæœ¬é¡µé“¾æ¥åšæˆç¨³å®šç­¾åï¼Œç”¨äºæ£€æµ‹é‡å¤é¡µé¢å†…å®¹"""
    uniq = sorted(set([normalize_link(x) for x in links if x]))
    return "|".join(uniq)


def camper_get_links():
    all_links = set()

    for spec in BASE_URLS:
        base_url, start, end = parse_pagination_spec(spec)

        empty_pages = 0
        consecutive_no_new = 0
        page_repeat_count = 0

        seen_page_sigs = set()

        page = start
        print(f"\nâ–¶ï¸ å…¥å£ï¼š{base_url}ï¼ˆé¡µæ•°èŒƒå›´: {start} â†’ {end or 'auto'}ï¼‰")

        while True:
            url = base_url.format(page)
            print(f"ğŸŒ æŠ“å–: {url}")
            links = get_links_from_page(url)

            # ---- æ–°å¢ï¼šé¡µé¢ç­¾åé‡å¤æ£€æµ‹ï¼ˆè§£å†³ï¼šæ¯é¡µéƒ½æœ‰ 8 æ¡ä½†å…¶å®æ˜¯åŒä¸€æ‰¹ï¼‰----
            sig = _page_signature(links)
            if sig and sig in seen_page_sigs:
                page_repeat_count += 1
                print(f"âš ï¸ ç¬¬ {page} é¡µé¡µé¢å†…å®¹é‡å¤ï¼ˆé‡å¤è®¡æ•° {page_repeat_count}/{PAGE_REPEAT_LIMIT}ï¼‰")
                if page_repeat_count >= PAGE_REPEAT_LIMIT and end is None:
                    print("â¹ï¸ é¡µé¢å†…å®¹é‡å¤ï¼Œåˆ¤å®šè¿›å…¥å…œåº•/æ¨èå¾ªç¯é¡µï¼Œåˆ‡æ¢ä¸‹ä¸€ä¸ªå…¥å£")
                    break
            else:
                if sig:
                    seen_page_sigs.add(sig)
                page_repeat_count = 0

            if links:
                print(f"âœ… ç¬¬ {page} é¡µ: {len(links)} æ¡é“¾æ¥")
                before = len(all_links)
                all_links.update(links)
                added = len(all_links) - before

                if added > 0:
                    print(f"   â†³ æ–°å¢ {added} æ¡ï¼ˆå»é‡åç´¯è®¡ {len(all_links)}ï¼‰")
                    consecutive_no_new = 0
                else:
                    consecutive_no_new += 1
                    print(f"   â†³ æœ¬é¡µæ— æ–°å¢ï¼ˆè¿ç»­ {consecutive_no_new}/{NO_NEW_LIMIT}ï¼‰")
                    if consecutive_no_new >= NO_NEW_LIMIT and end is None:
                        print(f"â¹ï¸ è¿ç»­ {NO_NEW_LIMIT} é¡µæ— æ–°å¢é“¾æ¥ï¼Œåˆ‡æ¢ä¸‹ä¸€ä¸ªå…¥å£")
                        break

                empty_pages = 0
            else:
                print(f"âš ï¸ ç¬¬ {page} é¡µæ— é“¾æ¥æˆ–æŠ“å–å¤±è´¥")
                empty_pages += 1
                if empty_pages >= MAX_EMPTY_PAGES and end is None:
                    print(f"â¹ï¸ è¿ç»­ {MAX_EMPTY_PAGES} é¡µä¸ºç©ºï¼Œåˆ‡æ¢ä¸‹ä¸€ä¸ªå…¥å£")
                    break

            # è‹¥æ‰‹åŠ¨è®¾äº†é¡µæ•°ä¸Šé™ï¼Œåˆ°è¾¾å°±æ¢å…¥å£
            if end is not None and page >= end:
                print(f"â¹ï¸ è¾¾åˆ°æ‰‹åŠ¨è®¾å®šé¡µæ•° {end}ï¼Œåˆ‡æ¢ä¸‹ä¸€ä¸ªå…¥å£")
                break

            page += 1
            time.sleep(WAIT)

    # è¾“å‡º
    LINKS_FILE.parent.mkdir(parents=True, exist_ok=True)
    with open(LINKS_FILE, "w", encoding="utf-8") as f:
        for link in sorted(all_links):
            f.write(link + "\n")

    print(f"\nğŸ‰ å…±æŠ“å–é“¾æ¥: {len(all_links)}ï¼Œå·²ä¿å­˜åˆ°: {LINKS_FILE}")


if __name__ == "__main__":
    camper_get_links()
