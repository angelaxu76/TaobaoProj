# fetch_product_info_v2_2.py
import os
import re
import time
import json
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed

from bs4 import BeautifulSoup
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

from config import CAMPER, SIZE_RANGE_CONFIG
from common_taobao.ingest.txt_writer import format_txt
from common_taobao.core.category_utils import infer_style_category
from common_taobao.core.selenium_utils import get_driver, quit_all_drivers

DEBUG_ENABLED = False   # True=å¼€å¯ debugï¼ŒFalse=å…³é—­ debug
# =========================
# Config
# =========================
HOME_URL = "https://www.camper.com/en_GB"
PRODUCT_URLS_FILE = CAMPER["LINKS_FILE"]
SAVE_PATH = CAMPER["TXT_DIR"]

MAX_WORKERS = 6
DEBUG_DIR = str(Path(SAVE_PATH).resolve().parent / "debug_camper")
Path(DEBUG_DIR).mkdir(parents=True, exist_ok=True)
print("ğŸ§ª DEBUG_DIR =", DEBUG_DIR)

os.makedirs(SAVE_PATH, exist_ok=True)

# å…¨å±€ cookiesï¼ˆä¸»çº¿ç¨‹ç™»å½•åèµ‹å€¼ï¼›å­çº¿ç¨‹è¯»å–ï¼‰
LOGIN_COOKIES: list[dict] = []


# =========================
# Debug dump
# =========================
def dump_debug_page(driver, product_code: str, base_dir=DEBUG_DIR):

    if not DEBUG_ENABLED:
        return

    debug_dir = Path(base_dir) / str(product_code)
    debug_dir.mkdir(parents=True, exist_ok=True)

    # 1) HTML
    with open(debug_dir / "page.html", "w", encoding="utf-8") as f:
        f.write(driver.page_source)

    # 2) __NEXT_DATA__
    soup = BeautifulSoup(driver.page_source, "html.parser")
    next_data_tag = soup.find("script", {"id": "__NEXT_DATA__"})
    if next_data_tag and next_data_tag.string:
        try:
            next_json = json.loads(next_data_tag.string)
            with open(debug_dir / "next_data.json", "w", encoding="utf-8") as f:
                json.dump(next_json, f, indent=2, ensure_ascii=False)
        except Exception as e:
            with open(debug_dir / "next_data_error.txt", "w", encoding="utf-8") as f:
                f.write(str(e))

    # 3) Cookies
    cookies = driver.get_cookies()
    with open(debug_dir / "cookies.json", "w", encoding="utf-8") as f:
        json.dump(cookies, f, indent=2, ensure_ascii=False)

    # 4) Meta quick check
    meta_lines = []
    meta_lines.append(f"URL: {driver.current_url}")
    meta_lines.append(f"Cookies count: {len(cookies)}")
    if next_data_tag and next_data_tag.string:
        meta_lines.append("voucherPrices: " + ("FOUND" if "voucherPrices" in next_data_tag.string else "NOT FOUND"))
    page_lower = driver.page_source.lower()
    meta_lines.append("login_hint: " + ("maybe_logged_in" if ("logout" in page_lower or "my account" in page_lower) else "unknown"))

    with open(debug_dir / "meta.txt", "w", encoding="utf-8") as f:
        f.write("\n".join(meta_lines))

    print(f"ğŸ§ª Debug dump saved to: {debug_dir}")


# =========================
# Helpers
# =========================
def infer_gender_from_url(url: str) -> str:
    url = url.lower()
    if "/women/" in url:
        return "å¥³æ¬¾"
    if "/men/" in url:
        return "ç”·æ¬¾"
    if "/kids/" in url or "/children/" in url:
        return "ç«¥æ¬¾"
    return "æœªçŸ¥"


def _safe_float(v):
    try:
        if v is None:
            return 0.0
        return float(v)
    except Exception:
        return 0.0


def pick_prices_from_product_sheet(product_sheet: dict):
    prices = product_sheet.get("prices") or {}

    def pick_from_voucher_dict(voucher_prices: dict):
        best = None  # (prev, cur, key)
        if not isinstance(voucher_prices, dict):
            return None
        for key, vp in voucher_prices.items():
            if not isinstance(vp, dict):
                continue
            v_cur = _safe_float(vp.get("current"))
            v_prev = _safe_float(vp.get("previous"))
            if v_cur > 0 and v_prev > 0 and v_cur < v_prev:
                cand = (v_prev, v_cur, f"voucher:{key}")
                # é€‰æŠ˜æ‰£åŠ›åº¦æœ€å¤§çš„
                if best is None or (cand[0] - cand[1]) > (best[0] - best[1]):
                    best = cand
        return best

    # 1) é¡¶å±‚ voucherPricesï¼ˆæœ€ç†æƒ³ï¼‰
    top = pick_from_voucher_dict(prices.get("voucherPrices") or {})
    if top:
        return top[0], top[1], top[2]

    # 2) å°ºç å±‚ voucherPrices å…œåº•ï¼ˆæœ‰äº›æ¬¾æŠ˜æ‰£åªæŒ‚åœ¨ size ä¸Šï¼‰
    sizes = product_sheet.get("sizes") or []
    best = None
    for s in sizes:
        if not isinstance(s, dict):
            continue
        cand = pick_from_voucher_dict(s.get("voucherPrices") or {})
        if cand:
            if best is None or (cand[0] - cand[1]) > (best[0] - best[1]):
                best = cand
    if best:
        return best[0], best[1], best[2] + "__from_size"

    # 3) public previous/current
    cur = _safe_float(prices.get("current"))
    prev = _safe_float(prices.get("previous"))
    if cur > 0 and prev > 0 and cur < prev:
        return prev, cur, "public"

    # 4) no discountï¼ˆå…³é”®ï¼šä¸èƒ½è®© Product Price=0ï¼‰
    if cur > 0:
        return cur, cur, "no_discount"

    return 0.0, 0.0, "no_price"


def apply_cookies_to_driver(driver, cookies: list[dict]):
    """æŠŠä¸»çº¿ç¨‹ cookies æ³¨å…¥åˆ°å­çº¿ç¨‹ driver"""
    if not cookies:
        return

    driver.get(HOME_URL)
    WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CSS_SELECTOR, "body")))
    time.sleep(1.5)

    # âœ… ä¸ä¾èµ–è§£æï¼Œå…ˆæŠŠçœŸå®è®¿é—®åˆ°çš„é¡µé¢è½ç›˜ï¼ˆé¿å…å¼‚å¸¸å¯¼è‡´æ²¡ dumpï¼‰
    safe_name = "PRE__" + re.sub(r"\W+", "_", HOME_URL)[-80:]
    dump_debug_page(driver, safe_name, base_dir=DEBUG_DIR)

    for c in cookies:
        if not isinstance(c, dict):
            continue
        c2 = dict(c)
        c2.pop("sameSite", None)
        if "expiry" in c2 and c2["expiry"] is not None:
            try:
                c2["expiry"] = int(c2["expiry"])
            except Exception:
                c2.pop("expiry", None)
        try:
            driver.add_cookie(c2)
        except Exception:
            pass

    driver.get(HOME_URL)
    WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CSS_SELECTOR, "body")))


def init_camper_login_cookies(wait_seconds: int = 30):
    """æ‰“å¼€é¦–é¡µï¼Œç»™ 30 ç§’æ‰‹åŠ¨ç™»å½•ï¼Œä¿å­˜ cookies"""
    global LOGIN_COOKIES
    print("=" * 80)
    print("ğŸ” [Camper Login] å°†æ‰“å¼€å®˜ç½‘é¦–é¡µï¼Œè¯·åœ¨æµè§ˆå™¨é‡Œæ‰‹åŠ¨å®Œæˆç™»å½•ã€‚")
    print(f"â³ ä½ æœ‰ {wait_seconds} ç§’å®Œæˆç™»å½•ã€‚")
    print("âœ… ç™»å½•å®Œæˆåä¸éœ€è¦ç‚¹ä»»ä½•æŒ‰é’®ï¼Œè„šæœ¬ä¼šè‡ªåŠ¨ç»§ç»­å¹¶å…±äº« cookie ç»™å¤šçº¿ç¨‹ã€‚")
    print("=" * 80)

    driver = None
    try:
        driver = get_driver(name="camper_login", headless=False)
        driver.get(HOME_URL)
        WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.CSS_SELECTOR, "body")))
        time.sleep(wait_seconds)
        LOGIN_COOKIES = driver.get_cookies() or []
        print(f"ğŸª å·²è·å– cookies æ•°é‡: {len(LOGIN_COOKIES)}")
    finally:
        try:
            if driver:
                driver.quit()
        except Exception:
            pass


# =========================
# Core
# =========================
def process_product_url(product_url: str):
    driver = None
    try:
        driver = get_driver(name="camper", headless=True)

        # æ³¨å…¥ç™»å½•æ€
        if LOGIN_COOKIES:
            apply_cookies_to_driver(driver, LOGIN_COOKIES)

        print(f"\nğŸ” æ­£åœ¨è®¿é—®: {product_url}")
        driver.get(product_url)
        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CSS_SELECTOR, "body")))
        time.sleep(1.5)

        soup = BeautifulSoup(driver.page_source, "html.parser")

        # å…ˆæ‰¾ __NEXT_DATA__
        script_tag = soup.find("script", {"id": "__NEXT_DATA__", "type": "application/json"})
        if not script_tag or not script_tag.string:
            dump_debug_page(driver, "NO_NEXT_DATA")
            print("âš ï¸ æœªæ‰¾åˆ° __NEXT_DATA__ JSON")
            return

        json_data = json.loads(script_tag.string)
        product_sheet = (
            json_data.get("props", {})
            .get("pageProps", {})
            .get("productSheet")
        )
        if not product_sheet:
            dump_debug_page(driver, "NO_PRODUCT_SHEET")
            print(f"âš ï¸ æœªæ‰¾åˆ° productSheetï¼Œè·³è¿‡: {product_url}")
            return

        data = product_sheet
        product_code = data.get("code", "Unknown_Code")

        # âœ… å…³é”®ï¼šç°åœ¨ product_code å·²ç»æœ‰äº†ï¼Œå† dump
        dump_debug_page(driver, product_code)

        # âœ… å…³é”®ï¼šproduct_title å¿…é¡»å®šä¹‰ï¼ˆä½ ç°åœ¨å°±æ˜¯ç¼ºäº†è¿™ä¸ªï¼‰
        title_tag = soup.find("title")
        product_title = (
            re.sub(r"\s*[-â€“â€”].*", "", title_tag.text.strip())
            if title_tag and title_tag.text
            else data.get("name") or "Unknown Title"
        )

        description = data.get("description", "")
        original_price, discount_price, price_src = pick_prices_from_product_sheet(data)

        color_data = data.get("color", "")
        color = color_data.get("name", "") if isinstance(color_data, dict) else str(color_data)

        # features
        features_raw = data.get("features") or []
        feature_texts = []
        for f in features_raw:
            value_html = (f.get("value") or "")
            clean_text = BeautifulSoup(value_html, "html.parser").get_text(strip=True)
            if clean_text:
                feature_texts.append(clean_text)
        feature_str = " | ".join(feature_texts) if feature_texts else "No Data"

        # upper material
        upper_material = "No Data"
        for feature in features_raw:
            name = (feature.get("name") or "").lower()
            if "upper" in name:
                raw_html = feature.get("value") or ""
                upper_material = BeautifulSoup(raw_html, "html.parser").get_text(strip=True)
                break

        # sizes
        size_map = {}
        size_detail = {}
        for s in data.get("sizes", []):
            value = (s.get("value", "") or "").strip()
            available = bool(s.get("available", False))
            quantity = s.get("quantity", 0)
            ean = s.get("ean", "")
            size_map[value] = "æœ‰è´§" if available else "æ— è´§"
            size_detail[value] = {"stock_count": quantity, "ean": ean}

        gender = infer_gender_from_url(product_url)

        # fill missing sizes
        standard_sizes = SIZE_RANGE_CONFIG.get("camper", {}).get(gender, [])
        if standard_sizes:
            missing = [x for x in standard_sizes if x not in size_detail]
            for x in missing:
                size_map[x] = "æ— è´§"
                size_detail[x] = {"stock_count": 0, "ean": ""}
            if missing:
                print(f"âš ï¸ {product_code} è¡¥å…¨å°ºç : {', '.join(missing)}")

        style_category = infer_style_category(description)

        info = {
            "Product Code": product_code,
            "Product Name": product_title,
            "Product Description": description,
            "Product Gender": gender,
            "Product Color": color,

            "Product Price": str(original_price),
            "Adjusted Price": str(discount_price),

            "Product Material": upper_material,
            "Style Category": style_category,
            "Feature": feature_str,
            "SizeMap": size_map,
            "SizeDetail": size_detail,
            "Source URL": product_url,
            "Price Source": price_src,  # æ–¹ä¾¿æ’æŸ¥
        }

        save_dir = Path(SAVE_PATH)
        save_dir.mkdir(parents=True, exist_ok=True)
        out_path = save_dir / f"{product_code}.txt"
        format_txt(info, out_path, brand="camper")
        print(f"âœ… å®Œæˆ TXT: {out_path.name}  (src={price_src}, P={original_price}, D={discount_price})")

    except Exception as e:
        try:
            if driver:
                dump_debug_page(driver, "EXCEPTION")
        except Exception:
            pass
        print(f"âŒ é”™è¯¯: {product_url} - {e}")

    finally:
        try:
            if driver:
                driver.quit()
        except Exception:
            pass


def camper_fetch_product_info(product_urls_file=None, max_workers=MAX_WORKERS, login_wait_seconds: int = 30):
    if product_urls_file is None:
        product_urls_file = PRODUCT_URLS_FILE

    print(f"ğŸ“„ ä½¿ç”¨é“¾æ¥æ–‡ä»¶: {product_urls_file}")
    with open(product_urls_file, "r", encoding="utf-8") as f:
        urls = [line.strip() for line in f if line.strip()]

    init_camper_login_cookies(wait_seconds=login_wait_seconds)

    try:
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [executor.submit(process_product_url, u) for u in urls]
            for fu in as_completed(futures):
                fu.result()
    finally:
        quit_all_drivers()


if __name__ == "__main__":
    camper_fetch_product_info()
