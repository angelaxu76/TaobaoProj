# fetch_product_info.py
import os
import re
import time
import json
import threading

from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
from config import CAMPER, SIZE_RANGE_CONFIG  # âœ… å¼•å…¥æ ‡å‡†å°ºç é…ç½®
from common.ingest.txt_writer import format_txt
from common.product.category_utils import infer_style_category
from selenium import webdriver
driver = webdriver.Chrome()
from common.browser.selenium_utils import get_driver

PRODUCT_URLS_FILE = CAMPER["LINKS_FILE"]
SAVE_PATH = CAMPER["TXT_DIR"]
MAX_WORKERS = 6

os.makedirs(SAVE_PATH, exist_ok=True)

def infer_gender_from_url(url: str) -> str:
    url = url.lower()
    if "/women/" in url:
        return "å¥³æ¬¾"
    elif "/men/" in url:
        return "ç”·æ¬¾"
    elif "/kids/" in url or "/children/" in url:
        return "ç«¥æ¬¾"
    return "æœªçŸ¥"

def create_driver():
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options

    chrome_options = Options()
    chrome_options.add_argument("--headless=new")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--window-size=1920x1080")
    chrome_options.add_argument("user-agent=Mozilla/5.0")
    chrome_options.add_argument("--log-level=3")
    chrome_options.add_argument("--disable-logging")
    chrome_options.add_argument("--disable-notifications")
    chrome_options.add_argument("--disable-background-networking")
    chrome_options.add_argument("--disable-default-apps")
    chrome_options.add_argument("--disable-sync")
    chrome_options.add_argument("--no-first-run")
    chrome_options.add_argument("--no-default-browser-check")
    chrome_options.add_argument("--disable-gcm-driver")
    chrome_options.add_argument("--disable-features=Translate,MediaRouter,AutofillServerCommunication")
    chrome_options.add_argument("--blink-settings=imagesEnabled=false")

    # âœ… ä¸å†æ‰‹åŠ¨æŒ‡å®šè·¯å¾„ï¼Œä¹Ÿä¸ä½¿ç”¨ chromedriver_autoinstaller
    driver = webdriver.Chrome(options=chrome_options)

    # æ‰“å°ç‰ˆæœ¬ç¡®è®¤åŒ¹é…
    try:
        caps = driver.capabilities
        print("Chrome:", caps.get("browserVersion"))
        print("ChromeDriver:", (caps.get("chrome") or {}).get("chromedriverVersion", ""))
    except Exception:
        pass

    return driver



# === æ–°å¢žï¼šå…¨å±€è®°å½• driver å¹¶ç»Ÿä¸€å›žæ”¶ï¼Œé¿å…å¤šè½®è¿è¡Œæ®‹ç•™è¿›ç¨‹ ===
drivers_lock = threading.Lock()
_all_drivers = set()

thread_local = threading.local()
# common/selenium_utils.py

def get_driver():
    from selenium.webdriver.chrome.service import Service
    from selenium.webdriver.chrome.options import Options
    import undetected_chromedriver as uc

    options = Options()
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")

    driver = uc.Chrome(options=options)
    return driver


def shutdown_all_drivers():
    # ä»»åŠ¡ç»“æŸç»Ÿä¸€å…³é—­æ‰€æœ‰æ— å¤´æµè§ˆå™¨ï¼Œé˜²æ³„æ¼
    with drivers_lock:
        for d in list(_all_drivers):
            try:
                d.quit()
            except Exception:
                pass
        _all_drivers.clear()

def process_product_url(PRODUCT_URL):
    try:
        driver = get_driver()
        print(f"\nðŸ” æ­£åœ¨è®¿é—®: {PRODUCT_URL}")
        driver.get(PRODUCT_URL)
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, "body")))
        time.sleep(5)

        soup = BeautifulSoup(driver.page_source, "html.parser")
        title_tag = soup.find("title")
        product_title = re.sub(r"\s*[-â€“â€”].*", "", title_tag.text.strip()) if title_tag else "Unknown Title"

        script_tag = soup.find("script", {"id": "__NEXT_DATA__", "type": "application/json"})
        if not script_tag:
            print("âš ï¸ æœªæ‰¾åˆ° JSON æ•°æ®")
            return

        json_data = json.loads(script_tag.string)
        product_sheet = json_data.get("props", {}).get("pageProps", {}).get("productSheet")
        if not product_sheet:
            print(f"âš ï¸ æœªæ‰¾åˆ° productSheetï¼Œè·³è¿‡: {PRODUCT_URL}")
            return
        data = product_sheet

        product_code = data.get("code", "Unknown_Code")
        product_url = PRODUCT_URL
        description = data.get("description", "")

        price_info = data.get("prices", {})
        original_price = price_info.get("previous", 0)
        discount_price = price_info.get("current", 0)

        color_data = data.get("color", "")
        color = color_data.get("name", "") if isinstance(color_data, dict) else str(color_data)

        # === æå– features ===
        features_raw = data.get("features") or []  # âœ… ä¿è¯æ˜¯åˆ—è¡¨
        feature_texts = []
        for f in features_raw:
            try:
                value_html = f.get("value", "")
                clean_text = BeautifulSoup(value_html, "html.parser").get_text(strip=True)
                if clean_text:
                    feature_texts.append(clean_text)
            except Exception as e:
                print(f"âš ï¸ Feature è§£æžå¤±è´¥: {e}")
        feature_str = " | ".join(feature_texts) if feature_texts else "No Data"

        # === æå– Upper æè´¨ï¼ˆä¼˜å…ˆ featuresï¼‰ ===
        upper_material = "No Data"
        for feature in features_raw:
            name = (feature.get("name") or "").lower()
            if "upper" in name:
                raw_html = feature.get("value") or ""
                upper_material = BeautifulSoup(raw_html, "html.parser").get_text(strip=True)
                break

        # === æå–å°ºç ã€åº“å­˜ã€EAN ===
        size_map = {}
        size_detail = {}
        for size in data.get("sizes", []):
            value = size.get("value", "").strip()
            available = size.get("available", False)
            quantity = size.get("quantity", 0)
            ean = size.get("ean", "")
            size_map[value] = "æœ‰è´§" if available else "æ— è´§"
            size_detail[value] = {
                "stock_count": quantity,
                "ean": ean
            }

        gender = infer_gender_from_url(PRODUCT_URL)

        # âœ… å°ºç è¡¥å…¨é€»è¾‘
        standard_sizes = SIZE_RANGE_CONFIG.get("camper", {}).get(gender, [])
        if standard_sizes:
            missing_sizes = [s for s in standard_sizes if s not in size_detail]
            for s in missing_sizes:
                size_map[s] = "æ— è´§"
                size_detail[s] = {"stock_count": 0, "ean": ""}
            if missing_sizes:
                print(f"âš ï¸ {product_code} è¡¥å…¨å°ºç : {', '.join(missing_sizes)}")

        style_category = infer_style_category(description)
        # === æ•´ç† info å­—å…¸ ===
        info = {
            "Product Code": product_code,
            "Product Name": product_title,
            "Product Description": description,
            "Product Gender": gender,
            "Product Color": color,
            "Product Price": str(original_price),
            "Adjusted Price": str(discount_price),
            "Product Material": upper_material,
            "Style Category": style_category,  # âœ… æ–°å¢žå­—æ®µ
            "Feature": feature_str,
            "SizeMap": size_map,
            "SizeDetail": size_detail,
            "Source URL": product_url
        }

        # === å†™å…¥ TXT æ–‡ä»¶ ===
        filepath = SAVE_PATH / f"{product_code}.txt"
        format_txt(info, filepath, brand="camper")
        print(f"âœ… å®Œæˆ TXT: {filepath.name}")

    except Exception as e:
        print(f"âŒ é”™è¯¯: {PRODUCT_URL} - {e}")

def camper_fetch_product_info(product_urls_file=None, max_workers=MAX_WORKERS):
    """
    Camper å•†å“æŠ“å–ä¸»å…¥å£ã€‚
    :param product_urls_file: å¯é€‰ï¼Œè‡ªå®šä¹‰çš„ product_links.txt è·¯å¾„ã€‚å¦‚æžœä¸º Noneï¼Œåˆ™ä½¿ç”¨ config ä¸­çš„ CAMPER["LINKS_FILE"]ã€‚
    :param max_workers: çº¿ç¨‹æ•°ã€‚
    """
    if product_urls_file is None:
        product_urls_file = PRODUCT_URLS_FILE

    print(f"ðŸ“„ ä½¿ç”¨é“¾æŽ¥æ–‡ä»¶: {product_urls_file}")

    with open(product_urls_file, "r", encoding="utf-8") as f:
        urls = [line.strip() for line in f if line.strip()]

    try:
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [executor.submit(process_product_url, url) for url in urls]
            for future in as_completed(futures):
                future.result()
    finally:
        # âœ… å…³é”®ï¼šæ¯è½®ä»»åŠ¡ç»“æŸéƒ½å…³é—­å…¨éƒ¨ driverï¼Œé¿å…æ®‹ç•™è¿›ç¨‹å †ç§¯
        shutdown_all_drivers()


# === New: URL->code è§£æžä¸Žç¼ºå¤±è¡¥æŠ“å·¥å…· ===
import re
from pathlib import Path
from urllib.parse import urlparse

CODE_PATTERNS = [
    r"[AK]\d{6}-\d{3}",     # K100743-003 / A700019-001
    r"\d{5,6}-\d{3}",       # 90203-051 / 16002-323 ç­‰
]
CODE_REGEX = re.compile(r"(" + "|".join(CODE_PATTERNS) + r")")

def normalize_url(u: str) -> str:
    u = u.strip()
    if not u:
        return u
    if not u.startswith("http"):
        u = "https://" + u.lstrip("/")
    return u

def code_from_url(u: str) -> str | None:
    u = normalize_url(u)
    m = list(CODE_REGEX.finditer(u))
    return m[-1].group(0) if m else None  # å–æœ€åŽä¸€ä¸ªåŒ¹é…ï¼Œæœ€ç¨³å¦¥

def load_all_urls(path: str) -> list[str]:
    with open(path, "r", encoding="utf-8") as f:
        return [normalize_url(x) for x in (line.strip() for line in f) if x.strip()]

def existing_codes_from_txt_dir(txt_dir: str) -> set[str]:
    p = Path(txt_dir)
    if not p.exists():
        p.mkdir(parents=True, exist_ok=True)
    return {fn.stem.upper() for fn in p.glob("*.txt")}

def expected_maps(urls: list[str]) -> dict[str, str]:
    # è¿”å›ž {code: url}
    mapping = {}
    for u in urls:
        c = code_from_url(u)
        if c:
            mapping[c.upper()] = u
    return mapping

def run_batch_fetch(urls: list[str], max_workers: int = MAX_WORKERS):
    # å¤ç”¨ä½ å·²æœ‰çš„å¹¶å‘æŠ“å–é€»è¾‘ï¼Œä½†åªæŠ•é€’ç»™å®š urls
    try:
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [executor.submit(process_product_url, u) for u in urls]
            for fut in as_completed(futures):
                fut.result()
    finally:
        shutdown_all_drivers()  # ä½ å·²æœ‰çš„ç»Ÿä¸€å›žæ”¶ï¼Œé˜²æ³„æ¼

def camper_fetch_all_with_retry(
    product_urls_file=None,
    txt_dir: str = str(SAVE_PATH),
    max_passes: int = 3,
    first_pass_workers: int = MAX_WORKERS,
    retry_workers: int = 6
):
    
    if product_urls_file is None:
        product_urls_file = PRODUCT_URLS_FILE

    all_urls = load_all_urls(product_urls_file)
    code2url = expected_maps(all_urls)

    print(f"ðŸ“¦ æ€»é“¾æŽ¥æ•°ï¼š{len(all_urls)}")
    print(f"ðŸ“ TXT ç›®å½•ï¼š{txt_dir}")

    # ç¬¬1è½®ï¼šå…¨é‡
    print(f"\n==> Pass 1 / {max_passes}: å…¨é‡æŠ“å– {len(all_urls)} æ¡")
    run_batch_fetch(all_urls, max_workers=first_pass_workers)

    for i in range(2, max_passes + 1):
        have = existing_codes_from_txt_dir(txt_dir)
        need = set(code2url.keys())
        missing_codes = sorted((need - have))

        print(f"\nðŸ” Pass {i} æ£€æŸ¥ç¼ºå¤±:")
        print(f"    å·²æœ‰TXTæ•°é‡ï¼š{len(have)}")
        print(f"    åº”æœ‰æ€»æ•°ï¼š{len(need)}")
        print(f"    ç¼ºå¤±æ•°é‡ï¼š{len(missing_codes)}")

        if not missing_codes:
            print("ðŸŽ‰ æ²¡æœ‰ç¼ºå¤±ï¼Œä»»åŠ¡å®Œæˆã€‚")
            break

        # æ‰“å°éƒ¨åˆ†ç¼ºå¤±ç¼–ç é¢„è§ˆ
        print("    ç¼ºå¤±ç¼–ç ç¤ºä¾‹ï¼š", ", ".join(missing_codes[:10]), "..." if len(missing_codes) > 10 else "")

        # ç”Ÿæˆç¼ºå¤±åå•ä¸Žå¯¹åº” URL åˆ—è¡¨
        missing_urls = [code2url[c] for c in missing_codes if c in code2url]
        miss_list_path = Path(txt_dir) / f"missing_camper_pass{i}.txt"
        with open(miss_list_path, "w", encoding="utf-8") as f:
            for c in missing_codes:
                f.write(f"{c}\t{code2url.get(c,'')}\n")

        print(f"ðŸ§¾ å·²å†™å…¥ç¼ºå¤±æ¸…å•ï¼š{miss_list_path}")
        print(f"ðŸš€ å¼€å§‹è¡¥æŠ“ {len(missing_urls)} æ¡é“¾æŽ¥...")

        # æ‰§è¡Œè¡¥æŠ“
        run_batch_fetch(missing_urls, max_workers=retry_workers)

        # æŠ“å–åŽå†æ£€æŸ¥æ•°é‡å˜åŒ–
        after_have = existing_codes_from_txt_dir(txt_dir)
        new_files = sorted(after_have - have)
        print(f"âœ… Pass {i} ç»“æŸåŽæ–°å¢ž {len(new_files)} ä¸ªTXTã€‚")
        if new_files:
            print("    æ–°å¢žæ–‡ä»¶ç¤ºä¾‹ï¼š", ", ".join(new_files[:10]), "..." if len(new_files) > 10 else "")

    # æ”¶å°¾æ±‡æ€»
    have_final = existing_codes_from_txt_dir(txt_dir)
    need_final = set(code2url.keys())
    still_missing = sorted(need_final - have_final)
    summary_path = Path(txt_dir) / "missing_camper_final.txt"
    if still_missing:
        with open(summary_path, "w", encoding="utf-8") as f:
            for c in still_missing:
                f.write(f"{c}\t{code2url.get(c,'')}\n")
        print(f"\nâš ï¸ ä»æœ‰ {len(still_missing)} æ¡æœªæŠ“åˆ°ï¼Œæ¸…å•è§: {summary_path}")
    else:
        if summary_path.exists():
            summary_path.unlink(missing_ok=True)
        print("\nâœ… æœ€ç»ˆæ²¡æœ‰ç¼ºå¤±ã€‚")

def camper_retry_missing_once(product_urls_file=None):
    """
    ä»…è¡¥æŠ“ç¼ºå¤±çš„ TXTï¼Œä¸è·‘å…¨é‡ã€‚
    å¯åå¤è°ƒç”¨å¤šæ¬¡ä»¥è¿›ä¸€æ­¥è¡¥é½ã€‚

    :param product_urls_file: å¯é€‰ï¼Œè‡ªå®šä¹‰ links æ–‡ä»¶ã€‚ä¸ä¼ åˆ™ä½¿ç”¨ config é»˜è®¤ã€‚
    """
    if product_urls_file is None:
        product_urls_file = PRODUCT_URLS_FILE

    txt_dir = str(SAVE_PATH)
    max_workers = 6
    preview = 30

    all_urls = load_all_urls(product_urls_file)
    code2url = expected_maps(all_urls)

    have = existing_codes_from_txt_dir(txt_dir)
    need = set(code2url.keys())
    missing_codes = sorted(need - have)

    print("\nðŸ” Camper Retry Missing Once")
    print("ðŸ“¦ æ€»é“¾æŽ¥æ•°ï¼š", len(all_urls))
    print("ðŸ“ TXT ç›®å½•ï¼š", txt_dir)
    print("ðŸ§® å·²æœ‰TXTï¼š", len(have))
    print("âŒ ç¼ºå¤±æ•°é‡ï¼š", len(missing_codes))

    if not missing_codes:
        print("ðŸŽ‰ æ²¡æœ‰ç¼ºå¤±å¯è¡¥æŠ“ã€‚")
        return

    print("ðŸ“ ç¼ºå¤±ç¼–ç ç¤ºä¾‹ï¼š", ", ".join(missing_codes[:preview]), "..." if len(missing_codes) > preview else "")

    missing_urls = [code2url[c] for c in missing_codes if c in code2url]
    miss_list_path = Path(txt_dir) / "missing_camper_once.txt"
    with open(miss_list_path, "w", encoding="utf-8") as f:
        for c in missing_codes:
            f.write(f"{c}\t{code2url.get(c,'')}\n")

    print(f"ðŸ§¾ å·²å†™å…¥ç¼ºå¤±æ¸…å•ï¼š{miss_list_path}")
    print(f"ðŸš€ å¼€å§‹è¡¥æŠ“ç¼ºå¤± {len(missing_urls)} æ¡â€¦â€¦")

    run_batch_fetch(missing_urls, max_workers=max_workers)

    after = existing_codes_from_txt_dir(txt_dir)
    new_files = sorted(after - have)
    print(f"âœ… æœ¬æ¬¡è¡¥æŠ“æ–°å¢ž TXTï¼š{len(new_files)}")
    if new_files:
        print("ðŸ“‚ æ–°å¢žæ–‡ä»¶é¢„è§ˆï¼š", ", ".join(new_files[:preview]), "..." if len(new_files) > preview else "")



if __name__ == "__main__":
    camper_fetch_product_info()
