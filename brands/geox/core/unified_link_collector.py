import time
import re
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import requests
from pathlib import Path
import sys
sys.stdout.reconfigure(encoding='utf-8')


# === é…ç½®é¡¹ ===
BASE_URL = "https://www.geox.com"
CATEGORY_URLS = {
    "woman": "https://www.geox.com/en-GB/woman/shoes/",
    "man": "https://www.geox.com/en-GB/man/shoes/",
    "Gril": "https://www.geox.com/en-GB/girl/shoes/",
    "kids": "https://www.geox.com/en-GB/boy/"
}
SAVE_FILE = Path("D:/TB/Products/GEOX/publication/product_links.txt")
HTML_DEBUG_FILE = Path("D:/TB/Products/GEOX/publication/debug_geox_page.html")
MAX_PAGES = 30
PAGE_SIZE = 24

SAVE_FILE.parent.mkdir(parents=True, exist_ok=True)

# åˆ¤æ–­æ˜¯å¦æ˜¯å•†å“é“¾æ¥
def is_valid_product_link(href: str) -> bool:
    return (
        href.startswith("https://www.geox.com/en-GB/")
        and href.endswith(".html")
        and re.search(r"[A-Z0-9]{4}\.html$", href) is not None
    )

# æŠ“å–å•ä¸ªç±»ç›®çš„æ‰€æœ‰é“¾æ¥
def fetch_product_links_for_category(category_name: str, category_url: str):
    all_links = set()
    last_soup = None
    print(f"\nğŸ” å¼€å§‹æŠ“å–ç±»ç›®: {category_name}")

    for page in range(MAX_PAGES):
        page_url = category_url if page == 0 else f"{category_url}?start=0&sz={PAGE_SIZE*(page+1)}"
        print(f"  ğŸ”„ ç¬¬ {page+1} é¡µ: {page_url}")
        response = requests.get(page_url, headers={"User-Agent": "Mozilla/5.0"})
        if response.status_code != 200:
            print(f"  âŒ é¡µé¢è¯·æ±‚å¤±è´¥: {response.status_code}")
            break

        soup = BeautifulSoup(response.text, "html.parser")
        last_soup = soup
        candidate_links = soup.find_all("a", href=True)
        count_before = len(all_links)

        for tag in candidate_links:
            full_url = urljoin(BASE_URL, tag["href"])
            if is_valid_product_link(full_url):
                all_links.add(full_url)

        added = len(all_links) - count_before
        print(f"  âœ… æ–°å¢é“¾æ¥: {added}")

        if added == 0:
            break

        time.sleep(1)

    return all_links, last_soup

# ä¸»å‡½æ•°
def main():
    total_links = set()
    last_soup = None

    for category_name, category_url in CATEGORY_URLS.items():
        links, soup = fetch_product_links_for_category(category_name, category_url)
        total_links.update(links)
        last_soup = soup  # ç”¨äºä¿å­˜æœ€åä¸€ä¸ªé¡µé¢çš„ HTML è°ƒè¯•ä¿¡æ¯

    with open(SAVE_FILE, "w", encoding="utf-8") as f:
        for link in sorted(total_links):
            f.write(link + "\n")
    print(f"\nğŸ“¦ å…±æå–å•†å“é“¾æ¥: {len(total_links)} æ¡")
    print(f"ğŸ“„ é“¾æ¥å·²ä¿å­˜è‡³: {SAVE_FILE}")

    if last_soup:
        with open(HTML_DEBUG_FILE, "w", encoding="utf-8") as f:
            f.write(last_soup.prettify())
        print(f"ğŸ“„ æœ€åé¡µé¢ HTML å·²ä¿å­˜è‡³: {HTML_DEBUG_FILE}")

if __name__ == "__main__":
    main()
