import os
import re
import time
from pathlib import Path
from typing import Dict, Optional

from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

from config import SIZE_RANGE_CONFIG, GEOX
from common.ingest.txt_writer import format_txt
from common.core.category_utils import infer_style_category

# ===================== åŸºæœ¬é…ç½® =====================
PRODUCT_LINK_FILE = GEOX["BASE"] / "publication" / "product_links.txt"
TXT_OUTPUT_DIR = GEOX["TXT_DIR"]
BRAND = "geox"
MAX_THREADS = 1              # å…ˆå•çº¿ç¨‹ï¼ŒæŠŠç™»å½•æ€/æŠ˜æ‰£è·‘ç¨³åå†è°ƒé«˜
LOGIN_WAIT_SECONDS = 20      # æ‰‹åŠ¨ç™»å½•ç­‰å¾…æ—¶é—´ï¼ˆå¯æŒ‰éœ€è°ƒæ•´ï¼‰

TXT_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# ===================== WebDriver åˆ›å»ºï¼ˆå›ºå®šä½¿ç”¨å·²å­˜åœ¨çš„ Chrome Profileï¼‰ =====================
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import SessionNotCreatedException

# âš ï¸ é‡è¦ï¼šæŠŠ PROFILE_ROOT æŒ‡å‘ â€œéé»˜è®¤ç›®å½•â€ çš„æ ¹ï¼ˆé¿å… DevToolsActivePort æŠ¥é”™ï¼‰
# å»ºè®®å…ˆæŠŠä½ å·²ç™»å½•çš„ Profile å¤åˆ¶åˆ°è¿™ä¸ªç›®å½•ä¸‹ï¼ˆè§æ–‡æ¡£æ­¥éª¤ï¼‰
PROFILE_ROOT = r"D:\ChromeProfiles\AutoProfile_GEOX"   # éé»˜è®¤ç›®å½•æ ¹
PROFILE_NAME = "Profile 2"                              # å­ç›®å½•åï¼šProfile 1/2/3/4/Default ç­‰
FIXED_UA = ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120.0.0.0 Safari/537.36")

def _build_options(headless: bool = False, user_data_dir: str = PROFILE_ROOT, profile_name: str = PROFILE_NAME) -> Options:
    chrome_options = Options()

    # è°ƒè¯•æœŸå»ºè®®å…ˆä¸å¼€æ— å¤´ï¼Œç¡®è®¤å¯è§çª—å£èƒ½ç™»å½•/æ˜¾ç¤ºæŠ˜æ‰£åå†å¼€å¯
    if headless:
        chrome_options.add_argument("--headless=new")

    # ï¼ˆå¯é€‰ï¼‰æŒ‡å®šäºŒè¿›åˆ¶è·¯å¾„ï¼›å¦‚æœä½ çš„ Chrome ä¸åœ¨æ ‡å‡†è·¯å¾„ï¼Œæ”¹æˆå®é™…è·¯å¾„
    chrome_options.binary_location = r"C:\Program Files\Google\Chrome\Application\chrome.exe"

    # âœ… å¤ç”¨å·²å­˜åœ¨çš„ Profileï¼ˆå…³é”®ï¼‰
    chrome_options.add_argument(f"--user-data-dir={user_data_dir}")
    chrome_options.add_argument(f"--profile-directory={profile_name}")

    # ç¨³å®šå‚æ•°
    chrome_options.add_argument("--start-maximized")
    chrome_options.add_argument("--no-first-run")
    chrome_options.add_argument("--no-default-browser-check")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option("useAutomationExtension", False)
    chrome_options.add_argument(f"user-agent={FIXED_UA}")

    # â›” å…ˆä¸è¦ remote-debugging-portï¼Œé¿å… â€œéé»˜è®¤ç›®å½•â€ çš„é™åˆ¶è§¦å‘
    # chrome_options.add_argument("--remote-debugging-port=9222")

    return chrome_options

def _kill_chrome():
    os.system('taskkill /F /IM chrome.exe /T')
    os.system('taskkill /F /IM chromedriver.exe /T')

def create_driver(headless: bool = False) -> webdriver.Chrome:
    # æ‰“å°å…³é”®å‚æ•°ï¼Œä¾¿äºä½ åœ¨æ§åˆ¶å°ç¡®è®¤
    print("Using user-data-dir =", PROFILE_ROOT)
    print("Using profile-directory =", PROFILE_NAME)
    try:
        opts = _build_options(headless=headless)
        driver = webdriver.Chrome(options=opts)
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined});")
        print("Chrome =", driver.capabilities.get("browserVersion"),
              "| Chromedriver =", driver.capabilities.get("chrome", {}).get("chromedriverVersion"))
        return driver
    except SessionNotCreatedException:
        # å…¸å‹æ˜¯ç›®å½•è¢«å ç”¨/é”ä½ï¼šæ€è¿›ç¨‹åé‡è¯•
        _kill_chrome()
        time.sleep(0.5)
        opts = _build_options(headless=headless)
        driver = webdriver.Chrome(options=opts)
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined});")
        print("Chrome =", driver.capabilities.get("browserVersion"),
              "| Chromedriver =", driver.capabilities.get("chrome", {}).get("chromedriverVersion"))
        return driver


def create_worker_driver() -> webdriver.Chrome:
    # è¿™ä¸ª driver ç”¨æ¥é•¿æ—¶é—´å¤ç”¨ï¼›ä¸åŠ¨ä½ åŸæœ‰çš„ create_driver é€»è¾‘
    from selenium.webdriver.chrome.options import Options
    o = Options()
    o.add_argument("--headless=new")                 # æ— å¤´ï¼Œæé€Ÿ
    o.add_argument("--disable-gpu")
    o.add_argument("--no-sandbox")
    o.add_argument("--disable-dev-shm-usage")
    o.add_argument("--window-size=1920,1080")
    o.add_argument("--disable-blink-features=AutomationControlled")
    o.add_experimental_option("excludeSwitches", ["enable-automation"])
    o.add_experimental_option("useAutomationExtension", False)
    o.add_argument("--blink-settings=imagesEnabled=false")  # ç¦å›¾è¿›ä¸€æ­¥æé€Ÿ
    o.add_argument(
        "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    )
    # DOMContentLoaded å³è¿”å›ï¼Œå‡å°‘ç­‰å¾…
    o.set_capability("pageLoadStrategy", "eager")
    d = webdriver.Chrome(options=o)
    d.execute_script("Object.defineProperty(navigator,'webdriver',{get:()=>undefined});")
    return d

# ===================== ä¼šè¯å¯¼å‡º/å¯¼å…¥ï¼ˆå¯é€‰ï¼Œç”¨äºå¹¶å‘çº¿ç¨‹æ³¨å…¥ï¼‰ =====================
def export_session(driver: webdriver.Chrome) -> Dict:
    """å¯¼å‡º cookies + localStorageï¼Œä¾›å¹¶å‘çº¿ç¨‹å¤ç”¨ç™»å½•æ€ã€‚"""
    cookies = driver.get_cookies()
    ls_items = driver.execute_script(
        """
        const out = {}; 
        for (let i=0; i<localStorage.length; i++){
            const k = localStorage.key(i);
            out[k] = localStorage.getItem(k);
        }
        return out;
        """
    )
    return {"cookies": cookies, "localStorage": ls_items}

def import_session(driver: webdriver.Chrome, session: Dict, base_url: str = "https://www.geox.com/") -> None:
    """å°† cookies + localStorage æ³¨å…¥åˆ°æ–° driverã€‚å¿…é¡»å…ˆæ‰“å¼€åŒåŸŸé¡µé¢ã€‚"""
    driver.get(base_url)

    # å†™å…¥ cookies
    for c in session.get("cookies", []):
        to_add = {
            "name": c.get("name"),
            "value": c.get("value"),
            "path": c.get("path", "/"),
            "secure": c.get("secure", True),
            "httpOnly": c.get("httpOnly", False),
        }
        if c.get("domain"):
            to_add["domain"] = c["domain"]
        if c.get("expiry"):
            to_add["expiry"] = c["expiry"]
        try:
            driver.add_cookie(to_add)
        except Exception as e:
            print(f"cookie æ³¨å…¥å¤±è´¥ {to_add.get('name')}: {e}")

    # å†™å…¥ localStorage
    driver.execute_script("localStorage.clear();")
    for k, v in session.get("localStorage", {}).items():
        driver.execute_script("localStorage.setItem(arguments[0], arguments[1]);", k, v)

# ===================== é¡µé¢æŠ“å–ï¼ˆç­‰å¾…ä¸»ä»·æ ¼/æŠ˜æ‰£/æŒ‰é’®æ³¨å…¥ï¼‰ =====================
def get_html(driver: webdriver.Chrome, url: str) -> Optional[str]:
    def _accept_cookies():
        for sel in [
            "button#onetrust-accept-btn-handler",
            "button.cookie-accept", "button.js-accept-all"
        ]:
            try:
                btn = WebDriverWait(driver, 0.1).until(
                    EC.element_to_be_clickable((By.CSS_SELECTOR, sel))
                )
                btn.click()
                time.sleep(0)   # â† ç«‹å³ç»§ç»­
                return
            except Exception:
                continue

    def _scroll_warmup():
        driver.execute_script("window.scrollTo(0, 400);"); time.sleep(0)
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight - 400);"); time.sleep(0)
        driver.execute_script("window.scrollTo(0, 0);"); time.sleep(0)

    driver.get(url)
    _accept_cookies()

    try:
        WebDriverWait(driver, 0.1).until(
            EC.presence_of_element_located((By.CSS_SELECTOR,
                "div.product-info div.price, div.right-side div.price, div.price-mobile div.price, div.price"))
        )
        WebDriverWait(driver, 0.1).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "button.add-to-cart"))
        )
    except Exception:
        pass

    _scroll_warmup()

    try:
        WebDriverWait(driver, 0.1).until(
            EC.presence_of_element_located((By.CSS_SELECTOR,
                "span.product-price span.sales span.value[content]"))
        )
        try:
            WebDriverWait(driver, 1).until(
                EC.presence_of_element_located((By.CSS_SELECTOR,
                    "span.product-price span.sales.discount span.value[content]"))
            )
        except Exception:
            pass
    except Exception:
        pass

    try:
        WebDriverWait(driver, 0.1).until(
            lambda d: (
                d.execute_script("""
                    const btn = document.querySelector('button.add-to-cart');
                    if(!btn) return false;
                    try { return typeof JSON.parse(btn.getAttribute('data-gtmdata')||'{}').item_promo === 'string'; }
                    catch(e){ return false; }
                """) is True
            )
        )
    except Exception:
        pass

    time.sleep(0)
    return driver.page_source


# ===================== ä¸šåŠ¡è§£æ =====================
def supplement_geox_sizes(size_stock: Dict[str, str], gender: str) -> Dict[str, str]:
    standard_sizes = SIZE_RANGE_CONFIG.get("geox", {}).get(gender, [])
    for size in standard_sizes:
        if size not in size_stock:
            size_stock[size] = "0"  # æ— è´§
    return size_stock

def detect_gender_by_code(code: str) -> str:
    if not code:
        return "æœªçŸ¥"
    code = code.strip().upper()
    if code.startswith("D"): return "å¥³æ¬¾"
    if code.startswith("U"): return "ç”·æ¬¾"
    if code.startswith("J"): return "ç«¥æ¬¾"
    return "æœªçŸ¥"

def parse_product(html: str, url: str) -> Dict:
    soup = BeautifulSoup(html, "html.parser")

    # åŸºæœ¬ä¿¡æ¯
    code_tag = soup.select_one("span.product-id")
    code = code_tag.text.strip() if code_tag else "No Data"

    name_tag = soup.select_one("div.sticky-image img")
    name = name_tag["alt"].strip() if name_tag and name_tag.has_attr("alt") else "No Data"

    # ä»·æ ¼ï¼ˆä»…é™ PDP ä¸»åŒºåŸŸï¼›æ’é™¤æ¨è/è½®æ’­ï¼›åŒºé—´ä»·å–æœ€å¤§ï¼‰
    def _safe_select_value(_soup, selector: str):
        for node in _soup.select(selector):
            if node.find_parent(class_="product-tile") or node.find_parent(class_="product-carousel-tile"):
                continue
            return node
        return None

    def extract_max_price(val):
        if not val: return "No Data"
        s = str(val).strip()
        if "-" in s:
            try:
                parts = [float(p.strip()) for p in s.split("-") if p.strip()]
                return f"{max(parts):.2f}"
            except Exception:
                return s
        return s

    price_tag = _safe_select_value(soup, "span.product-price span.value")
    discount_tag = _safe_select_value(soup, "span.sales.discount span.value")

    full_price_raw = (price_tag.get("content") or price_tag.get_text(strip=True).replace("Â£","")).strip() if price_tag else ""
    discount_price_raw = (discount_tag.get("content") or discount_tag.get_text(strip=True).replace("Â£","")).strip() if discount_tag else ""

    original_price = extract_max_price(full_price_raw) or "No Data"

    # æŠ˜æ‰£ä»·æ ¡éªŒï¼šæ— æŠ˜æ‰£/å¼‚å¸¸å€¼ï¼Œå›é€€åŸä»·
    discount_price = extract_max_price(discount_price_raw) if discount_price_raw else ""
    try:
        op = float(original_price) if original_price not in ("", "No Data") else None
        dp = float(discount_price) if discount_price not in ("", "No Data") else None
        if dp is None or op is None or dp >= op or dp < op * 0.3:
            discount_price = original_price
    except Exception:
        discount_price = original_price

    # é¢œè‰² / æè´¨ / æè¿°
    color_block = soup.select_one("div.sticky-color")
    color = color_block.get_text(strip=True).replace("Color:", "").strip() if color_block else "No Data"

    materials_block = soup.select_one("div.materials-container")
    material_text = materials_block.get_text(" ", strip=True) if materials_block else "No Data"

    desc_block = soup.select_one("div.product-description div.value")
    description = desc_block.get_text(strip=True) if desc_block else "No Data"

    # æ€§åˆ«
    gender = detect_gender_by_code(code)

    # å°ºç åº“å­˜
    size_blocks = soup.select("div.size-value")
    size_stock: Dict[str, str] = {}
    for sb in size_blocks:
        size = sb.get("data-attr-value") or sb.get("prodsize") or sb.get("aria-label")
        size = size.strip().replace(",", ".") if size else "Unknown"
        available = "1" if "disabled" not in sb.get("class", []) else "0"
        size_stock[size] = available

    size_stock = supplement_geox_sizes(size_stock, gender)

    # Jingya æ¨¡å¼ï¼šè¾“å‡º SizeMap / SizeDetail
    size_map: Dict[str, str] = {}
    size_detail: Dict[str, Dict] = {}
    for eu, flag in size_stock.items():
        has = (str(flag) == "1")
        size_map[eu] = "æœ‰è´§" if has else "æ— è´§"
        size_detail[eu] = {"stock_count": 3 if has else 0, "ean": "0000000000000"}

    # é£æ ¼åˆ†ç±»
    style_category = infer_style_category(f"{name} {description}")

    info = {
        "Product Code": code,
        "Product Name": name,
        "Product Description": description,
        "Product Gender": gender,
        "Product Color": color,
        "Product Price": original_price,
        "Adjusted Price": discount_price,
        "Product Material": material_text,
        "Style Category": style_category,
        "Feature": "No Data",
        "SizeMap": size_map,
        "SizeDetail": size_detail,
        "Source URL": url,
    }
    return info

from urllib.parse import urlparse


def derive_code_from_url(url: str) -> str:
    """ä» URL æœ«å°¾æå–å•†å“ç¼–ç ï¼ˆâ€œ-<code>.htmlâ€ï¼‰ã€‚"""
    try:
        path = urlparse(url).path
        name = Path(path).name
        base = name.split('?', 1)[0]
        token = base.rsplit('-', 1)[-1]
        code = token.split('.', 1)[0].upper()
        if len(code) < 6 or not any(ch.isdigit() for ch in code):
            m = re.search(r"([A-Za-z0-9]{6,})\.html$", base)
            if m: code = m.group(1).upper()
        return code
    except Exception:
        return Path(urlparse(url).path).stem.upper()

# ===================== ä¸»æµç¨‹ï¼ˆç™»å½•ä¸€æ¬¡â†’æ‰¹é‡æŠ“å–ï¼‰ =====================
def fetch_all_product_info(links_file=None):
    """
    GEOX å•†å“æŠ“å–ä¸»å…¥å£ï¼ˆæ”¯æŒå¤–éƒ¨ä¼ å…¥ product_links.txt è¦†ç›– config é»˜è®¤è·¯å¾„ï¼‰ã€‚

    :param links_file: å¯é€‰ï¼Œè‡ªå®šä¹‰ product_links.txt è·¯å¾„ã€‚å¦‚æœä¸º None ï¼Œåˆ™ä½¿ç”¨ PRODUCT_LINK_FILEã€‚
    """
    # 1) è§£æ links æ–‡ä»¶è·¯å¾„
    if links_file is None:
        links_path = PRODUCT_LINK_FILE   # config é»˜è®¤
    else:
        links_path = Path(links_file)    # å…è®¸ str / Path

    if not links_path.exists():
        print(f"âŒ ç¼ºå°‘é“¾æ¥æ–‡ä»¶: {links_path}")
        return

    with open(links_path, "r", encoding="utf-8") as f:
        urls = [u.strip() for u in f if u.strip()]

    if not urls:
        print(f"âš ï¸ é“¾æ¥åˆ—è¡¨ä¸ºç©º: {links_path}")
        return

    # =========================
    # === ä¸»æµç¨‹ä¸å˜ï¼ˆç™»å½• â†’ æ‰¹é‡æŠ“å–ï¼‰===
    # =========================

    # 1) ç”¨ä½ çš„å®šåˆ¶ Chrome/å›ºå®š Profile æ‰“å¼€â€œå¯è§â€çª—å£ç™»å½•ä¸€æ¬¡
    login_driver = create_driver(headless=False)
    login_driver.get(urls[0])
    print(f"â³ å¦‚éœ€ç™»å½•ï¼Œè¯·åœ¨æ–°çª—å£æ‰‹åŠ¨ç™»å½• GEOXï¼ˆç­‰å¾… {LOGIN_WAIT_SECONDS} ç§’ï¼‰")
    time.sleep(LOGIN_WAIT_SECONDS)
    session = export_session(login_driver)
    login_driver.quit()

    # 2) åˆ›å»ºä¸€ä¸ªå¯å¤ç”¨çš„å·¥ä½œç”¨ driver
    driver = create_driver(headless=False)

    try:
        import_session(driver, session, base_url="https://www.geox.com/")

        for idx, url in enumerate(urls, 1):
            try:
                print(f"[{idx}/{len(urls)}] ğŸªŸ æ­£åœ¨æ‰“å¼€ï¼š{url}")
                driver.get(url)
                time.sleep(0.5)

                html = driver.page_source
                if not html:
                    print(f"[{idx}] âš ï¸ ç©ºé¡µé¢: {url}")
                    continue

                info = parse_product(html, url)
                if not info:
                    print(f"[{idx}] âš ï¸ è§£æå¤±è´¥: {url}")
                    continue

                txt_path = TXT_OUTPUT_DIR / f"{info['Product Code']}.txt"
                txt_path.parent.mkdir(parents=True, exist_ok=True)
                format_txt(info, txt_path, brand=BRAND)
                print(f"[{idx}] âœ… å†™å…¥æˆåŠŸ: {txt_path.name}")

                time.sleep(1)

            except Exception as e:
                print(f"[{idx}] âŒ å¤„ç†å¤±è´¥ {url} â†’ {e}")

    finally:
        driver.quit()

    print("\nâœ… å…¨éƒ¨å¤„ç†å®Œæˆã€‚")



if __name__ == "__main__":
    fetch_all_product_info()
