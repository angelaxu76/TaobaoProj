# -*- coding: utf-8 -*-
"""
ECCO å•†å“å›¾ç‰‡ä¸‹è½½ï¼ˆå…¼å®¹æ–°ç‰ˆç«™ç‚¹ï¼‰
- è§£æ <img>/<source> çš„ srcsetï¼ŒæŒ‘æœ€å¤§å°ºå¯¸
- ä»æ–‡ä»¶åè§£æ 6ä½æ¬¾å·+5ä½è‰²å·+è§†è§’ï¼Œè§„èŒƒå‘½åä¿å­˜
- å…¼å®¹ .png/.webp/.jpgï¼Œå»é‡é¿å…é‡å¤ä¸‹è½½
- å…¼å®¹ä» URL æˆ–å›¾ç‰‡åå›é€€æå–ç¼–ç 
- ä¿ç•™åŸæœ‰ pipeline å…¥å£å‡½æ•°ä¸å‚æ•°
"""

import time
import re
import requests
from pathlib import Path
from urllib.parse import urlparse, urlunparse

from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import WebDriverException

# === ä½ çš„é¡¹ç›®å†…é…ç½® ===
from config import ECCO, ensure_all_dirs

# ---------------- åŸºæœ¬é…ç½® ----------------
PRODUCT_LINKS_FILE = ECCO["BASE"] / "publication" / "product_links.txt"
IMAGE_DIR = ECCO["IMAGE_DOWNLOAD"]
WAIT = 0                # æ‰“å¼€é¡µé¢åçš„é™æ€ç­‰å¾…ï¼ˆè‹¥éœ€å¯è°ƒå¤§ï¼‰
DELAY = 0               # æ¯å¼ å›¾ä¸‹è½½çš„èŠ‚æµé—´éš”ï¼ˆç§’ï¼‰
SKIP_EXISTING_IMAGE = True
MAX_WORKERS = 5         # å¹¶å‘çº¿ç¨‹æ•°ï¼ˆå¯¹ URL ä»»åŠ¡çš„å¹¶å‘ï¼Œä¸æ˜¯å›¾ç‰‡ï¼‰

# ç¡®ä¿ç›®å½•å­˜åœ¨
ensure_all_dirs(IMAGE_DIR)

# ============== WebDriverï¼ˆç¨³ï¼‰=============
# å…ˆå°è¯• Selenium Managerï¼ˆæ— éœ€è‡ªå·±ç®¡ç†é©±åŠ¨ï¼‰ï¼Œå¤±è´¥å†å›é€€ webdriver-manager
try:
    from selenium.webdriver.chrome.service import Service
    from webdriver_manager.chrome import ChromeDriverManager
except Exception:
    ChromeDriverManager = None
    Service = None

def create_driver():
    opts = Options()
    for a in ["--headless=new", "--disable-gpu", "--no-sandbox",
              "--disable-dev-shm-usage", "--window-size=1920x1080"]:
        opts.add_argument(a)

    # A. Selenium Managerï¼ˆä¸ä¼  Service/driver è·¯å¾„ï¼‰
    try:
        return webdriver.Chrome(options=opts)
    except Exception as e:
        print(f"[WARN] Selenium Manager å¯åŠ¨å¤±è´¥ï¼š{e}")

    # B. webdriver-manager å›é€€
    if ChromeDriverManager and Service:
        try:
            driver_path = ChromeDriverManager().install()
            return webdriver.Chrome(service=Service(driver_path), options=opts)
        except Exception as e:
            raise RuntimeError("æ— æ³•åˆ›å»º Chrome WebDriverï¼ˆSelenium Manager ä¸ webdriver-manager å‡å¤±è´¥ï¼‰: " + str(e))

    raise RuntimeError("æ— æ³•åˆ›å»º Chrome WebDriverã€‚è¯·å®‰è£… selenium>=4.6ï¼›å¿…è¦æ—¶å®‰è£… webdriver-managerã€‚")

# ============== å·¥å…·å‡½æ•°ï¼šsrcset è§£æ/å‘½åè§„èŒƒ ==============
_VIEW_TOKEN = r"(?:o|m|b|s|top_left_pair|front_pair)"
_EXT_TOKEN  = r"(?:png|webp|jpg|jpeg)"

def _strip_query(url: str) -> str:
    """å»æ‰ URL æŸ¥è¯¢å‚æ•°ï¼ˆå‘½åæ—¶ç”¨ï¼‰ï¼Œä¸‹è½½å¯ä¿ç•™åŸå§‹ URLã€‚"""
    u = urlparse(url)
    return urlunparse(u._replace(query=""))

def _pick_largest_from_srcset(srcset: str) -> str | None:
    """ä» srcset å­—ç¬¦ä¸²ä¸­é€‰ width æœ€å¤§çš„ URLã€‚"""
    best_url, best_w = None, -1
    for part in srcset.split(","):
        part = part.strip()
        if not part:
            continue
        pieces = part.split()
        url = pieces[0]
        w = 0
        if len(pieces) > 1 and pieces[1].endswith("w"):
            try:
                w = int(pieces[1][:-1])
            except:
                w = 0
        if w > best_w:
            best_url, best_w = url, w
    return best_url

def _iter_image_candidate_urls(soup: BeautifulSoup):
    """éå†é¡µé¢ä¸­å¯èƒ½çš„å•†å“å›¾ URLï¼Œä¼˜å…ˆå– srcset çš„æœ€å¤§å°ºå¯¸ã€‚"""
    # <img>
    for img in soup.find_all("img"):
        srcset = img.get("srcset")
        if srcset:
            best = _pick_largest_from_srcset(srcset)
            if best:
                yield best
                continue
        for key in ("src", "data-src", "data-original"):
            if img.get(key):
                yield img.get(key)

    # <source>
    for tag in soup.find_all("source"):
        srcset = tag.get("srcset")
        if srcset:
            best = _pick_largest_from_srcset(srcset)
            if best:
                yield best

def _parse_code_view_from_filename(url: str) -> tuple[str | None, str | None, str | None, str]:
    """
    ä» URL æ–‡ä»¶åè§£æ (style6, color5, view, ext)
    å…¼å®¹ï¼š470824-51866-m_eCom.png / 470824-51866-top_left_pair.webp / 470824-51866-o.png ç­‰
    """
    no_q = _strip_query(url).lower()
    path = urlparse(no_q).path
    fname = Path(path).name  # e.g. 470824-51866-o_ecom.png

    # 1) -<view>_eCom.<ext>
    m = re.search(fr"(\d{{6}})-(\d{{5}})-({_VIEW_TOKEN})_ecom\.{_EXT_TOKEN}$", fname, flags=re.I)
    if not m:
        # 2) -<view>.<ext>
        m = re.search(fr"(\d{{6}})-(\d{{5}})-({_VIEW_TOKEN})\.{_EXT_TOKEN}$", fname, flags=re.I)

    if m:
        style6, color5, view = m.group(1), m.group(2), m.group(3).lower()
        ext = Path(path).suffix.lower()
        return style6, color5, view, ext

    # 3) åªè§£æ 6+5
    m2 = re.search(r"(\d{6})-(\d{5})", fname)
    ext = Path(path).suffix.lower()
    if m2:
        return m2.group(1), m2.group(2), None, ext

    return None, None, None, ext

def _normalize_save_name(url: str, fallback_code: str | None) -> tuple[str, str]:
    """
    ç”Ÿæˆä¿å­˜å (basename, ext)ã€‚ä¼˜å…ˆç”¨ 6+5+viewï¼›å¦åˆ™ç”¨ 6+5ï¼›å¦åˆ™ fallbackï¼›ä»æ— åˆ™ç”¨æ–‡ä»¶åå…œåº•ã€‚
    basename ä¸å«æ‰©å±•åï¼›ext ä»¥ URL å®é™…æ‰©å±•åä¸ºå‡†ï¼ˆ.png/.webp/.jpgï¼‰
    """
    style6, color5, view, ext = _parse_code_view_from_filename(url)
    if style6 and color5 and view:
        return f"{style6}{color5}_{view}", ext
    if style6 and color5:
        return f"{style6}{color5}", ext
    if fallback_code:
        return fallback_code, ext

    stem = Path(urlparse(_strip_query(url)).path).stem
    return stem.replace("-", "_"), ext

def _extract_code_from_url(u: str) -> str | None:
    """ä»æ–°ç‰ˆ URL /product/.../<6ä½>/<5ä½> æå–ç¼–ç """
    m = re.search(r'/(\d{6})/(\d{5})(?:[/?#]|$)', u)
    if m:
        return m.group(1) + m.group(2)
    return None

def _extract_code_from_images_html(html: str) -> str | None:
    """ä»æ•´é¡µ HTML ä¸­çš„å›¾ç‰‡æ–‡ä»¶åæå– 6+5 ç¼–ç ï¼ˆå›é€€ç”¨ï¼‰"""
    m = re.search(r'/(\d{6})-(\d{5})-(?:' + _VIEW_TOKEN + r')\.(?:' + _EXT_TOKEN + r')', html, flags=re.I)
    if m:
        return m.group(1) + m.group(2)
    m2 = re.search(r'/(\d{6})-(\d{5})\.', html)
    if m2:
        return m2.group(1) + m2.group(2)
    return None

# ============== ä¸‹è½½ä¸»é€»è¾‘ï¼ˆæ–°ç‰ˆè§£æï¼‰ ==============
def download_images_from_soup(soup: BeautifulSoup, formatted_code: str | None):
    """
    æ‰«æé¡µé¢çš„å•†å“å›¾ç‰‡å¹¶ä¸‹è½½ï¼š
    - ä¼˜å…ˆå– srcset æœ€å¤§å°ºå¯¸
    - è§„èŒƒå‘½åï¼ˆ6ä½æ¬¾å·+5ä½è‰²å·+è§†è§’ï¼‰
    - å»é‡ï¼ˆåŒå›¾ä¸åŒå°ºå¯¸åªå–ä¸€æ¬¡ï¼‰
    """
    seen_basenames = set()

    for raw_url in _iter_image_candidate_urls(soup):
        if not raw_url:
            continue

        # ä»…æ¥å—å›¾ç‰‡èµ„æº
        lower_url = raw_url.lower()
        if not any(ext in lower_url for ext in (".png", ".webp", ".jpg", ".jpeg")):
            continue

        # ç”Ÿæˆä¿å­˜å
        basename, ext = _normalize_save_name(raw_url, formatted_code)
        if basename in seen_basenames:
            continue
        seen_basenames.add(basename)

        save_path = IMAGE_DIR / f"{basename}{ext}"
        if SKIP_EXISTING_IMAGE and save_path.exists():
            print(f"âœ… è·³è¿‡: {save_path.name}")
            continue

        try:
            resp = requests.get(raw_url, timeout=20)
            resp.raise_for_status()
            save_path.write_bytes(resp.content)
            print(f"ğŸ–¼ï¸ ä¸‹è½½: {save_path.name}")
            time.sleep(DELAY)
        except Exception as e:
            print(f"âŒ ä¸‹è½½å¤±è´¥: {raw_url} - {e}")

# ============== å•é¡µå¤„ç†ï¼šä¿æŒåŸå‡½æ•°åä¸å‚æ•° ==============
def process_image_url(url):
    """
    æ‰“å¼€å•†å“é¡µ â†’ æå–ç¼–ç ï¼ˆURL / å›¾ç‰‡åå›é€€ï¼‰â†’ æ‰«æå¹¶ä¸‹è½½æ‰€æœ‰å•†å“å›¾
    """
    driver = None
    try:
        driver = create_driver()
        driver.get(url)
        time.sleep(WAIT)

        real_url = driver.current_url or url
        html = driver.page_source
        soup = BeautifulSoup(html, "html.parser")

        # è€ç«™ç‚¹çš„ DOMï¼ˆå…¼å®¹æ—§æ•°æ®æºï¼‰ï¼Œè‹¥å­˜åœ¨å°±ä¼˜å…ˆ
        formatted_code = None
        code_info = soup.find('div', class_='product_info__product-number')
        if code_info:
            try:
                text = code_info.text.strip()
                # æ—§é€»è¾‘ï¼šå½¢å¦‚ "... Product no. 47082451866 ..."
                digits = re.search(r'(\d{11})', text)
                if digits:
                    formatted_code = digits.group(1)
            except Exception:
                formatted_code = None

        # æ–°ç«™ç‚¹ï¼šä» URL æå– 6+5
        if not formatted_code:
            formatted_code = _extract_code_from_url(real_url)

        # å›é€€ï¼šä»é¡µé¢å›¾ç‰‡æ–‡ä»¶åä¸­æå– 6+5
        if not formatted_code:
            formatted_code = _extract_code_from_images_html(html)

        download_images_from_soup(soup, formatted_code)

    except Exception as e:
        print(f"âŒ å•†å“å¤„ç†å¤±è´¥: {url} - {e}")
    finally:
        if driver:
            driver.quit()

# ============== æ‰¹é‡å…¥å£ï¼šä¸åŸæ¥ä¿æŒä¸€è‡´ ==============
from concurrent.futures import ThreadPoolExecutor, as_completed

def main():
    if not PRODUCT_LINKS_FILE.exists():
        print(f"âŒ æœªæ‰¾åˆ°é“¾æ¥æ–‡ä»¶: {PRODUCT_LINKS_FILE}")
        return
    url_list = [u.strip() for u in PRODUCT_LINKS_FILE.read_text(encoding="utf-8").splitlines() if u.strip()]
    print(f"\nğŸ“¸ å¼€å§‹ä¸‹è½½ {len(url_list)} ä¸ªå•†å“çš„å›¾ç‰‡...")

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = [executor.submit(process_image_url, url) for url in url_list]
        for _ in as_completed(futures):
            pass

    print("\nâœ… æ‰€æœ‰å›¾ç‰‡ä¸‹è½½å®Œæˆã€‚")

# ============== æ ¹æ®ç¼–ç è¡¥å›¾ï¼šä¸åŸæ¥ä¿æŒä¸€è‡´ ==============
import psycopg2
from psycopg2.extras import RealDictCursor

def fetch_urls_from_db_by_codes(code_file_path, pgsql_config, table_name):
    code_list = [line.strip() for line in Path(code_file_path).read_text(encoding="utf-8").splitlines() if line.strip()]
    print(f"ğŸ” è¯»å–åˆ° {len(code_list)} ä¸ªç¼–ç ")

    urls = set()
    try:
        conn = psycopg2.connect(**pgsql_config)
        cursor = conn.cursor(cursor_factory=RealDictCursor)

        placeholders = ",".join(["%s"] * len(code_list))
        query = f"""
            SELECT DISTINCT product_code, product_url
            FROM {table_name}
            WHERE product_code IN ({placeholders})
        """
        cursor.execute(query, code_list)
        rows = cursor.fetchall()

        code_to_url = {row["product_code"]: row["product_url"] for row in rows}
        for code in code_list:
            url = code_to_url.get(code)
            if url:
                urls.add(url)
            else:
                print(f"âš ï¸ æœªæ‰¾åˆ°å•†å“ç¼–ç : {code}")

        cursor.close()
        conn.close()
    except Exception as e:
        print(f"âŒ æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {e}")

    return list(urls)

def download_images_by_code_file(code_txt_path):
    pgsql_config = ECCO["PGSQL_CONFIG"]
    table_name = ECCO["TABLE_NAME"]

    urls = fetch_urls_from_db_by_codes(code_txt_path, pgsql_config, table_name)
    print(f"ğŸ“¦ å…±éœ€å¤„ç† {len(urls)} ä¸ªå•†å“å›¾ç‰‡")

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = [executor.submit(process_image_url, url) for url in urls]
        for _ in as_completed(futures):
            pass

    print("\nâœ… æ‰€æœ‰è¡¥å›¾å®Œæˆ")

# ------------- è„šæœ¬ç›´æ¥è¿è¡Œæ—¶çš„é»˜è®¤å…¥å£ -------------
if __name__ == "__main__":
    # main()  # æ­£å¸¸ï¼šæŒ‰ product_links.txt æ‰¹é‡ä¸‹è½½

    # è¡¥å›¾æ¨¡å¼ï¼šæŒ‰ç¼–ç æ–‡ä»¶
    code_txt_path = ECCO["BASE"] / "publication" / "è¡¥å›¾ç¼–ç .txt"
    download_images_by_code_file(code_txt_path)
