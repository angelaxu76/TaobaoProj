import time
import re
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from config import ECCO, ensure_all_dirs
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed

# === é…ç½® ===
PRODUCT_LINKS_FILE = ECCO["BASE"] / "publication" / "product_links.txt"
IMAGE_DIR = ECCO["IMAGE_DIR"]
CHROMEDRIVER_PATH = "D:/Software/chromedriver-win64/chromedriver.exe"
WAIT = 0
DELAY = 0
SKIP_EXISTING_IMAGE = True
MAX_WORKERS = 5  # å¹¶å‘çº¿ç¨‹æ•°

# ç¡®ä¿ç›®å½•å­˜åœ¨
ensure_all_dirs(IMAGE_DIR)

def create_driver():
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--window-size=1920x1080")
    return webdriver.Chrome(service=Service(CHROMEDRIVER_PATH), options=chrome_options)

def download_images_from_soup(soup, formatted_code):
    for img in soup.select("div.product_details__media-item-img img"):
        if "src" not in img.attrs:
            continue
        img_url = img["src"].replace("DetailsMedium", "ProductDetailslarge3x")
        match = re.search(r'/([0-9A-Za-z-]+-(?:o|m|b|s|top_left_pair|front_pair))\.webp', img_url)
        img_code = match.group(1) if match else formatted_code
        img_path = IMAGE_DIR / f"{img_code}.webp"

        if SKIP_EXISTING_IMAGE and img_path.exists():
            print(f"âœ… è·³è¿‡: {img_path.name}")
            continue

        try:
            with open(img_path, "wb") as f:
                f.write(requests.get(img_url, timeout=10).content)
            print(f"ğŸ–¼ï¸ ä¸‹è½½: {img_path.name}")
            time.sleep(DELAY)
        except Exception as e:
            print(f"âŒ ä¸‹è½½å¤±è´¥: {img_url} - {e}")

def process_image_url(url):
    driver = None
    try:
        driver = create_driver()
        driver.get(url)
        time.sleep(WAIT)
        soup = BeautifulSoup(driver.page_source, "html.parser")

        code_info = soup.find('div', class_='product_info__product-number')
        if not code_info:
            print(f"âš ï¸ æœªæ‰¾åˆ°ç¼–ç ï¼Œè·³è¿‡: {url}")
            return

        product_code = code_info.text.strip().split()[2]
        code, color = product_code[:6], product_code[6:]
        formatted_code = f"{code}-{color}"

        download_images_from_soup(soup, formatted_code)

    except Exception as e:
        print(f"âŒ å•†å“å¤„ç†å¤±è´¥: {url} - {e}")
    finally:
        if driver:
            driver.quit()

def main():
    if not PRODUCT_LINKS_FILE.exists():
        print(f"âŒ æœªæ‰¾åˆ°é“¾æ¥æ–‡ä»¶: {PRODUCT_LINKS_FILE}")
        return
    url_list = [u.strip() for u in PRODUCT_LINKS_FILE.read_text(encoding="utf-8").splitlines() if u.strip()]
    print(f"\nğŸ“¸ å¼€å§‹ä¸‹è½½ {len(url_list)} ä¸ªå•†å“çš„å›¾ç‰‡...")

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = [executor.submit(process_image_url, url) for url in url_list]
        for future in as_completed(futures):
            pass

    print("\nâœ… æ‰€æœ‰å›¾ç‰‡ä¸‹è½½å®Œæˆã€‚")

# === æ–°å¢åŠŸèƒ½ï¼šæ ¹æ®å•†å“ç¼–ç ä¸‹è½½å›¾ç‰‡ ===
import psycopg2
from psycopg2.extras import RealDictCursor

def fetch_urls_from_db_by_codes(code_file_path, pgsql_config, table_name):
    code_list = [line.strip() for line in Path(code_file_path).read_text(encoding="utf-8").splitlines() if line.strip()]
    print(f"ğŸ” è¯»å–åˆ° {len(code_list)} ä¸ªç¼–ç ")

    urls = set()
    try:
        conn = psycopg2.connect(**pgsql_config)
        cursor = conn.cursor(cursor_factory=RealDictCursor)

        placeholders = ",".join(["%s"] * len(code_list))
        query = f"""
            SELECT DISTINCT product_code, product_url
            FROM {table_name}
            WHERE product_code IN ({placeholders})
        """
        cursor.execute(query, code_list)
        rows = cursor.fetchall()

        code_to_url = {row["product_code"]: row["product_url"] for row in rows}
        for code in code_list:
            url = code_to_url.get(code)
            if url:
                urls.add(url)
            else:
                print(f"âš ï¸ æœªæ‰¾åˆ°å•†å“ç¼–ç : {code}")

        cursor.close()
        conn.close()
    except Exception as e:
        print(f"âŒ æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {e}")

    return list(urls)

def download_images_by_code_file(code_txt_path):
    from config import ECCO
    pgsql_config = ECCO["PGSQL_CONFIG"]
    table_name = ECCO["TABLE_NAME"]

    urls = fetch_urls_from_db_by_codes(code_txt_path, pgsql_config, table_name)
    print(f"ğŸ“¦ å…±éœ€å¤„ç† {len(urls)} ä¸ªå•†å“å›¾ç‰‡")

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = [executor.submit(process_image_url, url) for url in urls]
        for future in as_completed(futures):
            pass

    print("\nâœ… æ‰€æœ‰è¡¥å›¾å®Œæˆ")

if __name__ == "__main__":
    # main()  # æ­£å¸¸å¤„ç† product_links.txt ä¸­å…¨éƒ¨é“¾æ¥

    # ğŸ‘‡ è¡¥å›¾æ¨¡å¼
    code_txt_path = ECCO["BASE"] / "publication" / "è¡¥å›¾ç¼–ç .txt"
    download_images_by_code_file(code_txt_path)