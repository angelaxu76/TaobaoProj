
import os
import re
import time
import json
import requests
from bs4 import BeautifulSoup
from pathlib import Path
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from concurrent.futures import ThreadPoolExecutor, as_completed
from config import ECCO, ensure_all_dirs
from common_taobao.txt_writer import format_txt  # ‚úÖ ‰ΩøÁî®Ê†áÂáÜÂÜôÂÖ•

# ========== ÂÖ®Â±ÄË∑ØÂæÑ & ÂèÇÊï∞ ==========
PRODUCT_LINKS_FILE = ECCO["BASE"] / "publication" / "product_links.txt"
TXT_DIR = ECCO["TXT_DIR"]
IMAGE_DIR = ECCO["IMAGE_DIR"]
CHROMEDRIVER_PATH = "D:/Software/chromedriver-win64/chromedriver.exe"
MAX_THREADS = 10
WAIT = 0
DELAY = 0.5

ensure_all_dirs(TXT_DIR, IMAGE_DIR)

DOWNLOAD_IMAGE = False
SKIP_EXISTING_IMAGE = True

def create_driver():
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--window-size=1920x1080")
    return webdriver.Chrome(service=Service(CHROMEDRIVER_PATH), options=chrome_options)

def size_to_eu(uk_size):
    uk_to_eu = {
        "2.5-3": "35", "3.5-4": "36", "4.5": "37", "5-5.5": "38",
        "6": "39", "6.5-7": "40", "7.5": "41", "8-8.5": "42",
        "9-9.5": "43", "10": "44", "10.5-11": "45", "11.5": "46"
    }
    return uk_to_eu.get(uk_size)

def extract_sizes_and_stock_status(html):
    soup = BeautifulSoup(html, "html.parser")
    size_div = soup.find("div", class_="size-picker__rows")
    if not size_div:
        return []
    results = []
    for btn in size_div.find_all("button"):
        uk_size = btn.text.strip()
        eu_size = size_to_eu(uk_size)
        if not eu_size:
            continue
        is_soldout = "size-picker__item--soldout" in btn.get("class", [])
        status = "Êó†Ë¥ß" if is_soldout else "ÊúâË¥ß"
        results.append(f"{eu_size}:{status}")
    return results

def extract_price_info(html):
    try:
        match = re.search(r'productdetailctrl\.onProductPageInit\((\{.*?\})\)', html, re.DOTALL)
        if not match:
            return 0.0, 0.0
        json_text = match.group(1).replace("&quot;", '"')
        data = json.loads(json_text)
        return data.get("Price", 0.0), data.get("AdjustedPrice", 0.0)
    except:
        return 0.0, 0.0

def process_product(url, idx, total):
    driver = None
    try:
        print(f"\nüîç ({idx}/{total}) Ê≠£Âú®Â§ÑÁêÜ: {url}")
        driver = create_driver()
        driver.get(url)
        time.sleep(WAIT)
        soup = BeautifulSoup(driver.page_source, "html.parser")

        code_info = soup.find('div', class_='product_info__product-number')
        if not code_info:
            raise Exception("Êú™ÊâæÂà∞ÁºñÁ†Å")
        product_code = code_info.text.strip().split()[2]
        code, color = product_code[:6], product_code[6:]
        #formatted_code = f"{code}-{color}"

        product_name = soup.select_one("span.product_info__intro-title").get_text(strip=True)
        description = soup.select_one("div.product-description").get_text(strip=True)
        color_name = soup.select_one("span.product_info__color--selected").get_text(strip=True)
        #gender = "women" if "women" in url.lower() else ("men" if "men" in url.lower() else "unisex")



        price, adjusted_price = extract_price_info(driver.page_source)
        material = "No Data"
        sizes = []
        try:
            sizes = extract_sizes_and_stock_status(driver.page_source)
        except Exception as e:
            print(f"‚ö†Ô∏è ÊèêÂèñÂ∞∫Á†ÅÂ§±Ë¥•: {e}")

        eu_sizes = [s.split(":")[0] for s in sizes if ":" in s]
        gender = "unisex"
        if any(s for s in eu_sizes if s.isdigit() and int(s) < 35):
            gender = "kids"
        elif any(s for s in eu_sizes if s in ("45", "46")):
            gender = "men"
        elif any(s for s in eu_sizes if s in ("35", "36")):
            gender = "women"

        info = {
            "Product Code": product_code,
            "Product Name": product_name,
            "Product Description": description,
            "Product Gender": gender,
            "Product Color": color_name,
            "Product Price": price,
            "Adjusted Price": adjusted_price,
            "Product Material": material,
            "Product Size": ";".join(sizes),
            "Source URL": url
        }

        filepath = TXT_DIR / f"{product_code}.txt"
        format_txt(info, filepath,brand="ecco")
        print(f"üìÑ ‰ø°ÊÅØ‰øùÂ≠ò: {filepath.name}")

        if DOWNLOAD_IMAGE:
            for img in soup.select("div.product_details__media-item-img img"):
                if "src" not in img.attrs:
                    continue
                img_url = img["src"].replace("DetailsMedium", "ProductDetailslarge3x")
                match = re.search(r'/([0-9A-Za-z-]+-(?:o|m|b|s|top_left_pair|front_pair))\.webp', img_url)
                img_code = match.group(1) if match else product_code
                img_path = IMAGE_DIR / f"{img_code}.webp"
                if SKIP_EXISTING_IMAGE and img_path.exists():
                    print(f"‚úÖ Â∑≤Â≠òÂú®ÂõæÁâáÔºåË∑≥Ëøá: {img_path.name}")
                    continue
                try:
                    with open(img_path, "wb") as f:
                        f.write(requests.get(img_url, timeout=10).content)
                    print(f"üñºÔ∏è ÂõæÁâá: {img_path.name}")
                    time.sleep(DELAY)
                except Exception as e:
                    print(f"‚ùå ÂõæÁâáÂ§±Ë¥•: {img_url} - {e}")

    except Exception as e:
        print(f"‚ùå ÈîôËØØ: {url} - {e}")
    finally:
        if driver:
            driver.quit()

def main():
    if not PRODUCT_LINKS_FILE.exists():
        print(f"‚ùå ÂïÜÂìÅÈìæÊé•Êñá‰ª∂‰∏çÂ≠òÂú®: {PRODUCT_LINKS_FILE}")
        return
    urls = [u.strip() for u in PRODUCT_LINKS_FILE.read_text(encoding="utf-8").splitlines() if u.strip()]
    total = len(urls)
    print(f"\nüì¶ ÂÖ± {total} Êù°ÂïÜÂìÅÔºåÁ∫øÁ®ãÊï∞: {MAX_THREADS}")
    with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:
        futures = [executor.submit(process_product, url, i + 1, total) for i, url in enumerate(urls)]
        for _ in as_completed(futures):
            pass
    print("\n‚úÖ ECCO ÂïÜÂìÅ‰ø°ÊÅØ‰∏éÂõæÁâáÂ§ÑÁêÜÂÆåÊØïÔºÅ")

if __name__ == "__main__":
    main()
