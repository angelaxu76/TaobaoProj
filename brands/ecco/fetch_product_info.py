# -*- coding: utf-8 -*-
"""
ECCO æŠ“å–ï¼ˆå…¨æ–°ç‹¬ç«‹ç‰ˆï¼‰
- è¯»å–å•†å“é“¾æ¥ï¼ˆæ”¯æŒ http(s) æˆ–æœ¬åœ° .htm æ–‡ä»¶ï¼‰
- æå–ï¼šProduct Code/Name/Description/Gender/Color/Price/Adjusted Price/Size/Material/Feature/Source URL
- å†™å‡º TXTï¼šä½¿ç”¨ clarks_jingya è§„èŒƒï¼ˆtxt_writer.format_txtï¼‰
- æŠ“å–ç­–ç•¥ï¼šrequests ä¼˜å…ˆï¼›å¦‚æŠ“ä¸åˆ°å…³é”®æ ‡é¢˜ï¼Œå†å›é€€ Seleniumï¼ˆå¯å…³é—­ï¼‰
"""
import os
import re
import json
import time
import traceback
import threading
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed

import requests
from bs4 import BeautifulSoup
from html import unescape

# ====== å¯æŒ‰éœ€ä¿®æ”¹ï¼šåŸºç¡€é…ç½® ======
LINKS_FILE = Path(r"D:/TB/Products/ecco/publication/product_links.txt")  # å•†å“é“¾æ¥åˆ—è¡¨ï¼Œæ¯è¡Œä¸€ä¸ª URL æˆ–æœ¬åœ° .htm æ–‡ä»¶è·¯å¾„
OUTPUT_DIR = Path(r"D:/TB/Products/ecco/publication/TXT")               # TXT è¾“å‡ºç›®å½•
IMAGE_DIR = Path(r"D:/TB/Products/ecco/publication/images")             # å›¾ç‰‡è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼‰
LOG_EVERY = 1

# ä¸‹è½½å›¾ç‰‡ï¼ˆå¯é€‰ï¼‰
DOWNLOAD_IMAGES = False
SKIP_EXISTING_IMAGE = True

# å¤šçº¿ç¨‹
MAX_WORKERS = 10
REQUEST_TIMEOUT = 20

# Selenium å›é€€å¼€å…³ä¸è·¯å¾„ï¼ˆéœ€è¦æ—¶æ‰ç”¨ï¼‰
ENABLE_SELENIUM_FALLBACK = True
CHROMEDRIVER_PATH = r"D:/Software/chromedriver-win64/chromedriver.exe"

# ====== ä¸è¦åŠ¨ï¼šWriter ======
# ä¾èµ–ä½ ç°æœ‰çš„ txt_writer.pyï¼ˆåŒä¸€å·¥ç¨‹ä¸­ï¼‰
from txt_writer import format_txt  # ç¡®ä¿è¯¥æ¨¡å—ä¸­æœ‰ format_txt(info, filepath, brand=...)

# ====== æè´¨å…³é”®è¯ï¼ˆå¯è¡¥å……ï¼‰======
MATERIAL_KEYWORDS = [
    "Leather", "GORE-TEX", "Gore-Tex", "Suede", "Nubuck", "Textile", "Fabric",
    "Canvas", "Mesh", "Synthetic", "Rubber", "PU", "TPU", "EVA", "Wool", "Neoprene"
]

# =============== åŸºç¡€å·¥å…· ===============
def ensure_dirs(*paths: Path):
    for p in paths:
        p.mkdir(parents=True, exist_ok=True)

def read_links(file: Path):
    if not file.exists():
        raise FileNotFoundError(f"é“¾æ¥æ–‡ä»¶ä¸å­˜åœ¨: {file}")
    lines = [x.strip() for x in file.read_text(encoding="utf-8").splitlines()]
    return [x for x in lines if x]

def clean_spaces(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "")).strip()

def unique_join(items, sep="; "):
    seen, out = set(), []
    for x in items:
        xx = clean_spaces(x)
        if not xx:
            continue
        low = xx.lower()
        if low in seen:
            continue
        seen.add(low)
        out.append(xx)
    return sep.join(out)

def is_url(s: str) -> bool:
    return s.startswith("http://") or s.startswith("https://")

# =============== ç½‘ç»œæŠ“å–ï¼ˆrequests / fallback seleniumï¼‰===============
def fetch_html(url_or_path: str) -> str:
    """ä¼˜å…ˆ requestsï¼›å¦‚æœæ˜¯æœ¬åœ°æ–‡ä»¶åˆ™ç›´æ¥è¯»å–ï¼›å¿…è¦æ—¶å›é€€ selenium"""
    if not is_url(url_or_path):
        # æœ¬åœ°æ–‡ä»¶
        p = Path(url_or_path)
        return p.read_text(encoding="utf-8", errors="ignore")

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                      "AppleWebKit/537.36 (KHTML, like Gecko) "
                      "Chrome/120.0.0.0 Safari/537.36"
    }
    try:
        r = requests.get(url_or_path, headers=headers, timeout=REQUEST_TIMEOUT)
        r.raise_for_status()
        html = r.text
        # å¦‚æœæ²¡æœ‰å…³é”®æ ‡é¢˜å—ï¼Œä¸”å…è®¸å›é€€ï¼Œåˆ™å°è¯• selenium
        if ENABLE_SELENIUM_FALLBACK and ('data-testid="product-card-titleandprice"' not in html):
            try:
                return fetch_html_by_selenium(url_or_path)
            except Exception:
                # selenium å¤±è´¥å°±è¿”å› requests çš„ç»“æœï¼ˆå¯èƒ½ä¹Ÿèƒ½è§£æï¼‰
                pass
        return html
    except Exception:
        if ENABLE_SELENIUM_FALLBACK:
            return fetch_html_by_selenium(url_or_path)
        raise

_selenium_lock = threading.Lock()
def fetch_html_by_selenium(url: str) -> str:
    from selenium import webdriver
    from selenium.webdriver.chrome.service import Service
    from selenium.webdriver.chrome.options import Options

    with _selenium_lock:  # é¿å…å¹¶å‘åˆå§‹åŒ–é©±åŠ¨
        chrome_options = Options()
        chrome_options.add_argument("--headless=new")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--window-size=1920,1080")

        service = Service(CHROMEDRIVER_PATH)
        driver = webdriver.Chrome(service=service, options=chrome_options)
    try:
        driver.get(url)
        time.sleep(1.0)  # è½»ç­‰å¾…ï¼Œå¿…è¦æ—¶å¯è°ƒ
        return driver.page_source
    finally:
        driver.quit()

# =============== é¡µé¢è§£æ ===============
def extract_title_block(soup: BeautifulSoup):
    """
    è§£æç»“æ„ï¼š
    <div data-testid="product-card-titleandprice">
      <h1>
        <p>Men's Leather Gore-Tex Trainer</p>
        ECCO Street 720
      </h1>
    </div>
    è¿”å› (subtitle, main_title)
    """
    h1 = soup.select_one('div[data-testid="product-card-titleandprice"] h1')
    if not h1:
        return "", ""
    p = h1.find("p")
    subtitle = clean_spaces(p.get_text(" ", strip=True)) if p else ""
    # h1 å†…â€œç›´æ¥æ–‡æœ¬èŠ‚ç‚¹â€ï¼ˆä¸å« <p> çš„ï¼‰é€šå¸¸æ˜¯ä¸»æ ‡é¢˜
    tail_nodes = [t for t in h1.find_all(string=True, recursive=False)]
    main_title = clean_spaces(tail_nodes[0]) if tail_nodes and clean_spaces(tail_nodes[0]) else ""
    return subtitle, main_title

def parse_gender_from_text(text: str) -> str:
    t = text.lower()
    if "women" in t or "womenâ€™s" in t or "women's" in t or "ladies" in t:
        return "women"
    if "men" in t or "menâ€™s" in t or "men's" in t:
        return "men"
    if "kid" in t or "junior" in t or "youth" in t:
        return "kids"
    return ""

def parse_materials_from_text(text: str):
    found = []
    for kw in MATERIAL_KEYWORDS:
        if re.search(rf'(?<!\w){re.escape(kw)}(?!\w)', text, re.IGNORECASE):
            # è§„èŒƒæ˜¾ç¤ºï¼ˆä¿æŒ GORE-TEX åŸæ ·ï¼‰
            norm = kw if kw.isupper() else kw.title()
            found.append(norm)
    # å»é‡ä¿åºï¼šæŒ‰å‡ºç°ä½ç½®æ’åº
    idxs = {m: text.lower().find(m.lower()) for m in found}
    found_sorted = sorted(set(found), key=lambda m: idxs[m])
    return found_sorted

def extract_description(soup: BeautifulSoup) -> str:
    # å…ˆä» JSON-LD å–
    for script in soup.find_all("script", {"type": "application/ld+json"}):
        try:
            data = json.loads(script.string or "[]")
            if isinstance(data, list):
                for item in data:
                    desc = item.get("description", "")
                    if desc:
                        text = re.sub(r"<[^>]+>", " ", desc)
                        return clean_spaces(unescape(text))
            elif isinstance(data, dict):
                desc = data.get("description", "")
                if desc:
                    text = re.sub(r"<[^>]+>", " ", desc)
                    return clean_spaces(unescape(text))
        except Exception:
            continue
    # å›é€€ï¼šå¯è§æè¿°å®¹å™¨
    node = soup.select_one("div.product-description")
    return clean_spaces(node.get_text(" ", strip=True)) if node else ""

def extract_features(soup: BeautifulSoup) -> str:
    items = []
    for li in soup.select("div.about-this-product__container div.product-description-list ul li"):
        txt = clean_spaces(li.get_text(" ", strip=True))
        if txt:
            items.append(txt)
    return " | ".join(items)

def extract_price(html: str, soup: BeautifulSoup):
    """
    ä¼˜å…ˆè§£æ onProductPageInit(...)ï¼Œå¦åˆ™å›é€€ JSON-LD offers
    """
    # 1) JS é’©å­
    try:
        m = re.search(r'productdetailctrl\.onProductPageInit\((\{.*?\})\)', html, re.DOTALL)
        if m:
            js = m.group(1).replace("&quot;", '"')
            data = json.loads(js)
            price = float(data.get("Price", 0) or 0)
            adj = float(data.get("AdjustedPrice", 0) or 0)
            return price, adj
    except Exception:
        pass
    # 2) JSON-LD offers
    try:
        for script in soup.find_all("script", {"type": "application/ld+json"}):
            data = json.loads(script.string or "{}")
            if isinstance(data, dict):
                offers = data.get("offers", {})
                if isinstance(offers, dict):
                    p = float(offers.get("price", 0) or 0)
                    return p, 0.0
    except Exception:
        pass
    return 0.0, 0.0

def extract_sizes_and_stock(html: str, soup: BeautifulSoup):
    """
    è¿”å› ["41:æœ‰è´§","42:æ— è´§",...]
    """
    # ä¼˜å…ˆ DOM
    size_div = soup.find("div", class_="size-picker__rows")
    results = []
    if size_div:
        for btn in size_div.find_all("button"):
            label = clean_spaces(btn.get_text(" ", strip=True))
            if not label:
                continue
            # ä½ åŸå…ˆé€»è¾‘ï¼šç¤ºä¾‹é‡ŒæŒ‰é’®æ–‡å­—å¯èƒ½æ˜¯ UK æˆ–èŒƒå›´ï¼Œè¿™é‡Œç›´æ¥æŠŠ label ä½œä¸º EU æ¨æ–­ä¸å¯é 
            # ç®€åŒ–ï¼šå¦‚æœæŒ‰é’®å†™çš„å°±æ˜¯ EUï¼ˆå¸¸è§ï¼‰ï¼Œç›´æ¥ç”¨ï¼›å¦åˆ™å°è¯•æ•°å­—æå–
            eu = re.findall(r"\d{2}", label)
            eu_size = eu[0] if eu else label
            classes = btn.get("class", [])
            soldout = any("soldout" in c.lower() for c in classes)
            status = "æ— è´§" if soldout else "æœ‰è´§"
            results.append(f"{eu_size}:{status}")
        if results:
            return results
    # å›é€€ï¼šä» html é‡ŒçŒœï¼ˆå¼±åŒ–ï¼‰
    for m in re.finditer(r'>(\d{2})<', html):
        sz = m.group(1)
        if f"{sz}:" not in ";".join(results):
            results.append(f"{sz}:æœ‰è´§")
    return results

def extract_product_code_color(soup: BeautifulSoup):
    """
    é¡µé¢ä¸Šé€šå¸¸æœ‰ç±»ä¼¼ï¼š
    <div class="product_info__product-number">Product number: 069563 50034</div>
    """
    node = soup.find("div", class_="product_info__product-number")
    if not node:
        raise RuntimeError("æœªæ‰¾åˆ°å•†å“ç¼–ç ")
    text = clean_spaces(node.get_text(" ", strip=True))
    # å°è¯•ä»æœ€åä¸¤æ®µè¿ç»­æ•°å­—é‡Œæ‹¼èµ·æ¥
    nums = re.findall(r"(\d{5,6})", text.replace(" ", ""))
    # æœ‰äº›é¡µé¢å†™æˆ 06956350034 ä¸€ä¸²ï¼ˆ6+5ï¼‰
    if not nums:
        nums = re.findall(r"(\d+)", text)
    joined = "".join(nums)
    # å…œåº•ï¼šå»éæ•°å­—
    joined = re.sub(r"\D+", "", joined)
    if len(joined) < 8:
        # æœ€å°‘ä¹Ÿè¯¥æœ‰ 6+5 ä½
        raise RuntimeError(f"ç¼–ç æ ¼å¼å¼‚å¸¸: {text}")
    # e.g. 06956350034 => code:069563, color:50034
    code = joined[:6]
    color = joined[6:]
    return code + color, code, color

def extract_color_name(soup: BeautifulSoup) -> str:
    node = soup.select_one("span.product_info__color--selected")
    if node:
        return clean_spaces(node.get_text(" ", strip=True))
    # å¯èƒ½åœ¨æ ‡é¢˜å°¾éƒ¨æˆ–å˜ä½“é€‰æ‹©é‡Œï¼›ç¼ºå¤±åˆ™ No Data
    return "No Data"

def decide_gender(gender_by_size: str, gender_from_title: str) -> str:
    if gender_from_title in ("men", "women", "kids"):
        return gender_from_title
    if gender_by_size in ("men", "women", "kids"):
        return gender_by_size
    return "unisex"

# =============== ä¸»æµç¨‹ ===============
def process_one(url: str, idx: int, total: int):
    try:
        html = fetch_html(url)
        soup = BeautifulSoup(html, "html.parser")

        # --- ç¼–ç /é¢œè‰²ç  ---
        product_code_full, code6, color5 = extract_product_code_color(soup)
        color_name = extract_color_name(soup)

        # --- æ ‡é¢˜ä¸¤æ®µ ---
        subtitle, main_title = extract_title_block(soup)
        # æ—§ç»“æ„å…œåº•ï¼ˆè€ç«™çš„ intro-titleï¼‰
        if not (subtitle or main_title):
            legacy = soup.select_one("span.product_info__intro-title")
            main_title = clean_spaces(legacy.get_text(" ", strip=True)) if legacy else main_title

        # åˆå¹¶ Product Name
        name_parts = [x for x in [subtitle, main_title] if x]
        product_name = " | ".join(name_parts) if name_parts else "No Data"

        # --- æè¿°/è¦ç‚¹ ---
        description = extract_description(soup)
        feature = extract_features(soup)

        # --- æè´¨ ---
        materials = []
        materials += parse_materials_from_text(subtitle + " " + main_title)
        # é€‚åº¦ä»æè¿°è¡¥å……ï¼ˆå¯èƒ½åŒ…å« outsole/lining ç­‰è¯ï¼ŒæŒ‰éœ€å–èˆï¼‰
        if description:
            materials += parse_materials_from_text(description)
        material = unique_join(materials) if materials else "No Data"

        # --- ä»·æ ¼ ---
        price, adjusted = extract_price(html, soup)

        # --- å°ºç  + æ€§åˆ«æ¨æ–­ ---
        sizes = extract_sizes_and_stock(html, soup)  # ["41:æœ‰è´§","42:æ— è´§"]
        eu_sizes = [s.split(":")[0] for s in sizes if ":" in s]
        gender_by_size = "unisex"
        if any(s.isdigit() and int(s) < 35 for s in eu_sizes):
            gender_by_size = "kids"
        elif any(s in ("45", "46") for s in eu_sizes):
            gender_by_size = "men"
        elif any(s in ("35", "36") for s in eu_sizes):
            gender_by_size = "women"

        gender_from_title = parse_gender_from_text(subtitle + " " + main_title)
        gender = decide_gender(gender_by_size, gender_from_title)

        # --- ç»„ç»‡å†™å…¥ ---
        info = {
            "Product Code": product_code_full,       # ä¾‹å¦‚ 06956350034
            "Product Name": product_name,
            "Product Description": description,
            "Product Gender": gender,
            "Product Color": color_name,
            "Product Price": price,
            "Adjusted Price": adjusted,
            "Product Material": material,
            "Product Size": ";".join(sizes),
            "Feature": feature,
            "Source URL": url
        }

        ensure_dirs(OUTPUT_DIR)
        out_path = OUTPUT_DIR / f"{product_code_full}.txt"
        # å…³é”®ï¼šæŒ‰ clarks_jingya è§„èŒƒå†™å…¥
        format_txt(info, out_path, brand="clarks_jingya")
        if idx % LOG_EVERY == 0:
            print(f"[{idx}/{total}] âœ… TXT å†™å…¥ï¼š{out_path.name}")

        # --- å›¾ç‰‡ï¼ˆå¯é€‰ï¼‰---
        if DOWNLOAD_IMAGES:
            ensure_dirs(IMAGE_DIR)
            # ç¤ºä¾‹ï¼šæ”¾å¤§è§„åˆ™ï¼Œå¦‚éœ€å…¼å®¹ç«™ç‚¹å‘½åå¯è‡ªè¡Œæ‰©å±•
            for img in soup.select("div.product_details__media-item-img img"):
                src = img.get("src") or ""
                if not src:
                    continue
                # å°è¯•æ›¿æ¢æ›´é«˜æ¸…å°ºå¯¸å…³é”®è¯ï¼ˆæŒ‰ä½ çš„ä¹ æƒ¯ï¼‰
                src_hd = src.replace("DetailsMedium", "ProductDetailslarge3x")
                # æ„å»ºæ–‡ä»¶åï¼šç”¨å®Œæ•´ product_code å‰ç¼€ + é¡ºåº
                # è‹¥ URL è‡ªå¸¦è§„èŒƒåä¹Ÿå¯æ²¿ç”¨
                pic_name = Path(src_hd.split("?")[0]).name
                pic_path = IMAGE_DIR / pic_name
                if SKIP_EXISTING_IMAGE and pic_path.exists():
                    continue
                try:
                    r = requests.get(src_hd, timeout=REQUEST_TIMEOUT)
                    r.raise_for_status()
                    pic_path.write_bytes(r.content)
                except Exception as e:
                    print(f"âš ï¸ å›¾ç‰‡å¤±è´¥ï¼š{src_hd} - {e}")

    except Exception as e:
        print(f"[{idx}/{total}] âŒ å‡ºé”™ï¼š{url}\n{e}\n{traceback.format_exc()}")

def main():
    ensure_dirs(OUTPUT_DIR, IMAGE_DIR)
    links = read_links(LINKS_FILE)
    total = len(links)
    print(f"ğŸ“¦ å¾…å¤„ç† {total} ä¸ªé“¾æ¥ï¼Œçº¿ç¨‹ {MAX_WORKERS}ï¼ˆSelenium å›é€€ï¼š{ENABLE_SELENIUM_FALLBACK}ï¼‰")
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:
        futures = [ex.submit(process_one, url, i+1, total) for i, url in enumerate(links)]
        for _ in as_completed(futures):
            pass
    print("âœ… å…¨éƒ¨å¤„ç†å®Œæˆã€‚")

if __name__ == "__main__":
    main()
