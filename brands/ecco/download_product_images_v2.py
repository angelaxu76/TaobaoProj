# -*- coding: utf-8 -*-
"""
ECCO å•†å“å›¾ç‰‡ä¸‹è½½ V2ï¼ˆç¼–ç è¿‡æ»¤ + é˜²åå›¾ + å…¼å®¹æ–°æ—§ eComï¼‰
åŸºäºä½ ç°æœ‰è„šæœ¬å‡çº§ï¼š:contentReference[oaicite:1]{index=1}

æ ¸å¿ƒç‰¹æ€§ï¼š
1) ç¼–ç è¿‡æ»¤ï¼šåªä¸‹è½½ URL path ä¸­åŒ…å« 6-5 æˆ– 11 ä½ç¼–ç çš„å›¾ç‰‡ï¼ˆé¿å…æ¨èåŒºæ··å…¥ï¼‰
2) å…¼å®¹æ–°æ—§å‘½åï¼š-view_eCom.ext / -view.ext / png/webp/jpg/jpeg
3) é˜²åå›¾ï¼šæ ¡éªŒ Content-Type ä¸º image/* + magic bytesï¼ˆpng/jpg/webpï¼‰
4) åŒè§†è§’å»é‡ï¼šæŒ‰ view å»é‡ï¼ŒåŒè§†è§’ä¼˜å…ˆ eComï¼Œå…¶æ¬¡ä¼˜å…ˆ width æ›´å¤§çš„é“¾æ¥
"""

import time
import re
import requests
from pathlib import Path
from urllib.parse import urlparse, urlunparse

from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options

# === ä½ çš„é¡¹ç›®å†…é…ç½® ===
from config import ECCO, ensure_all_dirs

# ---------------- åŸºæœ¬é…ç½® ----------------
PRODUCT_LINKS_FILE = ECCO["BASE"] / "publication" / "product_links.txt"
IMAGE_DIR = ECCO["IMAGE_DOWNLOAD"]
WAIT = 0
DELAY = 0
SKIP_EXISTING_IMAGE = True
MAX_WORKERS = 5

ensure_all_dirs(IMAGE_DIR)

# ============== WebDriverï¼ˆç¨³ï¼‰=============
try:
    from selenium.webdriver.chrome.service import Service
    from webdriver_manager.chrome import ChromeDriverManager
except Exception:
    ChromeDriverManager = None
    Service = None


def create_driver():
    opts = Options()
    for a in ["--headless=new", "--disable-gpu", "--no-sandbox",
              "--disable-dev-shm-usage", "--window-size=1920x1080"]:
        opts.add_argument(a)

    # A. Selenium Manager
    try:
        return webdriver.Chrome(options=opts)
    except Exception as e:
        print(f"[WARN] Selenium Manager å¯åŠ¨å¤±è´¥ï¼š{e}")

    # B. webdriver-manager å›é€€
    if ChromeDriverManager and Service:
        driver_path = ChromeDriverManager().install()
        return webdriver.Chrome(service=Service(driver_path), options=opts)

    raise RuntimeError("æ— æ³•åˆ›å»º Chrome WebDriverã€‚è¯·å®‰è£… selenium>=4.6ï¼›å¿…è¦æ—¶å®‰è£… webdriver-managerã€‚")


# ============== å·¥å…·å‡½æ•°ï¼šsrcset / è§£æ / è¿‡æ»¤ ==============
_VIEW_TOKEN = r"(?:o|m|b|s|top_left_pair|front_pair)"
_EXT_TOKEN  = r"(?:png|webp|jpg|jpeg)"


def _strip_query(url: str) -> str:
    u = urlparse(url)
    return urlunparse(u._replace(query=""))


def _fix_url(u: str) -> str | None:
    if not u:
        return None
    u = u.strip().replace("\n", "").replace("\r", "")
    if not u:
        return None
    if u.startswith("//"):
        return "https:" + u
    return u



def _looks_like_image_url(u: str) -> bool:
    if not u:
        return False
    lu = u.lower()
    return any(ext in lu for ext in (".png", ".webp", ".jpg", ".jpeg"))


def _pick_largest_from_srcset(srcset: str) -> str | None:
    best_url, best_w = None, -1
    for part in srcset.split(","):
        part = part.strip()
        if not part:
            continue
        pieces = part.split()
        url = pieces[0]
        w = 0
        if len(pieces) > 1 and pieces[1].endswith("w"):
            try:
                w = int(pieces[1][:-1])
            except Exception:
                w = 0
        if w > best_w:
            best_url, best_w = url, w
    return best_url


def _iter_image_candidate_urls(soup: BeautifulSoup):
    """
    å€™é€‰å›¾ç‰‡ URLï¼š
    - img[src/srcset/data-src/data-original/data-srcset]
    - source[srcset/data-srcset]
    - a[href]ï¼ˆæ–°ç«™ eCom å›¾ç‰‡ç›´é“¾å¸¸åœ¨è¿™é‡Œï¼‰
    """
    # <img>
    for img in soup.find_all("img"):
        for k in ("srcset", "data-srcset"):
            srcset = img.get(k)
            if srcset:
                best = _pick_largest_from_srcset(srcset)
                best = _fix_url(best)
                if best and _looks_like_image_url(best):
                    yield best

        for k in ("src", "data-src", "data-original"):
            u = _fix_url(img.get(k))
            if u and _looks_like_image_url(u):
                yield u

    # <source>
    for tag in soup.find_all("source"):
        for k in ("srcset", "data-srcset"):
            srcset = tag.get(k)
            if srcset:
                best = _pick_largest_from_srcset(srcset)
                best = _fix_url(best)
                if best and _looks_like_image_url(best):
                    yield best

    # <a href>
    for a in soup.find_all("a"):
        href = _fix_url(a.get("href"))
        if href and _looks_like_image_url(href):
            yield href


def _parse_code_view_from_filename(url: str) -> tuple[str | None, str | None, str | None, str]:
    """
    ä» URL æ–‡ä»¶åè§£æ (style6, color5, view, ext)
    å…¼å®¹ï¼š
    - 835414-02308-m_eCom.png
    - 835414-02308-m.png
    - 835414-02308-top_left_pair_eCom.webp
    """
    no_q = _strip_query(url).lower()
    path = urlparse(no_q).path
    fname = Path(path).name

    # 1) -<view>_ecom.<ext>
    m = re.search(fr"(\d{{6}})-(\d{{5}})-({_VIEW_TOKEN})_ecom\.{_EXT_TOKEN}$", fname, flags=re.I)
    if not m:
        # 2) -<view>.<ext>
        m = re.search(fr"(\d{{6}})-(\d{{5}})-({_VIEW_TOKEN})\.{_EXT_TOKEN}$", fname, flags=re.I)

    if m:
        style6, color5, view = m.group(1), m.group(2), m.group(3).lower()
        ext = Path(path).suffix.lower()
        return style6, color5, view, ext

    # 3) åªè§£æ 6+5
    m2 = re.search(r"(\d{6})-(\d{5})", fname)
    ext = Path(path).suffix.lower()
    if m2:
        return m2.group(1), m2.group(2), None, ext

    return None, None, None, Path(path).suffix.lower()


def _normalize_save_name(url: str, fallback_code: str | None) -> str:
    """
    ç”Ÿæˆ basenameï¼ˆä¸å«æ‰©å±•åï¼‰
    - ä¼˜å…ˆ 6+5+view -> 83541402308_m
    - æ¬¡é€‰ 6+5     -> 83541402308
    - å† fallback  -> 83541402308
    - å…œåº• stem
    """
    style6, color5, view, _ = _parse_code_view_from_filename(url)
    if style6 and color5 and view:
        return f"{style6}{color5}_{view}"
    if style6 and color5:
        return f"{style6}{color5}"
    if fallback_code:
        return fallback_code

    stem = Path(urlparse(_strip_query(url)).path).stem
    return stem.replace("-", "_")


def _extract_code_from_url(u: str) -> str | None:
    """ä» URL /product/.../<6ä½>/<5ä½> æå– 11 ä½ç¼–ç """
    m = re.search(r'/(\d{6})/(\d{5})(?:[/?#]|$)', u)
    if m:
        return m.group(1) + m.group(2)
    return None


def _extract_code_from_images_html(html: str) -> str | None:
    """ä»é¡µé¢ HTML çš„å›¾ç‰‡æ–‡ä»¶åæå–ç¼–ç ï¼ˆå›é€€ï¼‰"""
    m = re.search(r'(\d{6})-(\d{5})', html, flags=re.I)
    if m:
        return m.group(1) + m.group(2)
    m2 = re.search(r'(\d{11})', html)
    if m2:
        return m2.group(1)
    return None


def _code_patterns(formatted_code: str) -> tuple[str, str]:
    """
    11ä½ç¼–ç  83541402308 -> ("835414-02308", "83541402308")
    """
    if not formatted_code or len(formatted_code) != 11:
        return ("", "")
    dashed = formatted_code[:6] + "-" + formatted_code[6:]
    return dashed, formatted_code


def _url_contains_code(url: str, formatted_code: str | None) -> bool:
    """
    ç¼–ç è¿‡æ»¤ï¼šå›¾ç‰‡ URLï¼ˆå»æ‰ queryï¼‰å¿…é¡»åŒ…å« 835414-02308 æˆ– 83541402308
    å¦‚æœæ‹¿ä¸åˆ° formatted_codeï¼Œå°±ä¸å¼ºè¡Œè¿‡æ»¤ï¼ˆé¿å…æ¼å›¾ï¼‰
    """
    if not formatted_code or len(formatted_code) != 11:
        return True
    dashed, plain = _code_patterns(formatted_code)
    path = urlparse(_strip_query(url)).path.lower()
    return (dashed.lower() in path) or (plain.lower() in path)


def _get_width_param(url: str) -> int:
    """ä» query é‡Œæå– width å‚æ•°ç”¨äºé€‰å¤§å›¾ï¼ˆæ²¡æœ‰å°± 0ï¼‰"""
    qs = (urlparse(url).query or "").lower()
    m = re.search(r"(?:^|&)(?:width|w)=(\d+)(?:&|$)", qs)
    if m:
        try:
            return int(m.group(1))
        except Exception:
            return 0
    return 0


def _is_ecom_url(url: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦ eCom ç‰ˆæœ¬é“¾æ¥"""
    p = _strip_query(url).lower()
    return "_ecom" in p


# ============== requests ä¸‹è½½ï¼ˆé˜²åå›¾ï¼‰ ==============
_SESSION = requests.Session()
_DEFAULT_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                  "(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Referer": "https://gb.ecco.com/",
    "Accept": "image/png,image/webp,image/jpeg,image/*,*/*;q=0.8",
}


def _detect_image_ext(data: bytes) -> str | None:
    if not data or len(data) < 16:
        return None
    if data.startswith(b"\x89PNG\r\n\x1a\n"):
        return ".png"
    if data.startswith(b"\xff\xd8\xff"):
        return ".jpg"
    if data[0:4] == b"RIFF" and data[8:12] == b"WEBP":
        return ".webp"
    # AVIF/HEIF: ....ftypavif / ftypheic / ftypmif1 ç­‰
    if b"ftypavif" in data[:64] or b"ftypheic" in data[:64] or b"ftypmif1" in data[:64]:
        return ".avif"
    return None



def _download_bytes(url: str, timeout=25, retries=2) -> tuple[bytes | None, str | None]:
    """
    è¿”å› (data, real_ext)ï¼š
    - data ä¸ºå›¾ç‰‡äºŒè¿›åˆ¶ï¼›å¤±è´¥è¿”å› (None, None)
    - real_ext é€šè¿‡ magic bytes åˆ¤æ–­ .png/.jpg/.webp
    """
    last_err = None
    for i in range(retries + 1):
        try:
            url = _force_image_format(url, "png")
            resp = _SESSION.get(url, headers=_DEFAULT_HEADERS, timeout=timeout, allow_redirects=True)

            ct = (resp.headers.get("Content-Type") or "").lower()
            data = resp.content or b""

            if not ct.startswith("image/"):
                raise RuntimeError(f"Not image content-type: {ct}")

            real_ext = _detect_image_ext(data)
            if not real_ext:
                raise RuntimeError("Invalid image magic bytes")

            return data, real_ext

        except Exception as e:
            last_err = e
            if i < retries:
                time.sleep(0.6 * (i + 1))

    print(f"âŒ ä¸‹è½½å¤±è´¥(é‡è¯•åä»å¤±è´¥): {url} - {last_err}")
    return None, None


# ============== ä¸‹è½½ä¸»é€»è¾‘ï¼ˆç¼–ç è¿‡æ»¤ + é€‰å¤§å›¾ + eCom ä¼˜å…ˆï¼‰ ==============
def download_images_from_soup(soup: BeautifulSoup, formatted_code: str | None):
    """
    - å…ˆç”¨ç¼–ç è¿‡æ»¤ï¼šåªè¦ URL path ä¸åŒ…å«è¯¥ç¼–ç ï¼Œå°±è·³è¿‡
    - å†æŒ‰ view å»é‡ï¼šåŒè§†è§’åªä¿ç•™ä¸€ä¸ª
      - ä¼˜å…ˆ eCom
      - eCom ç›¸åŒä¼˜å…ˆçº§ä¸‹ï¼Œé€‰ width æ›´å¤§çš„
    - æœ€åæ‰ä¸‹è½½å¹¶ä¿å­˜ï¼ˆå¹¶æ ¡éªŒæ˜¯çœŸå›¾ç‰‡ï¼‰
    """
    # view_key -> (priority, width, url)
    # priority: eCom=2, normal=1
    best_by_view: dict[str, tuple[int, int, str]] = {}

    for raw in _iter_image_candidate_urls(soup):
        if not raw:
            continue

        raw_url = _fix_url(raw)
        if not raw_url:
            continue
        if not _looks_like_image_url(raw_url):
            continue

        # âœ… ç¼–ç è¿‡æ»¤ï¼ˆæœ€å…³é”®ï¼‰
        if not _url_contains_code(raw_url, formatted_code):
            continue

        style6, color5, view, _ = _parse_code_view_from_filename(raw_url)
        view_key = view or "unknown"

        pri = 2 if _is_ecom_url(raw_url) else 1
        w = _get_width_param(raw_url)

        if view_key not in best_by_view:
            best_by_view[view_key] = (pri, w, raw_url)
        else:
            old_pri, old_w, _old_url = best_by_view[view_key]
            # eCom ä¼˜å…ˆï¼›åŒä¼˜å…ˆçº§å– width æ›´å¤§
            if pri > old_pri or (pri == old_pri and w > old_w):
                best_by_view[view_key] = (pri, w, raw_url)

    # ä¾æ¬¡ä¸‹è½½æœ€ä½³ URL
    for view_key, (_pri, _w, url) in best_by_view.items():
        basename = _normalize_save_name(url, formatted_code)

        # unknown å…œåº•é¿å…è¦†ç›–
        if view_key == "unknown" and formatted_code:
            basename = f"{formatted_code}_unknown"

        data, real_ext = _download_bytes(url)
        if not data:
            continue

        save_path = IMAGE_DIR / f"{basename}{real_ext}"
        if SKIP_EXISTING_IMAGE and save_path.exists():
            print(f"âœ… è·³è¿‡: {save_path.name}")
            continue

        save_path.write_bytes(data)
        print(f"ğŸ–¼ï¸ ä¸‹è½½: {save_path.name}")
        time.sleep(DELAY)


# ============== å•é¡µå¤„ç†ï¼šä¿æŒåŸå‡½æ•°åä¸å‚æ•° ==============
def process_image_url(url: str):
    driver = None
    try:
        driver = create_driver()
        driver.get(url)
        time.sleep(WAIT)

        real_url = driver.current_url or url
        html = driver.page_source
        soup = BeautifulSoup(html, "html.parser")

        formatted_code = None

        # æ—§ç«™ç‚¹ DOMï¼ˆå¦‚æœå­˜åœ¨ 11 ä½ç¼–ç ï¼‰
        code_info = soup.find('div', class_='product_info__product-number')
        if code_info:
            text = (code_info.text or "").strip()
            m = re.search(r'(\d{11})', text)
            if m:
                formatted_code = m.group(1)

        # æ–°ç«™ç‚¹ï¼šä» URL æå– 6+5
        if not formatted_code:
            formatted_code = _extract_code_from_url(real_url)

        # å›é€€ï¼šä»é¡µé¢å›¾ç‰‡æ–‡ä»¶åæå–
        if not formatted_code:
            formatted_code = _extract_code_from_images_html(html)

        download_images_from_soup(soup, formatted_code)

    except Exception as e:
        print(f"âŒ å•†å“å¤„ç†å¤±è´¥: {url} - {e}")
    finally:
        if driver:
            driver.quit()


# ============== æ‰¹é‡å…¥å£ï¼šä¸åŸæ¥ä¿æŒä¸€è‡´ ==============
from concurrent.futures import ThreadPoolExecutor, as_completed

def main():
    if not PRODUCT_LINKS_FILE.exists():
        print(f"âŒ æœªæ‰¾åˆ°é“¾æ¥æ–‡ä»¶: {PRODUCT_LINKS_FILE}")
        return
    url_list = [u.strip() for u in PRODUCT_LINKS_FILE.read_text(encoding="utf-8").splitlines() if u.strip()]
    print(f"\nğŸ“¸ å¼€å§‹ä¸‹è½½ {len(url_list)} ä¸ªå•†å“çš„å›¾ç‰‡...")

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = [executor.submit(process_image_url, url) for url in url_list]
        for _ in as_completed(futures):
            pass

    print("\nâœ… æ‰€æœ‰å›¾ç‰‡ä¸‹è½½å®Œæˆã€‚")


from urllib.parse import urlencode, parse_qsl

def _force_image_format(url: str, fmt: str = "png") -> str:
    u = urlparse(url)
    q = dict(parse_qsl(u.query, keep_blank_values=True))
    q["format"] = fmt  # å¼ºåˆ¶ png
    new_q = urlencode(q, doseq=True)
    return urlunparse(u._replace(query=new_q))


# ============== æ ¹æ®ç¼–ç è¡¥å›¾ï¼šä¸åŸæ¥ä¿æŒä¸€è‡´ ==============
import psycopg2
from psycopg2.extras import RealDictCursor

def fetch_urls_from_db_by_codes(code_file_path, pgsql_config, table_name):
    code_list = [line.strip() for line in Path(code_file_path).read_text(encoding="utf-8").splitlines() if line.strip()]
    print(f"ğŸ” è¯»å–åˆ° {len(code_list)} ä¸ªç¼–ç ")

    urls = set()
    try:
        conn = psycopg2.connect(**pgsql_config)
        cursor = conn.cursor(cursor_factory=RealDictCursor)

        placeholders = ",".join(["%s"] * len(code_list))
        query = f"""
            SELECT DISTINCT product_code, product_url
            FROM {table_name}
            WHERE product_code IN ({placeholders})
        """
        cursor.execute(query, code_list)
        rows = cursor.fetchall()

        code_to_url = {row["product_code"]: row["product_url"] for row in rows}
        for code in code_list:
            url = code_to_url.get(code)
            if url:
                urls.add(url)
            else:
                print(f"âš ï¸ æœªæ‰¾åˆ°å•†å“ç¼–ç : {code}")

        cursor.close()
        conn.close()
    except Exception as e:
        print(f"âŒ æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {e}")

    return list(urls)


def download_images_by_code_file(code_txt_path):
    pgsql_config = ECCO["PGSQL_CONFIG"]
    table_name = ECCO["TABLE_NAME"]

    urls = fetch_urls_from_db_by_codes(code_txt_path, pgsql_config, table_name)
    print(f"ğŸ“¦ å…±éœ€å¤„ç† {len(urls)} ä¸ªå•†å“å›¾ç‰‡")

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = [executor.submit(process_image_url, url) for url in urls]
        for _ in as_completed(futures):
            pass

    print("\nâœ… æ‰€æœ‰è¡¥å›¾å®Œæˆ")


if __name__ == "__main__":
    # main()  # æ­£å¸¸ï¼šæŒ‰ product_links.txt æ‰¹é‡ä¸‹è½½

    # è¡¥å›¾æ¨¡å¼ï¼šæŒ‰ç¼–ç æ–‡ä»¶
    code_txt_path = ECCO["BASE"] / "publication" / "è¡¥å›¾ç¼–ç .txt"
    download_images_by_code_file(code_txt_path)
