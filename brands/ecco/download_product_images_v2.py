# -*- coding: utf-8 -*-
"""
ECCO å•†å“å›¾ç‰‡ä¸‹è½½ V2ï¼ˆå…¼å®¹æ–°æ—§ç«™ç‚¹ + æ›´ç¨³å¥çš„å›¾ç‰‡URLæå–ï¼‰
åœ¨ V1 åŸºç¡€ä¸Šå¢å¼ºï¼š
1) å€™é€‰å›¾ç‰‡URLæ›´å…¨é¢ï¼šimg/source + a[href] + background-image + data-srcset ç­‰
2) requests å¢åŠ  headers + ç®€å•é‡è¯•
3) å…¼å®¹ _eCom / é_eCom å‘½åï¼›å…¼å®¹ png/webp/jpg/jpeg
4) ä¿ç•™åŸæœ‰ pipeline å…¥å£å‡½æ•°ä¸å‚æ•°ï¼ˆmain / download_images_by_code_fileï¼‰
"""

import time
import re
import requests
from pathlib import Path
from urllib.parse import urlparse, urlunparse

from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options

# === ä½ çš„é¡¹ç›®å†…é…ç½® ===
from config import ECCO, ensure_all_dirs

# ---------------- åŸºæœ¬é…ç½® ----------------
PRODUCT_LINKS_FILE = ECCO["BASE"] / "publication" / "product_links.txt"
IMAGE_DIR = ECCO["IMAGE_DOWNLOAD"]
WAIT = 0
DELAY = 0
SKIP_EXISTING_IMAGE = True
MAX_WORKERS = 5

ensure_all_dirs(IMAGE_DIR)

# ============== WebDriverï¼ˆç¨³ï¼‰=============
try:
    from selenium.webdriver.chrome.service import Service
    from webdriver_manager.chrome import ChromeDriverManager
except Exception:
    ChromeDriverManager = None
    Service = None


def create_driver():
    opts = Options()
    for a in ["--headless=new", "--disable-gpu", "--no-sandbox",
              "--disable-dev-shm-usage", "--window-size=1920x1080"]:
        opts.add_argument(a)

    try:
        return webdriver.Chrome(options=opts)
    except Exception as e:
        print(f"[WARN] Selenium Manager å¯åŠ¨å¤±è´¥ï¼š{e}")

    if ChromeDriverManager and Service:
        driver_path = ChromeDriverManager().install()
        return webdriver.Chrome(service=Service(driver_path), options=opts)

    raise RuntimeError("æ— æ³•åˆ›å»º Chrome WebDriverã€‚è¯·å®‰è£… selenium>=4.6ï¼›å¿…è¦æ—¶å®‰è£… webdriver-managerã€‚")


# ============== å·¥å…·å‡½æ•°ï¼šsrcset è§£æ/å‘½åè§„èŒƒ ==============
_VIEW_TOKEN = r"(?:o|m|b|s|top_left_pair|front_pair)"
_EXT_TOKEN  = r"(?:png|webp|jpg|jpeg)"


def _strip_query(url: str) -> str:
    u = urlparse(url)
    return urlunparse(u._replace(query=""))


def _pick_largest_from_srcset(srcset: str) -> str | None:
    best_url, best_w = None, -1
    for part in srcset.split(","):
        part = part.strip()
        if not part:
            continue
        pieces = part.split()
        url = pieces[0]
        w = 0
        if len(pieces) > 1 and pieces[1].endswith("w"):
            try:
                w = int(pieces[1][:-1])
            except Exception:
                w = 0
        if w > best_w:
            best_url, best_w = url, w
    return best_url


def _extract_urls_from_style(style_text: str):
    """
    ä» style="background-image: url(...)" æŠ½å– URL
    """
    if not style_text:
        return
    # æ”¯æŒ url("...") / url('...') / url(...)
    for m in re.finditer(r'url\((?P<q>[\'"]?)(?P<u>.+?)(?P=q)\)', style_text, flags=re.I):
        u = m.group("u").strip()
        if u:
            yield u


def _fix_url(u: str) -> str | None:
    """è¡¥å…¨åè®®ã€è¿‡æ»¤ç©ºå€¼"""
    if not u:
        return None
    u = u.strip()
    if not u:
        return None
    if u.startswith("//"):
        return "https:" + u
    return u


def _looks_like_image_url(u: str) -> bool:
    """åªæ¥å—å›¾ç‰‡é“¾æ¥ï¼ˆå«æŸ¥è¯¢å‚æ•°ä¹Ÿè¡Œï¼‰"""
    if not u:
        return False
    lu = u.lower()
    return any(ext in lu for ext in (".png", ".webp", ".jpg", ".jpeg"))


def _iter_image_candidate_urls(soup: BeautifulSoup):
    """
    éå†é¡µé¢ä¸­å¯èƒ½çš„å•†å“å›¾ URLï¼Œä¼˜å…ˆå– srcset çš„æœ€å¤§å°ºå¯¸ã€‚
    V2å¢å¼ºï¼šå…¼å®¹ data-srcsetã€a[href] å›¾ç‰‡ç›´é“¾ã€åè®®çœç•¥ //cdn...
    """
    # ---------- <img> ----------
    for img in soup.find_all("img"):
        # srcset / data-srcset ä¼˜å…ˆå–æœ€å¤§
        for k in ("srcset", "data-srcset"):
            srcset = img.get(k)
            if srcset:
                best = _pick_largest_from_srcset(srcset)
                best = _fix_url(best)
                if best and _looks_like_image_url(best):
                    yield best

        # src / data-src / data-original
        for k in ("src", "data-src", "data-original"):
            u = _fix_url(img.get(k))
            if u and _looks_like_image_url(u):
                yield u

    # ---------- <source> ----------
    for tag in soup.find_all("source"):
        for k in ("srcset", "data-srcset"):
            srcset = tag.get(k)
            if srcset:
                best = _pick_largest_from_srcset(srcset)
                best = _fix_url(best)
                if best and _looks_like_image_url(best):
                    yield best

    # âœ… ---------- <a href>ï¼ˆæ–°ç‰ˆ eCom ç»å¸¸åœ¨è¿™é‡Œï¼‰ ----------
    for a in soup.find_all("a"):
        href = _fix_url(a.get("href"))
        if href and _looks_like_image_url(href):
            yield href



def _parse_code_view_from_filename(url: str) -> tuple[str | None, str | None, str | None, str]:
    """
    ä» URL æ–‡ä»¶åè§£æ (style6, color5, view, ext)
    å…¼å®¹ï¼š
    - 470824-51866-m_eCom.png
    - 470824-51866-m.png
    - 470824-51866-top_left_pair_eCom.webp
    - 470824-51866-top_left_pair.webp
    - 470824-51866-o.jpg
    """
    no_q = _strip_query(url).lower()
    path = urlparse(no_q).path
    fname = Path(path).name

    # 1) -<view>_ecom.<ext>
    m = re.search(fr"(\d{{6}})-(\d{{5}})-({_VIEW_TOKEN})_ecom\.{_EXT_TOKEN}$", fname, flags=re.I)
    if not m:
        # 2) -<view>.<ext>
        m = re.search(fr"(\d{{6}})-(\d{{5}})-({_VIEW_TOKEN})\.{_EXT_TOKEN}$", fname, flags=re.I)

    if m:
        style6, color5, view = m.group(1), m.group(2), m.group(3).lower()
        ext = Path(path).suffix.lower()
        return style6, color5, view, ext

    # 3) åªè§£æ 6+5
    m2 = re.search(r"(\d{6})-(\d{5})", fname)
    ext = Path(path).suffix.lower()
    if m2:
        return m2.group(1), m2.group(2), None, ext

    return None, None, None, Path(path).suffix.lower()


def _normalize_save_name(url: str, fallback_code: str | None) -> tuple[str, str]:
    """
    ç”Ÿæˆä¿å­˜å (basename, ext)ï¼š
    - ä¼˜å…ˆ 6+5+view  -> 83541402308_m
    - æ¬¡é€‰ 6+5       -> 83541402308
    - å† fallback_code-> 83541402308
    - å†å…œåº•ç”¨ stem
    """
    style6, color5, view, ext = _parse_code_view_from_filename(url)
    if style6 and color5 and view:
        return f"{style6}{color5}_{view}", ext
    if style6 and color5:
        return f"{style6}{color5}", ext
    if fallback_code:
        return fallback_code, ext
    stem = Path(urlparse(_strip_query(url)).path).stem
    return stem.replace("-", "_"), ext


def _extract_code_from_url(u: str) -> str | None:
    m = re.search(r'/(\d{6})/(\d{5})(?:[/?#]|$)', u)
    if m:
        return m.group(1) + m.group(2)
    return None


def _extract_code_from_images_html(html: str) -> str | None:
    m = re.search(r'/(\d{6})-(\d{5})-(?:' + _VIEW_TOKEN + r')\.(?:' + _EXT_TOKEN + r')', html, flags=re.I)
    if m:
        return m.group(1) + m.group(2)
    m2 = re.search(r'/(\d{6})-(\d{5})\.(?:' + _EXT_TOKEN + r')', html, flags=re.I)
    if m2:
        return m2.group(1) + m2.group(2)
    m3 = re.search(r'/(\d{6})-(\d{5})', html, flags=re.I)
    if m3:
        return m3.group(1) + m3.group(2)
    return None


# ---------- ä¸‹è½½ï¼šheaders + é‡è¯• ----------
_SESSION = requests.Session()
_DEFAULT_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                  "(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Referer": "https://gb.ecco.com/",
    "Accept": "image/avif,image/webp,image/apng,image/*,*/*;q=0.8",
}


def _download_bytes(url: str, timeout=25, retries=2) -> bytes | None:
    last_err = None
    for i in range(retries + 1):
        try:
            resp = _SESSION.get(url, headers=_DEFAULT_HEADERS, timeout=timeout)
            resp.raise_for_status()
            return resp.content
        except Exception as e:
            last_err = e
            if i < retries:
                time.sleep(0.6 * (i + 1))
    print(f"âŒ ä¸‹è½½å¤±è´¥(é‡è¯•åä»å¤±è´¥): {url} - {last_err}")
    return None


def download_images_from_soup(soup: BeautifulSoup, formatted_code: str | None):
    """
    æ‰«æé¡µé¢çš„å•†å“å›¾ç‰‡å¹¶ä¸‹è½½ï¼š
    - ä¼˜å…ˆå– srcset æœ€å¤§å°ºå¯¸
    - è§„èŒƒå‘½åï¼ˆ6ä½æ¬¾å·+5ä½è‰²å·+è§†è§’ï¼‰
    - å»é‡ï¼ˆbasename å»é‡ï¼‰
    """
    seen_basenames = set()

    for raw_url in _iter_image_candidate_urls_v2(soup):
        if not raw_url:
            continue

        lower_url = raw_url.lower()

        # åªæ¥å—å›¾ç‰‡èµ„æºï¼ˆURL ä¸­å¸¦æ‰©å±•åï¼‰
        if not any(ext in lower_url for ext in (".png", ".webp", ".jpg", ".jpeg")):
            continue

        basename, ext = _normalize_save_name(raw_url, formatted_code)

        # å¦‚æœè§£æä¸åˆ° viewï¼Œä¸” fallback_code ä¸€æ ·ï¼Œä¼šå¯¼è‡´æ‰€æœ‰å›¾åŒåè¢«å»é‡ã€‚
        # V2 åšä¸€ä¸ªä¿æŠ¤ï¼šå½“ basename == formatted_code ä¸”æ²¡æœ‰ view æ—¶ï¼Œå…è®¸ç»§ç»­ä¸‹è½½ï¼Œ
        # ä½†ç”¨ URL stem åš suffixï¼Œé¿å…åªä¸‹è½½ä¸€å¼ å›¾ã€‚
        if formatted_code and basename == formatted_code:
            # ä»æ–‡ä»¶å stem æå–ä¸€ä¸ªå¯åŒºåˆ†çš„åç¼€ï¼ˆä¾‹å¦‚ m_ecom / o / top_left_pairï¼‰
            stem = Path(urlparse(_strip_query(raw_url)).path).stem.lower()
            # stem å¯èƒ½æ˜¯ "835414-02308-m_ecom" -> å–æœ€åä¸€æ®µ
            parts = re.split(r"[-_]", stem)
            if parts:
                tail = parts[-1]
                # é¿å… tail è¿˜æ˜¯æ•°å­—
                if not re.fullmatch(r"\d+", tail):
                    basename = f"{basename}_{tail}"

        if basename in seen_basenames:
            continue
        seen_basenames.add(basename)

        save_path = IMAGE_DIR / f"{basename}{ext}"
        if SKIP_EXISTING_IMAGE and save_path.exists():
            print(f"âœ… è·³è¿‡: {save_path.name}")
            continue

        data = _download_bytes(raw_url)
        if not data:
            continue

        save_path.write_bytes(data)
        print(f"ğŸ–¼ï¸ ä¸‹è½½: {save_path.name}")
        time.sleep(DELAY)


def process_image_url(url):
    driver = None
    try:
        driver = create_driver()
        driver.get(url)
        time.sleep(WAIT)

        real_url = driver.current_url or url
        html = driver.page_source
        soup = BeautifulSoup(html, "html.parser")

        formatted_code = None

        # æ—§ç«™ç‚¹ï¼šDOM é‡Œå¯èƒ½ç›´æ¥æœ‰ 11 ä½
        code_info = soup.find('div', class_='product_info__product-number')
        if code_info:
            text = code_info.text.strip()
            digits = re.search(r'(\d{11})', text)
            if digits:
                formatted_code = digits.group(1)

        # æ–°ç«™ç‚¹ï¼šURL é‡Œ 6+5
        if not formatted_code:
            formatted_code = _extract_code_from_url(real_url)

        # å›é€€ï¼šä» HTML çš„å›¾ç‰‡æ–‡ä»¶åæå– 6+5
        if not formatted_code:
            formatted_code = _extract_code_from_images_html(html)

        download_images_from_soup(soup, formatted_code)

    except Exception as e:
        print(f"âŒ å•†å“å¤„ç†å¤±è´¥: {url} - {e}")
    finally:
        if driver:
            driver.quit()


# ============== æ‰¹é‡å…¥å£ï¼šä¸åŸæ¥ä¿æŒä¸€è‡´ ==============
from concurrent.futures import ThreadPoolExecutor, as_completed

def main():
    if not PRODUCT_LINKS_FILE.exists():
        print(f"âŒ æœªæ‰¾åˆ°é“¾æ¥æ–‡ä»¶: {PRODUCT_LINKS_FILE}")
        return
    url_list = [u.strip() for u in PRODUCT_LINKS_FILE.read_text(encoding="utf-8").splitlines() if u.strip()]
    print(f"\nğŸ“¸ å¼€å§‹ä¸‹è½½ {len(url_list)} ä¸ªå•†å“çš„å›¾ç‰‡...")

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = [executor.submit(process_image_url, url) for url in url_list]
        for _ in as_completed(futures):
            pass

    print("\nâœ… æ‰€æœ‰å›¾ç‰‡ä¸‹è½½å®Œæˆã€‚")


# ============== æ ¹æ®ç¼–ç è¡¥å›¾ï¼šä¸åŸæ¥ä¿æŒä¸€è‡´ ==============
import psycopg2
from psycopg2.extras import RealDictCursor

def fetch_urls_from_db_by_codes(code_file_path, pgsql_config, table_name):
    code_list = [line.strip() for line in Path(code_file_path).read_text(encoding="utf-8").splitlines() if line.strip()]
    print(f"ğŸ” è¯»å–åˆ° {len(code_list)} ä¸ªç¼–ç ")

    urls = set()
    try:
        conn = psycopg2.connect(**pgsql_config)
        cursor = conn.cursor(cursor_factory=RealDictCursor)

        placeholders = ",".join(["%s"] * len(code_list))
        query = f"""
            SELECT DISTINCT product_code, product_url
            FROM {table_name}
            WHERE product_code IN ({placeholders})
        """
        cursor.execute(query, code_list)
        rows = cursor.fetchall()

        code_to_url = {row["product_code"]: row["product_url"] for row in rows}
        for code in code_list:
            url = code_to_url.get(code)
            if url:
                urls.add(url)
            else:
                print(f"âš ï¸ æœªæ‰¾åˆ°å•†å“ç¼–ç : {code}")

        cursor.close()
        conn.close()
    except Exception as e:
        print(f"âŒ æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {e}")

    return list(urls)


def download_images_by_code_file(code_txt_path):
    pgsql_config = ECCO["PGSQL_CONFIG"]
    table_name = ECCO["TABLE_NAME"]

    urls = fetch_urls_from_db_by_codes(code_txt_path, pgsql_config, table_name)
    print(f"ğŸ“¦ å…±éœ€å¤„ç† {len(urls)} ä¸ªå•†å“å›¾ç‰‡")

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = [executor.submit(process_image_url, url) for url in urls]
        for _ in as_completed(futures):
            pass

    print("\nâœ… æ‰€æœ‰è¡¥å›¾å®Œæˆ")


if __name__ == "__main__":
    # main()  # æ­£å¸¸ï¼šæŒ‰ product_links.txt æ‰¹é‡ä¸‹è½½
    code_txt_path = ECCO["BASE"] / "publication" / "è¡¥å›¾ç¼–ç .txt"
    download_images_by_code_file(code_txt_path)
