import requests
from bs4 import BeautifulSoup
import time
from pathlib import Path
import sys
from urllib.parse import urljoin
from config import TERRACES  # âœ… ä½¿ç”¨ config ä¸­çš„ TERRACES é…ç½®

sys.stdout.reconfigure(encoding='utf-8')

# ========= å‚æ•°é…ç½® =========
LINKS_FILE = TERRACES["LINKS_FILE"]
BASE_URL = "https://www.terracesmenswear.co.uk/barbour&page={}"
BASE_DOMAIN = "https://www.terracesmenswear.co.uk"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
}

# ========= å¸¦é‡è¯•è¯·æ±‚ =========
def fetch_with_retry(url, retries=3, timeout=20):
    for attempt in range(1, retries + 1):
        try:
            resp = requests.get(url, headers=HEADERS, timeout=timeout)
            resp.raise_for_status()
            return resp.text
        except Exception as e:
            print(f"âŒ ç¬¬ {attempt} æ¬¡è¯·æ±‚å¤±è´¥: {url}ï¼Œé”™è¯¯: {e}")
            if attempt < retries:
                time.sleep(2 * attempt)
    return None

# ========= è·å–å•†å“é“¾æ¥ =========
def get_links_from_page(url):
    html = fetch_with_retry(url)
    if not html:
        return []

    soup = BeautifulSoup(html, "html.parser")
    product_titles = soup.find_all("h4", class_="product__title")

    links = []
    for h4 in product_titles:
        a_tag = h4.find("a")
        if a_tag and a_tag.get("href"):
            href = a_tag["href"].strip()
            full_url = urljoin(BASE_DOMAIN, href)  # âœ… è½¬æ¢ä¸ºå®Œæ•´é“¾æ¥
            links.append(full_url)

    return links

# ========= ä¸»é€»è¾‘å‡½æ•°ï¼ˆå¯ä¾› pipeline è°ƒç”¨ï¼‰ =========
def collect_terraces_links(max_pages=50, wait=1, max_empty_pages=3, output_file=LINKS_FILE):
    all_links = set()
    empty_pages = 0

    for page in range(1, max_pages + 1):
        url = BASE_URL.format(page)
        print(f"\nğŸŒ æŠ“å–: {url}")
        links = get_links_from_page(url)

        if links:
            print(f"âœ… ç¬¬ {page} é¡µ: {len(links)} æ¡é“¾æ¥")
            all_links.update(links)
            empty_pages = 0
        else:
            print(f"âš ï¸ ç¬¬ {page} é¡µæ— é“¾æ¥")
            empty_pages += 1
            if empty_pages >= max_empty_pages:
                print(f"â¹ï¸ è¿ç»­ {max_empty_pages} é¡µä¸ºç©ºï¼Œæå‰ç»“æŸ")
                break
        time.sleep(wait)

    # âœ… ç¡®ä¿ç›®å½•å­˜åœ¨å¹¶åˆ é™¤æ—§æ–‡ä»¶
    output_file.parent.mkdir(parents=True, exist_ok=True)
    if output_file.exists():
        output_file.unlink()

    with open(output_file, "w", encoding="utf-8") as f:
        for link in sorted(all_links):
            f.write(link + "\n")

    print(f"\nğŸ‰ å…±æŠ“å–é“¾æ¥: {len(all_links)}ï¼Œå·²ä¿å­˜åˆ°: {output_file}")
    return len(all_links)

# ========= è„šæœ¬ç‹¬ç«‹è¿è¡Œå…¥å£ =========
if __name__ == "__main__":
    collect_terraces_links()
