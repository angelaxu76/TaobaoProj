# -*- coding: utf-8 -*-
"""
Marks & Spencer å¥³æ¬¾é’ˆç»‡å•†å“é“¾æ¥æŠ“å–ï¼ˆå‚è€ƒ camper ç‰ˆæœ¬çš„é€»è¾‘ï¼‰

åŠŸèƒ½ï¼š
1ï¼‰æ”¯æŒå¤šä¸ªç±»ç›®å…¥å£ï¼ˆä¾‹å¦‚ï¼šå¥³æ¬¾é’ˆç»‡å¼€è¡« / ç¾Šæ¯›è¡«ç­‰ï¼‰ï¼Œæ¯ä¸ªå…¥å£æŒ‰é¡µæ•°é€’å¢æŠ“å–ã€‚
2ï¼‰è‡ªåŠ¨ç¿»é¡µï¼šä»ç¬¬ 1 é¡µå¼€å§‹ï¼Œåªè¦è¿˜èƒ½æŠ“åˆ°å•†å“å°±ç»§ç»­ï¼›
   å¦‚æœå‘ç°ã€Œå½“å‰é¡µå•†å“åˆ—è¡¨å’Œä¸Šä¸€é¡µå®Œå…¨ä¸€æ ·ã€ï¼ˆæ¯”å¦‚è¢«é‡å®šå‘å›é¦–é¡µï¼‰ï¼Œå°±åœæ­¢è¯¥ç±»ç›®ã€‚
3ï¼‰ä»å•†å“å¡ç‰‡ <a class="product-card_cardWrapper__..."> ä¸­æå–é“¾æ¥ï¼Œé¿å…æŠ“åˆ°é¢œè‰²å°åœ†ç‚¹çš„é“¾æ¥ã€‚
4ï¼‰æ‰€æœ‰é“¾æ¥å»é‡ã€æ’åºåå†™å…¥ config ä¸­çš„ LINKS_FILEã€‚
"""

import time
import sys
from pathlib import Path

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urldefrag

from config import BRAND_CONFIG

# ----------------------------------------------------------------------
# é…ç½®åŒº
# ----------------------------------------------------------------------

# ç«™ç‚¹å‰ç¼€
DOMAIN = "https://www.marksandspencer.com"

# ä»å…¨å±€ config ä¸­è¯»å– Marks & Spencer å“ç‰Œé…ç½®
CFG = BRAND_CONFIG["marksandspencer"]
OUTPUT_FILE: Path = CFG["LINKS_FILE"]

# è¯·æ±‚å¤´ï¼ˆå¸¦ä¸ª User-Agent ç¨å¾®å‹å¥½ä¸€ç‚¹ï¼‰
HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    )
}

# æ¯ä¸ªç±»ç›®æœ€å¤šå…è®¸è¿ç»­å¤šå°‘é¡µâ€œæœªå‘ç°å•†å“â€å°±åœï¼ˆå¤‡ç”¨ï¼‰
MAX_EMPTY_PAGES = 3

# è¯·æ±‚ä¹‹é—´çš„ä¼‘çœ ï¼Œé¿å…å‹åŠ›å¤ªå¤§
SLEEP_SECONDS = 1.0

# ----------------------------------------------------------------------
# M&S å¥³æ¬¾é’ˆç»‡ç±»ç›®å…¥å£
# ç»Ÿä¸€ä½¿ç”¨ {} ä½œä¸º page å ä½ç¬¦ï¼Œå’Œ camper è„šæœ¬ä¿æŒä¸€è‡´
# ----------------------------------------------------------------------
BASE_URLS = [
    # âœ… ä½ æä¾›çš„ï¼šå¥³æ¬¾ M&S å“ç‰Œå¼€è¡«
    "https://www.marksandspencer.com/l/women/knitwear/cardigans?filter=Brand%253DM%2526S&page={}",

    # TODOï¼šåç»­å¯ä»¥åŠ  jumpers / æ‰€æœ‰é’ˆç»‡ç­‰
    # "https://www.marksandspencer.com/l/women/knitwear/jumpers?filter=Brand%253DM%2526S&page={}",
    # "https://www.marksandspencer.com/l/women/knitwear?filter=Brand%253DM%2526S&page={}",
]


# ----------------------------------------------------------------------
# å·¥å…·å‡½æ•°
# ----------------------------------------------------------------------

def fetch_page(url: str) -> str | None:
    """è¯·æ±‚ä¸€ä¸ªåˆ—è¡¨é¡µï¼Œè¿”å› HTML æ–‡æœ¬ï¼Œå¤±è´¥åˆ™è¿”å› Noneã€‚"""
    try:
        resp = requests.get(url, headers=HEADERS, timeout=15)
        if resp.status_code != 200:
            print(f"[WARN] {url} è¿”å›çŠ¶æ€ç  {resp.status_code}", file=sys.stderr)
            return None
        return resp.text
    except Exception as e:
        print(f"[ERROR] è¯·æ±‚å¤±è´¥: {url} -> {e}", file=sys.stderr)
        return None


def extract_product_links(html: str) -> list[str]:
    """
    ä»åˆ—è¡¨é¡µ HTML ä¸­æå–å•†å“é“¾æ¥ã€‚

    æ ¹æ®ä½ æä¾›çš„é¡µé¢ç»“æ„ï¼Œæ¯ä¸ªå•†å“å¡ç‰‡å¤§è‡´æ˜¯ï¼š
        <a class="product-card_cardWrapper__GVSTY" href="https://www.marksandspencer.com/...">...</a>

    æˆ‘ä»¬åªæŠ“ class ä¸­åŒ…å« "product-card_cardWrapper" çš„ <a>ï¼Œ
    å¯ä»¥é¿å…æŠ“åˆ°é¢œè‰²å°åœ†ç‚¹ï¼ˆcolour-swatch_elementï¼‰ä¹‹ç±»çš„é“¾æ¥ã€‚
    """
    soup = BeautifulSoup(html, "html.parser")

    links: list[str] = []

    for a in soup.find_all("a"):
        classes = a.get("class", [])
        if not classes:
            continue

        # åªè¦ class ä¸­å«æœ‰ä»¥ "product-card_cardWrapper" å¼€å¤´çš„å­—æ®µï¼Œå°±è®¤ä¸ºæ˜¯å•†å“å¡ç‰‡
        if any(cls.startswith("product-card_cardWrapper") for cls in classes):
            href = a.get("href")
            if not href:
                continue

            # å¤„ç†ç›¸å¯¹é“¾æ¥ -> ç»å¯¹é“¾æ¥
            full_url = urljoin(DOMAIN, href)

            # å»æ‰ URL ä¸­çš„ fragmentï¼ˆ#intid=...ï¼‰
            full_url, _ = urldefrag(full_url)

            links.append(full_url)

    return links


# ----------------------------------------------------------------------
# ä¸»é€»è¾‘ï¼šæŒ‰ç±»ç›®è‡ªåŠ¨ç¿»é¡µæŠ“å–
# ----------------------------------------------------------------------

def collect_all_links() -> list[str]:
    """éå† BASE_URLSï¼ŒæŠŠæ‰€æœ‰åˆ—è¡¨é¡µçš„å•†å“é“¾æ¥æŠ“å‡ºæ¥å¹¶å»é‡ã€‚"""
    all_links: set[str] = set()

    for base_url in BASE_URLS:
        print(f"\nğŸŸ¡ å¼€å§‹å¤„ç†ç±»ç›®å…¥å£: {base_url}")

        page = 1
        empty_pages = 0
        last_page_links: set[str] | None = None  # è®°å½•ä¸Šä¸€é¡µçš„é“¾æ¥é›†åˆ

        while True:
            url = base_url.format(page)
            print(f"  -> æŠ“å–ç¬¬ {page} é¡µ: {url}")

            html = fetch_page(url)
            if not html:
                # å¦‚æœç›´æ¥è¯·æ±‚å¤±è´¥ï¼Œä¹Ÿç®—ä¸€é¡µæ— ç»“æœ
                empty_pages += 1
                if empty_pages >= MAX_EMPTY_PAGES:
                    print(f"  âš  è¿ç»­ {MAX_EMPTY_PAGES} é¡µæ— æ•°æ®ï¼Œåœæ­¢è¯¥ç±»ç›®")
                    break
                page += 1
                time.sleep(SLEEP_SECONDS)
                continue

            links = extract_product_links(html)
            links = list(set(links))  # å½“å‰é¡µå»é‡
            current_set = set(links)

            if not links:
                empty_pages += 1
                print(f"    æœ¬é¡µæœªå‘ç°å•†å“å¡ç‰‡ï¼ˆè¿ç»­ç©ºé¡µ {empty_pages}ï¼‰")
                if empty_pages >= MAX_EMPTY_PAGES:
                    print(f"  âš  è¿ç»­ {MAX_EMPTY_PAGES} é¡µæ— æ•°æ®ï¼Œåœæ­¢è¯¥ç±»ç›®")
                    break
            else:
                # âœ… å…³é”®é€»è¾‘ï¼šå¦‚æœè¿™ä¸€é¡µå’Œä¸Šä¸€é¡µå®Œå…¨ç›¸åŒï¼Œè¯´æ˜å¾ˆå¯èƒ½è¢«é‡å®šå‘å›é¦–é¡µï¼Œç›´æ¥åœæ­¢è¯¥ç±»ç›®
                if last_page_links is not None and current_set == last_page_links:
                    print("    æœ¬é¡µå•†å“åˆ—è¡¨ä¸ä¸Šä¸€é¡µå®Œå…¨ç›¸åŒï¼ˆå¯èƒ½å·²ç»è·³å›é¦–é¡µï¼‰ï¼Œåœæ­¢è¯¥ç±»ç›®")
                    break

                # è®°å½•å½“å‰é¡µç”¨äºä¸‹ä¸€æ¬¡å¯¹æ¯”
                last_page_links = current_set

                # åªç»Ÿè®¡çœŸæ­£æ–°å¢çš„é“¾æ¥
                new_links = [u for u in links if u not in all_links]
                all_links.update(new_links)
                print(
                    f"    æœ¬é¡µæŠ“åˆ° {len(new_links)} ä¸ªã€æ–°å¢ã€‘å•†å“é“¾æ¥ï¼Œ"
                    f"ç´¯è®¡æ€»æ•° {len(all_links)}"
                )

            page += 1
            time.sleep(SLEEP_SECONDS)

    # å…¨éƒ¨ç±»ç›®æŠ“å®Œï¼Œåšä¸€æ¬¡å…¨å±€å»é‡ + æ’åº
    all_links_list = sorted(all_links)
    print(f"\nâœ… æ‰€æœ‰ç±»ç›®åˆè®¡æŠ“åˆ° {len(all_links_list)} æ¡å»é‡åçš„å•†å“é“¾æ¥")

    return all_links_list


def save_links(links: list[str], filepath: Path) -> None:
    """æŠŠé“¾æ¥åˆ—è¡¨æŒ‰è¡Œå†™å…¥åˆ° txt æ–‡ä»¶ã€‚"""
    filepath.parent.mkdir(parents=True, exist_ok=True)
    with filepath.open("w", encoding="utf-8") as f:
        for url in links:
            f.write(url + "\n")
    print(f"ğŸ’¾ å·²å†™å…¥åˆ°: {filepath.resolve()}")


def marksandspencer_get_links():
    """ä¿æŒç»™ pipeline è°ƒç”¨çš„å‡½æ•°åå’Œå‚æ•°ä¸å˜ã€‚"""
    links = collect_all_links()
    save_links(links, OUTPUT_FILE)


if __name__ == "__main__":
    marksandspencer_get_links()
