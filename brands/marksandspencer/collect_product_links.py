# -*- coding: utf-8 -*-
"""
M&S å¥³æ¬¾é’ˆç»‡å•†å“é“¾æ¥æŠ“å–ï¼ˆå‚è€ƒ camper ç‰ˆæœ¬çš„é€»è¾‘ï¼‰

åŠŸèƒ½ï¼š
1ï¼‰æ”¯æŒå¤šä¸ªç±»ç›®å…¥å£ï¼ˆä¾‹å¦‚ï¼šå¥³æ¬¾é’ˆç»‡å¼€è¡« / ç¾Šæ¯›è¡«ç­‰ï¼‰ï¼Œæ¯ä¸ªå…¥å£æŒ‰é¡µæ•°é€’å¢æŠ“å–ã€‚
2ï¼‰è‡ªåŠ¨ç¿»é¡µï¼šä»ç¬¬ 1 é¡µå¼€å§‹ï¼Œåªè¦è¿˜èƒ½æŠ“åˆ°å•†å“å°±ç»§ç»­ï¼›æŸä¸ªç±»ç›®è¿ç»­ N é¡µä¸ºç©ºå°±åœæ­¢è¯¥ç±»ç›®ã€‚
3ï¼‰ä»å•†å“å¡ç‰‡ <a class="product-card_cardWrapper__..."> ä¸­æå–é“¾æ¥ï¼Œé¿å…æŠ“åˆ°é¢œè‰²å°åœ†ç‚¹çš„é“¾æ¥ã€‚
4ï¼‰æ‰€æœ‰é“¾æ¥å»é‡ã€æ’åºåå†™å…¥æœ¬åœ° txt æ–‡ä»¶ã€‚
"""

import time
import sys
import re
from pathlib import Path

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urldefrag

# ----------------------------------------------------------------------
# é…ç½®åŒº
# ----------------------------------------------------------------------

# M&S ç«™ç‚¹å‰ç¼€
DOMAIN = "https://www.marksandspencer.com"

# è¾“å‡ºæ–‡ä»¶ï¼ˆä½ å¯ä»¥æ”¹æˆä½ ä¹ æƒ¯çš„è·¯å¾„ï¼‰
OUTPUT_FILE = Path("product_links_ms.txt")

# è¯·æ±‚å¤´ï¼ˆå¸¦ä¸ª User-Agent ç¨å¾®å‹å¥½ä¸€ç‚¹ï¼‰
HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    )
}

# æ¯ä¸ªç±»ç›®æœ€å¤šå…è®¸è¿ç»­å¤šå°‘é¡µâ€œæœªå‘ç°å•†å“â€å°±åœ
MAX_EMPTY_PAGES = 3

# è¯·æ±‚ä¹‹é—´çš„ä¼‘çœ ï¼Œé¿å…å‹åŠ›å¤ªå¤§
SLEEP_SECONDS = 1.0

# ----------------------------------------------------------------------
# M&S å¥³æ¬¾é’ˆç»‡ç±»ç›®å…¥å£
# æ³¨æ„ï¼šè¿™é‡Œç»™å‡ºç¤ºä¾‹ï¼Œä½ å¯ä»¥æŒ‰å®é™…éœ€è¦å¢åˆ æ”¹
# ç»Ÿä¸€ä½¿ç”¨ {} ä½œä¸º page å ä½ç¬¦ï¼Œå’Œ camper è„šæœ¬ä¿æŒä¸€è‡´
# ----------------------------------------------------------------------
BASE_URLS = [
    # âœ… ä½ æä¾›çš„ï¼šå¥³æ¬¾ M&S å“ç‰Œå¼€è¡«
    "https://www.marksandspencer.com/l/women/knitwear/cardigans?filter=Brand%253DM%2526S&page={}",

    # ä¾‹å¦‚ï¼šå¥³æ¬¾ M&S å“ç‰Œé’ˆç»‡è¡«ï¼ˆè‡ªå·±ç¡®è®¤è·¯å¾„æ²¡é—®é¢˜åå†å¯ç”¨ï¼‰
    # "https://www.marksandspencer.com/l/women/knitwear/jumpers?filter=Brand%253DM%2526S&page={}",

    # ä¹Ÿå¯ä»¥æ”¾ä¸€ä¸ªæ›´å¤§çš„æ±‡æ€»ç±»ç›®ï¼ˆå¦‚æœä½ æƒ³æŠŠæ‰€æœ‰é’ˆç»‡éƒ½æŠ“ä¸‹æ¥ï¼‰
    # "https://www.marksandspencer.com/l/women/knitwear?filter=Brand%253DM%2526S&page={}",
]


# ----------------------------------------------------------------------
# å·¥å…·å‡½æ•°
# ----------------------------------------------------------------------

def fetch_page(url: str) -> str | None:
    """è¯·æ±‚ä¸€ä¸ªåˆ—è¡¨é¡µï¼Œè¿”å› HTML æ–‡æœ¬ï¼Œå¤±è´¥åˆ™è¿”å› Noneã€‚"""
    try:
        resp = requests.get(url, headers=HEADERS, timeout=15)
        if resp.status_code != 200:
            print(f"[WARN] {url} è¿”å›çŠ¶æ€ç  {resp.status_code}", file=sys.stderr)
            return None
        return resp.text
    except Exception as e:
        print(f"[ERROR] è¯·æ±‚å¤±è´¥: {url} -> {e}", file=sys.stderr)
        return None


def extract_product_links(html: str) -> list[str]:
    """
    ä»åˆ—è¡¨é¡µ HTML ä¸­æå–å•†å“é“¾æ¥ã€‚

    æ ¹æ®ä½ æä¾›çš„é¡µé¢ç»“æ„ï¼Œæ¯ä¸ªå•†å“å¡ç‰‡å¤§è‡´æ˜¯ï¼š
        <a class="product-card_cardWrapper__GVSTY" href="https://www.marksandspencer.com/...">...</a>

    æˆ‘ä»¬åªæŠ“ class ä¸­åŒ…å« "product-card_cardWrapper" çš„ <a>ï¼Œ
    å¯ä»¥é¿å…æŠ“åˆ°é¢œè‰²å°åœ†ç‚¹ï¼ˆcolour-swatch_elementï¼‰ä¹‹ç±»çš„é“¾æ¥ã€‚
    """
    soup = BeautifulSoup(html, "html.parser")

    links: list[str] = []

    for a in soup.find_all("a"):
        classes = a.get("class", [])
        if not classes:
            continue

        # åªè¦ class ä¸­å«æœ‰ä»¥ "product-card_cardWrapper" å¼€å¤´çš„å­—æ®µï¼Œå°±è®¤ä¸ºæ˜¯å•†å“å¡ç‰‡
        if any(cls.startswith("product-card_cardWrapper") for cls in classes):
            href = a.get("href")
            if not href:
                continue

            # å¤„ç†ç›¸å¯¹é“¾æ¥ -> ç»å¯¹é“¾æ¥
            full_url = urljoin(DOMAIN, href)

            # å»æ‰ URL ä¸­çš„ fragmentï¼ˆ#intid=...ï¼‰
            full_url, _ = urldefrag(full_url)

            links.append(full_url)

    return links


# ----------------------------------------------------------------------
# ä¸»é€»è¾‘ï¼šæŒ‰ç±»ç›®è‡ªåŠ¨ç¿»é¡µæŠ“å–
# ----------------------------------------------------------------------

def collect_all_links() -> list[str]:
    """éå† BASE_URLSï¼ŒæŠŠæ‰€æœ‰åˆ—è¡¨é¡µçš„å•†å“é“¾æ¥æŠ“å‡ºæ¥å¹¶å»é‡ã€‚"""
    all_links: set[str] = set()

    for base_url in BASE_URLS:
        print(f"\nğŸŸ¡ å¼€å§‹å¤„ç†ç±»ç›®å…¥å£: {base_url}")

        page = 1
        empty_pages = 0

        while True:
            url = base_url.format(page)
            print(f"  -> æŠ“å–ç¬¬ {page} é¡µ: {url}")

            html = fetch_page(url)
            if not html:
                # å¦‚æœç›´æ¥è¯·æ±‚å¤±è´¥ï¼Œä¹Ÿç®—ä¸€é¡µæ— ç»“æœ
                empty_pages += 1
                if empty_pages >= MAX_EMPTY_PAGES:
                    print(f"  âš  è¿ç»­ {MAX_EMPTY_PAGES} é¡µæ— æ•°æ®ï¼Œåœæ­¢è¯¥ç±»ç›®")
                    break
                page += 1
                time.sleep(SLEEP_SECONDS)
                continue

            links = extract_product_links(html)
            links = list(set(links))  # å½“å‰é¡µå»é‡

            if not links:
                empty_pages += 1
                print(f"    æœ¬é¡µæœªå‘ç°å•†å“å¡ç‰‡ï¼ˆè¿ç»­ç©ºé¡µ {empty_pages}ï¼‰")
                if empty_pages >= MAX_EMPTY_PAGES:
                    print(f"  âš  è¿ç»­ {MAX_EMPTY_PAGES} é¡µæ— æ•°æ®ï¼Œåœæ­¢è¯¥ç±»ç›®")
                    break
            else:
                empty_pages = 0
                print(f"    æœ¬é¡µæŠ“åˆ° {len(links)} ä¸ªå•†å“é“¾æ¥")
                all_links.update(links)

            page += 1
            time.sleep(SLEEP_SECONDS)

    # å…¨éƒ¨ç±»ç›®æŠ“å®Œï¼Œåšä¸€æ¬¡å…¨å±€å»é‡ + æ’åº
    all_links_list = sorted(all_links)
    print(f"\nâœ… æ‰€æœ‰ç±»ç›®åˆè®¡æŠ“åˆ° {len(all_links_list)} æ¡å»é‡åçš„å•†å“é“¾æ¥")

    return all_links_list


def save_links(links: list[str], filepath: Path) -> None:
    """æŠŠé“¾æ¥åˆ—è¡¨æŒ‰è¡Œå†™å…¥åˆ° txt æ–‡ä»¶ã€‚"""
    filepath.parent.mkdir(parents=True, exist_ok=True)
    with filepath.open("w", encoding="utf-8") as f:
        for url in links:
            f.write(url + "\n")
    print(f"ğŸ’¾ å·²å†™å…¥åˆ°: {filepath.resolve()}")


def main():
    links = collect_all_links()
    save_links(links, OUTPUT_FILE)


if __name__ == "__main__":
    main()
