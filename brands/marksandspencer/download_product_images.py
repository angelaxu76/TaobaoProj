# -*- coding: utf-8 -*-
"""
Marks & Spencer å•†å“å›¾ç‰‡ä¸‹è½½

- ä» links_jacket.txt / links_lingerie.txt è¯»å–å•†å“é“¾æ¥
- è§£æ __NEXT_DATA__ æå–å›¾ç‰‡ URLï¼ˆå…¼å®¹æ–°æ—§ä¸¤ç§ M&S é¡µé¢ç»“æ„ï¼‰
- å¤šçº¿ç¨‹å¹¶å‘ä¸‹è½½ï¼Œè·³è¿‡å·²å­˜åœ¨å›¾ç‰‡
- æ”¯æŒæŒ‰å•†å“ç¼–ç ä»æ•°æ®åº“æŸ¥è¯¢ URL è¡¥å›¾
"""

import re
import json
import requests
import psycopg2
from bs4 import BeautifulSoup
from pathlib import Path
from urllib.parse import urlparse, parse_qs
from concurrent.futures import ThreadPoolExecutor, as_completed
from config import MARKSANDSPENCER, ensure_all_dirs
from psycopg2.extras import RealDictCursor

# === é…ç½® ===
LINKS_FILE_JACKET   = MARKSANDSPENCER["LINKS_FILE_JACKET"]
LINKS_FILE_LINGERIE = MARKSANDSPENCER["LINKS_FILE_LINGERIE"]
IMAGE_DIR           = MARKSANDSPENCER["IMAGE_DOWNLOAD"]
TABLE_NAME          = MARKSANDSPENCER["TABLE_NAME"]
PGSQL_CONFIG        = MARKSANDSPENCER["PGSQL_CONFIG"]

# M&S å›¾ç‰‡ CDN æ¨¡æ¿ï¼šassetId â†’ å®Œæ•´ URL
MS_IMAGE_CDN = "https://asset1.cxnmarksandspencer.com/is/image/mands/{}?$PDP_M_ZOOM$&fmt=webp&wid=1008"

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    )
}
MAX_WORKERS         = 5
SKIP_EXISTING_IMAGE = True

ensure_all_dirs(IMAGE_DIR)


# ===================== å·¥å…·å‡½æ•° =====================

def _load_json_safe(text: str):
    if not text:
        return None
    text = text.replace("undefined", "null")
    try:
        return json.loads(text)
    except Exception:
        return None


def _normalize_color_code(color: str) -> str:
    """å»æ‰éå­—æ¯æ•°å­—å­—ç¬¦å¹¶è½¬å¤§å†™ï¼Œä¸ fetch_jacket_info.py ä¿æŒä¸€è‡´ã€‚"""
    if not color:
        return ""
    return re.sub(r"[^A-Za-z0-9]+", "", color).upper()


def extract_product_code(url: str, next_data: dict | None) -> str:
    """
    æå–å•†å“ç¼–ç ã€‚
    ä¼˜å…ˆä» __NEXT_DATA__ å– strokeId / productExternalIdï¼›
    å…¶æ¬¡ä» URL è·¯å¾„ /p/<code> æå–ï¼›
    æœ€åé™„åŠ é¢œè‰²åç¼€ï¼ˆ?color=XXXï¼‰ã€‚
    """
    code = None

    if next_data:
        page_props = (next_data.get("props") or {}).get("pageProps") or {}

        # æ–°ç»“æ„ï¼šproductDetails
        pd = page_props.get("productDetails")
        if isinstance(pd, dict):
            attrs = pd.get("attributes") or {}
            code = attrs.get("strokeId") or pd.get("productExternalId") or pd.get("id")

        # æ—§ç»“æ„ï¼šproductSheet
        if not code:
            sheet = page_props.get("productSheet")
            if isinstance(sheet, dict):
                code = sheet.get("code") or sheet.get("id")

    # ä» URL è·¯å¾„å…œåº•
    if not code:
        m = re.search(r"/p/(\w+)", url)
        if m:
            code = m.group(1)

    code = str(code) if code else "unknown"

    # é™„åŠ é¢œè‰²åç¼€
    qs = parse_qs(urlparse(url).query)
    color_raw = (qs.get("color") or [None])[0]
    if color_raw:
        code = f"{code}_{_normalize_color_code(color_raw)}"

    return code


def extract_image_urls(next_data: dict | None, url: str) -> list[str]:
    """
    ä» __NEXT_DATA__ æå– M&S å•†å“å›¾ç‰‡ URLã€‚

    M&S å›¾ç‰‡å®é™…å­˜å‚¨ä¸º assetIdï¼Œä½äºï¼š
        productDetails.variants[n].assets[].assetId

    é€šè¿‡ CDN æ¨¡æ¿æ‹¼æ¥æˆçœŸå® URLã€‚
    é¢œè‰²åŒ¹é…ï¼šä¼˜å…ˆå– URL ?color= å¯¹åº”çš„ variantï¼›
    è‹¥æœªåŒ¹é…åˆ™é€€ä¸ºç¬¬ä¸€ä¸ª variantã€‚

    å…œåº•å…¼å®¹æ—§ç»“æ„ï¼ˆproductSheet.multiMedia / imageUrlsï¼‰ã€‚
    """
    if not next_data:
        return []

    page_props = (next_data.get("props") or {}).get("pageProps") or {}

    # --- å½“å‰é¢œè‰²ï¼ˆä» URL è¯»å–ï¼‰ ---
    color_from_url = (parse_qs(urlparse(url).query).get("color") or [None])[0]
    color_norm = _normalize_color_code(color_from_url) if color_from_url else ""

    # --- æ–°ç»“æ„ï¼šproductDetails.variants[].assets[] ---
    pd = page_props.get("productDetails")
    if isinstance(pd, dict):
        variants = pd.get("variants") or []

        # åŒ¹é…é¢œè‰²å¯¹åº”çš„ variant
        matched_variant = None
        if color_norm:
            for v in variants:
                vc = _normalize_color_code(v.get("colour") or v.get("color") or "")
                if vc == color_norm:
                    matched_variant = v
                    break

        # æœªå‘½ä¸­åˆ™å–ç¬¬ä¸€ä¸ª
        if matched_variant is None and variants:
            matched_variant = variants[0]

        if matched_variant:
            asset_ids = [
                a["assetId"]
                for a in (matched_variant.get("assets") or [])
                if a.get("assetId")
            ]
            if asset_ids:
                return [MS_IMAGE_CDN.format(aid) for aid in asset_ids]

    # --- æ—§ç»“æ„ï¼šproductSheet ---
    sheet = page_props.get("productSheet")
    if isinstance(sheet, dict):
        raw: list[str] = []

        for mm in (sheet.get("multiMedia") or []):
            for key in ("src", "url", "href", "imageUrl"):
                v = mm.get(key)
                if v and isinstance(v, str):
                    raw.append(v)
                    break

        for u in (sheet.get("imageUrls") or []):
            if isinstance(u, str):
                raw.append(u)

        seen: set[str] = set()
        result: list[str] = []
        for u in raw:
            u = u.strip()
            if u and u not in seen:
                seen.add(u)
                result.append(u)
        return result

    return []


# ===================== ä¸‹è½½é€»è¾‘ =====================

def download_image(url: str, save_path: Path):
    try:
        if SKIP_EXISTING_IMAGE and save_path.exists():
            print(f"âœ… å·²å­˜åœ¨ï¼Œè·³è¿‡: {save_path.name}")
            return
        r = requests.get(url, headers=HEADERS, timeout=15)
        if r.status_code == 200:
            save_path.write_bytes(r.content)
            print(f"ğŸ–¼ï¸ ä¸‹è½½æˆåŠŸ: {save_path.name}")
        else:
            print(f"âš ï¸ å›¾ç‰‡è¯·æ±‚å¤±è´¥ï¼ˆ{r.status_code}ï¼‰: {url}")
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {url} â†’ {e}")


def process_product_url(url: str):
    print(f"ğŸ“· å¤„ç†å•†å“: {url}")
    try:
        r = requests.get(url, headers=HEADERS, timeout=20)
        r.raise_for_status()
        soup = BeautifulSoup(r.text, "html.parser")

        script_tag = soup.find("script", id="__NEXT_DATA__")
        next_data = _load_json_safe(script_tag.string) if script_tag else None

        code       = extract_product_code(url, next_data)
        image_urls = extract_image_urls(next_data, url)

        if not image_urls:
            print(f"âš ï¸ æœªæ‰¾åˆ°å›¾ç‰‡: {url}")
            return

        for idx, img_url in enumerate(image_urls, 1):
            # CDN URL å« fmt=webp å‚æ•°ï¼Œæ‰©å±•åç»Ÿä¸€ç”¨ .webp
            img_path = IMAGE_DIR / f"{code}_{idx}.webp"
            download_image(img_url, img_path)

    except Exception as e:
        print(f"âŒ å•†å“å¤„ç†å¤±è´¥: {url} â†’ {e}")


def _run_batch(urls: list[str]):
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = [executor.submit(process_product_url, url) for url in urls]
        for future in as_completed(futures):
            future.result()


# ===================== å…¬å¼€å…¥å£ =====================

def download_jacket_images():
    """ä¸‹è½½ links_jacket.txt ä¸­æ‰€æœ‰å•†å“çš„å›¾ç‰‡ã€‚"""
    if not LINKS_FILE_JACKET.exists():
        print(f"âŒ é“¾æ¥æ–‡ä»¶ä¸å­˜åœ¨: {LINKS_FILE_JACKET}")
        return
    urls = [u.strip() for u in LINKS_FILE_JACKET.read_text(encoding="utf-8").splitlines() if u.strip()]
    print(f"ğŸ“¦ å…± {len(urls)} ä¸ªå¤–å¥—å•†å“å›¾ç‰‡å¾…ä¸‹è½½")
    _run_batch(urls)
    print("\nâœ… å¤–å¥—å›¾ç‰‡ä¸‹è½½å®Œæˆ")


def download_lingerie_images():
    """ä¸‹è½½ links_lingerie.txt ä¸­æ‰€æœ‰å•†å“çš„å›¾ç‰‡ã€‚"""
    if not LINKS_FILE_LINGERIE.exists():
        print(f"âŒ é“¾æ¥æ–‡ä»¶ä¸å­˜åœ¨: {LINKS_FILE_LINGERIE}")
        return
    urls = [u.strip() for u in LINKS_FILE_LINGERIE.read_text(encoding="utf-8").splitlines() if u.strip()]
    print(f"ğŸ“¦ å…± {len(urls)} ä¸ªå†…è¡£å•†å“å›¾ç‰‡å¾…ä¸‹è½½")
    _run_batch(urls)
    print("\nâœ… å†…è¡£å›¾ç‰‡ä¸‹è½½å®Œæˆ")


# ===================== è¡¥å›¾ï¼ˆæŒ‰ç¼–ç ä»æ•°æ®åº“æŸ¥ URLï¼‰ =====================

def fetch_urls_from_db_by_codes(code_file_path, pgsql_config, table_name):
    code_list = [
        line.strip()
        for line in Path(code_file_path).read_text(encoding="utf-8").splitlines()
        if line.strip()
    ]
    print(f"ğŸ” å…±è¯»å–åˆ° {len(code_list)} ä¸ªå•†å“ç¼–ç ")

    urls: set[str] = set()
    try:
        conn   = psycopg2.connect(**pgsql_config)
        cursor = conn.cursor(cursor_factory=RealDictCursor)

        placeholders = ",".join(["%s"] * len(code_list))
        query = f"""
            SELECT DISTINCT product_code, product_url
            FROM {table_name}
            WHERE product_code IN ({placeholders})
        """
        cursor.execute(query, code_list)
        rows = cursor.fetchall()

        code_to_url = {row["product_code"]: row["product_url"] for row in rows}
        for code in code_list:
            url = code_to_url.get(code)
            if url:
                urls.add(url)
            else:
                print(f"âš ï¸ ç¼–ç æœªæ‰¾åˆ°: {code}")

        cursor.close()
        conn.close()
    except Exception as e:
        print(f"âŒ æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {e}")

    return list(urls)


def download_images_by_code_file(code_txt_path):
    """æŒ‰ç¼–ç æ–‡ä»¶ä»æ•°æ®åº“æŸ¥è¯¢ URL å¹¶è¡¥å›¾ã€‚"""
    urls = fetch_urls_from_db_by_codes(code_txt_path, PGSQL_CONFIG, TABLE_NAME)
    print(f"ğŸ“¦ å…±éœ€ä¸‹è½½ {len(urls)} ä¸ªå•†å“çš„å›¾ç‰‡\n")
    _run_batch(urls)
    print("\nâœ… æ‰€æœ‰æŒ‡å®šå•†å“å›¾ç‰‡ä¸‹è½½å®Œæˆ")


# ===================== ä¸»å…¥å£ =====================

def main():
    download_jacket_images()
    download_lingerie_images()


if __name__ == "__main__":
    main()
