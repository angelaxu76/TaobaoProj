
from brands.clarks.clarks_scraper import ClarksScraper
from common_taobao.db_import import import_txt_to_db
from common_taobao.export_discount_price import export_discount_excel
from brands.clarks.export_skuid_stock import export_stock_excel
from common_taobao.prepare_publication import prepare_products
from common_taobao.generate_reports import generate_publish_report
from config import CLARKS, PGSQL_CONFIG
import psycopg2
import shutil
import datetime

from common_taobao.backup_and_clear_txt import backup_and_clear_txt

def backup_and_clear_publication():
    pub_dir = CLARKS["BASE"] / "publication"
    backup_base = CLARKS["BASE"] / "backup"
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_dir = backup_base / f"publication_{timestamp}"

    # å¤‡ä»½æ•´ä¸ª publication
    if pub_dir.exists():
        shutil.copytree(pub_dir, backup_dir)
        print(f"ğŸ“¦ å·²å¤‡ä»½ publication â†’ {backup_dir}")
    # æ¸…ç©º TXTï¼ˆä½¿ç”¨é€šç”¨æ¨¡å—ï¼‰
    backup_and_clear_txt(CLARKS["TXT_DIR"], CLARKS["BASE"] / "backup")

def main():
    backup_and_clear_publication()

    scraper = ClarksScraper()
    link_file = CLARKS["BASE"] / "publication" / "product_links.txt"
    txt_dir = CLARKS["TXT_DIR"]
    image_dir = CLARKS["IMAGE_DIR"]
    output_dir = CLARKS["OUTPUT_DIR"]
    store_id = "2219163936872"

    # Step 1: æŠ“å–ä¿¡æ¯
    print("\nğŸŸ¡ Step 1ï¼šæŠ“å–å•†å“ä¿¡æ¯")
    with open(link_file, "r", encoding="utf-8") as f:
        for url in [line.strip() for line in f if line.strip()]:
            try:
                product = scraper.fetch_product_info(url)
                code = product["Product Code"]
                txt_file = txt_dir / f"{code}.txt"
                with open(txt_file, "w", encoding="utf-8") as f_txt:
                    for key, value in product.items():
                        f_txt.write(f"{key}: {value}\n")
                print(f"âœ… ä¿¡æ¯å·²ä¿å­˜: {code}")
            except Exception as e:
                print(f"âŒ ä¿¡æ¯æŠ“å–å¤±è´¥: {url} - {e}")

    # Step 2: ä¸‹è½½å›¾ç‰‡
    print("\nğŸŸ¡ Step 2ï¼šä¸‹è½½å•†å“å›¾ç‰‡")
    with open(link_file, "r", encoding="utf-8") as f:
        for url in [line.strip() for line in f if line.strip()]:
            try:
                scraper.fetch_product_images(url, image_dir)
                print(f"âœ… å›¾ç‰‡ä¸‹è½½å®Œæˆ: {url}")
            except Exception as e:
                print(f"âŒ å›¾ç‰‡ä¸‹è½½å¤±è´¥: {url} - {e}")

    # Step 3: å¯¼å…¥æ•°æ®åº“
    print("\nğŸŸ¡ Step 3ï¼šå¯¼å…¥ TXT æ•°æ®åˆ°æ•°æ®åº“")
    conn = psycopg2.connect(**PGSQL_CONFIG)
    import_txt_to_db(txt_dir=txt_dir, brand="clarks", conn=conn)

    # Step 4: å¯¼å‡ºæ‰“æŠ˜ä»· Excel
    print("\nğŸŸ¡ Step 4ï¼šå¯¼å‡ºæ‰“æŠ˜ä»·æ ¼ Excel")
    export_discount_excel(txt_dir, "clarks", output_dir / "æ‰“æŠ˜ä»·æ ¼.xlsx")

    # Step 5: å¯¼å‡ºåº“å­˜ Excel
    print("\nğŸŸ¡ Step 5ï¼šå¯¼å‡ºåº“å­˜ Excel")
    export_stock_excel(txt_dir, "clarks", store_id, output_dir / "åº“å­˜ä¿¡æ¯.xlsx")

    # Step 6: å‡†å¤‡å‘å¸ƒå•†å“
    print("\nğŸŸ¡ Step 6ï¼šå‡†å¤‡å‘å¸ƒå•†å“")
    prepare_products(txt_dir, image_dir, output_dir, "clarks")

    # Step 7: å‘å¸ƒæŠ¥å‘Š
    print("\nğŸŸ¡ Step 7ï¼šç”Ÿæˆå‘å¸ƒçŠ¶æ€æŠ¥å‘Š")
    published_codes = set()
    generate_publish_report(txt_dir, "clarks", published_codes, output_dir / "å‘å¸ƒçŠ¶æ€æŠ¥å‘Š.xlsx")

    print("\nâœ… å…¨éƒ¨æµç¨‹æ‰§è¡Œå®Œæ¯•")

if __name__ == "__main__":
    main()
