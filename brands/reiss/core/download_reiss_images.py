# -*- coding: utf-8 -*-
"""
REISS å›¾ç‰‡ä¸‹è½½è„šæœ¬ï¼ˆSelenium ç‰ˆæœ¬ï¼Œå…¼å®¹ REISS å‘½åï¼šå»æ¨ªçº¿ã€æŠ“ preloadï¼‰

- ä» codes_file è¯»å–å•†å“ç¼–ç 
- åœ¨ TXT_DIR æ‰¾åˆ°å¯¹åº” TXTï¼Œè§£æ URL
- ç”¨ Selenium æ‰“å¼€å•†å“é¡µï¼Œæ‹¿åˆ° HTML
- BeautifulSoup æ±‡æ€» <img src> / data-* / srcset / <link rel=preload imagesrcset>
- è¿‡æ»¤ï¼šæ–‡ä»¶åéœ€åŒ…å«å­—æ¯ "s" ä¸”åŒ…å«ã€Œå½’ä¸€åŒ–åçš„ç¼–ç ã€ï¼ˆå»æ‰éå­—æ¯æ•°å­—ï¼‰
- requests ä¸‹è½½åˆ° IMAGE_DOWNLOAD
"""

import os
import re
import time
from pathlib import Path
from typing import Optional, List, Tuple, Set

import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager

from config import BRAND_CONFIG

# ===== é…ç½® =====
REISS_CFG = BRAND_CONFIG["reiss"]  # ä¸“ç”¨
HEADLESS = True  # æ˜¯å¦æ— å¤´è¿è¡Œ

# ===== å¯åŠ¨ Selenium æµè§ˆå™¨ =====
def make_driver(headless: bool = HEADLESS):
    options = webdriver.ChromeOptions()
    if headless:
        options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument(
        "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    )
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    return driver

# ===== å·¥å…·å‡½æ•° =====
def read_codes(codes_file: Path) -> List[str]:
    with open(codes_file, "r", encoding="utf-8") as f:
        return [line.strip() for line in f if line.strip()]

def find_txt_for_code(txt_dir: Path, code: str) -> Optional[Path]:
    exact = txt_dir / f"{code}.txt"
    if exact.exists():
        return exact
    candidates = list(txt_dir.glob(f"*{code}*.txt"))
    if candidates:
        candidates.sort(key=lambda p: len(p.name))
        return candidates[0]
    return None

def parse_url_from_txt(txt_path: Path) -> Optional[str]:
    url_pat = re.compile(r"(https?://\S+)")
    with open(txt_path, "r", encoding="utf-8") as f:
        for line in f:
            m = url_pat.search(line)
            if m:
                return m.group(1)
    return None

def _norm(s: str) -> str:
    """å½’ä¸€åŒ–æ¯”è¾ƒç”¨ï¼šå…¨å°å†™ + å»æ‰éå­—æ¯æ•°å­—"""
    return re.sub(r"[^a-z0-9]+", "", s.lower())

def extract_codes_from_url(url: str) -> List[str]:
    """æŠŠ URL å„æ®µä½œä¸ºå€™é€‰ç¼–ç ï¼ˆå°¤å…¶æ˜¯æœ€åä¸€æ®µï¼‰"""
    try:
        path = url.split("://", 1)[-1].split("/", 1)[-1]  # å»åŸŸå
    except Exception:
        return []
    parts = [p for p in path.split("/") if p]
    cands = []
    for p in parts:
        if re.match(r"^[A-Za-z0-9_-]+$", p):
            cands.append(p.lower())
    # å»é‡ä¿æŒé¡ºåº
    seen = set(); uniq = []
    for c in cands:
        if c not in seen:
            uniq.append(c); seen.add(c)
    return uniq

def download_image(img_url: str, out_dir: Path):
    try:
        resp = requests.get(img_url, headers={"User-Agent": "Mozilla/5.0"}, stream=True, timeout=15)
        resp.raise_for_status()
        filename = os.path.basename(img_url.split("?")[0])
        out_path = out_dir / filename
        with open(out_path, "wb") as f:
            for chunk in resp.iter_content(1024 * 16):
                if chunk:
                    f.write(chunk)
        print(f"âœ… å›¾ç‰‡å·²ä¿å­˜: {out_path}")
    except Exception as e:
        print(f"âŒ å›¾ç‰‡ä¸‹è½½å¤±è´¥ [{img_url}]: {e}")

def parse_candidate_image_urls(soup: BeautifulSoup) -> List[str]:
    """ä»é¡µé¢å°½å¯èƒ½å¤šåœ°æ”¶é›†å›¾ç‰‡ URLï¼ˆimg/src, data-*, srcset, preload/imagesrcsetï¼‰"""
    candidates: Set[str] = set()

    # <img src> + å¸¸è§æ‡’åŠ è½½å±æ€§
    for img in soup.find_all("img"):
        src = img.get("src")
        if src: candidates.add(src)
        for k in ("data-src", "data-original", "data-lazy", "data-zoom-image"):
            v = img.get(k)
            if v: candidates.add(v)

    # srcset
    for tag in soup.find_all(["img", "source"]):
        srcset = tag.get("srcset")
        if srcset:
            parts = [p.strip().split(" ")[0] for p in srcset.split(",") if p.strip()]
            candidates.update(parts)

    # <link rel="preload" as="image" imagesrcset="...">
    for link in soup.find_all("link", rel=lambda v: v and "preload" in v, attrs={"as": "image"}):
        srcset = link.get("imagesrcset")
        if srcset:
            parts = [p.strip().split(" ")[0] for p in srcset.split(",") if p.strip()]
            candidates.update(parts)

    # æ¸…æ´—ï¼šè¡¥åè®®ã€å» queryã€å¿½ç•¥ç›¸å¯¹é“¾æ¥
    cleaned: List[str] = []
    for u in candidates:
        if not u: continue
        if u.startswith("//"): u = "https:" + u
        if u.startswith("/"):  # ç›¸å¯¹è·¯å¾„è¿™é‡Œå…ˆå¿½ç•¥
            continue
        cleaned.append(u.split("?")[0])

    # å»é‡ä¿æŒé¡ºåº
    seen = set(); uniq = []
    for u in cleaned:
        if u not in seen:
            uniq.append(u); seen.add(u)
    return uniq

def extract_and_download_images(html: str, url: str, code: str, image_dir: Path, debug_dir: Path):
    soup = BeautifulSoup(html, "html.parser")

    # å€™é€‰ç¼–ç é›†åˆï¼šæ–‡ä»¶é‡Œçš„ code + URL æ®µè½ï¼ˆå¦‚ an6312ï¼‰
    candidates = {code.lower()}
    for c in extract_codes_from_url(url):
        candidates.add(c)
    cand_norm = {_norm(c) for c in candidates if c}  # å½’ä¸€åŒ–

    urls = parse_candidate_image_urls(soup)

    matched: List[str] = []
    for u in urls:
        fname = os.path.basename(u)
        if "s" not in fname.lower():  # ä¿æŒä½ çš„æ—§è§„åˆ™ï¼šæ–‡ä»¶åé‡Œè¦æœ‰å­—æ¯ s
            continue
        f_norm = _norm(fname)
        if any(cn and cn in f_norm for cn in cand_norm):
            matched.append(u)

    if matched:
        for idx, img_url in enumerate(matched, 1):
            print(f"ğŸ–¼ï¸ æ‰¾åˆ°å›¾ç‰‡ [{idx}]: {img_url}")
            download_image(img_url, image_dir)
    else:
        print(f"âš  æœªæ‰¾åˆ°ä»»ä½•å›¾ç‰‡ [{code}]ï¼Œä¿å­˜ HTML ä»¥ä¾¿æ’æŸ¥")
        debug_file = debug_dir / f"{_norm(code) or code}.html"
        debug_file.write_text(html, encoding="utf-8", errors="ignore")

# ===== ä¸»æµç¨‹ =====
def download_reiss_images_from_codes(codes_file: Path):
    txt_dir = Path(REISS_CFG["TXT_DIR"])
    image_dir = Path(REISS_CFG["IMAGE_DOWNLOAD"])
    debug_dir = codes_file.parent / "DEBUG"

    image_dir.mkdir(parents=True, exist_ok=True)
    debug_dir.mkdir(parents=True, exist_ok=True)

    codes = read_codes(codes_file)
    print(f"ğŸš€ REISS å›¾ç‰‡ä¸‹è½½ï¼ˆSeleniumï¼‰ | å…± {len(codes)} ä¸ªç¼–ç ")

    driver = make_driver()
    for idx, code in enumerate(codes, 1):
        print(f"\n[{idx}/{len(codes)}] ===== {code} =====")
        txt_path = find_txt_for_code(txt_dir, code)
        if not txt_path:
            print(f"âš  æœªæ‰¾åˆ° TXTï¼š{code}")
            continue
        url = parse_url_from_txt(txt_path)
        if not url:
            print(f"âš  TXT æœªè§£æåˆ° URLï¼š{code}")
            continue

        try:
            driver.get(url)
            time.sleep(3)  # è®©é¦–å±/é¢„åŠ è½½å®Œæˆ
            html = driver.page_source
            extract_and_download_images(html, url, code, image_dir, debug_dir)
        except Exception as e:
            print(f"âŒ é¡µé¢åŠ è½½å¤±è´¥ï¼š{url} | {e}")

    driver.quit()
    print(f"\nâœ… å®Œæˆï¼šå›¾ç‰‡ä¿å­˜åœ¨ {image_dir}")

# ===== ç›´æ¥è¿è¡Œ =====
if __name__ == "__main__":
    sample_codes_file = Path(r"D:\TB\Products\reiss\repulibcation\publication_codes_outerwear.txt")
    download_reiss_images_from_codes(sample_codes_file)
