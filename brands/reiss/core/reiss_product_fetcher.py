from __future__ import annotations
import json, re, time
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from typing import Dict, List, Tuple, Optional

import requests
from bs4 import BeautifulSoup

# === é¡¹ç›®å†…è·¯å¾„/é…ç½® ===
# ä½ ä¹‹å‰å·²ç»åœ¨ config é‡ŒåŠ äº† REISS é…ç½®ï¼Œè¿™é‡Œç›´æ¥å¼•ç”¨
try:
    from config import REISS  # åŒ…å« TXT_DIR / LINKS_FILE / BRAND / ...
except ImportError:
    # å…è®¸ç‹¬ç«‹è¿è¡Œï¼ˆå¯æ‰‹å·¥ä¼ å‚ï¼‰
    REISS = None

# === æ–‡æœ¬å†™å…¥ï¼šæ²¿ç”¨ä½ ä¸Šä¼ çš„ writerï¼Œè¾“å‡ºå®Œå…¨å…¼å®¹ parse_txt / parse_barbour_jingya ç­‰ ===
from txt_writer import format_txt

UA = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
    "AppleWebKit/537.36 (KHTML, like Gecko) "
    "Chrome/126.0.0.0 Safari/537.36"
)
HEADERS = {"User-Agent": UA, "Accept-Language": "en-GB,en;q=0.9,zh-CN;q=0.8"}

def _clean_text(s: str) -> str:
    return re.sub(r"\s+", " ", s).strip()

def _money_to_float(text: Optional[str]) -> float:
    if not text:
        return 0.0
    t = text.replace(",", "")
    m = re.search(r"(\d+(?:\.\d+)?)", t)
    return float(m.group(1)) if m else 0.0

def _fetch_html(url: str, retry: int = 3, timeout: int = 25) -> str:
    last_exc = None
    for i in range(retry):
        try:
            r = requests.get(url, headers=HEADERS, timeout=timeout)
            if r.status_code == 200 and r.text:
                return r.text
        except Exception as e:
            last_exc = e
        time.sleep(1.2)
    raise RuntimeError(f"GET {url} failed: {last_exc or 'HTTP '+str(r.status_code)}")

def _product_code_from_url(url: str) -> Optional[str]:
    # /style/su583537/ap8544 -> ap8544 -> AP8-544
    m = re.search(r"/style/[a-z0-9\-]+/([a-z0-9]{6,8})", url, flags=re.I)
    if not m:
        return None
    pid = m.group(1).upper()
    if len(pid) >= 4:
        return f"{pid[:3]}-{pid[3:]}"
    return pid

def _parse_next_data(soup: BeautifulSoup) -> Optional[Dict]:
    tag = soup.find("script", id="__NEXT_DATA__", type="application/json")
    if not tag:
        return None
    data = json.loads(tag.string or "{}")
    q = (((data.get("props") or {}).get("pageProps") or {})
         .get("dehydratedState") or {}).get("queries") or []
    # æ‰¾åˆ° queryKey = ["product", "<pid>"] çš„é¡¹
    for item in q:
        state = item.get("state") or {}
        key = item.get("queryKey")
        if isinstance(key, list) and key and key[0] == "product":
            if isinstance(state.get("data"), dict):
                return state["data"]
    return None

def _parse_dom_prices(soup: BeautifulSoup) -> Tuple[float, float]:
    was_el = soup.select_one('[data-testid="product-was-price"]')
    now_el = soup.select_one('[data-testid="product-now-price"] span')
    original = _money_to_float(was_el.get_text(strip=True) if was_el else "")
    discount = _money_to_float(now_el.get_text(strip=True) if now_el else "")
    if original == 0.0 and discount > 0.0:
        original = discount  # éæŠ˜æ‰£åœºæ™¯åªæœ‰ä¸€ä¸ªä»·
    return original, discount

def _parse_dom_color(soup: BeautifulSoup) -> str:
    el = soup.select_one('span[data-testid="selected-colour-label"]')
    return _clean_text(el.get_text()) if el else ""

def _parse_dom_desc_features(soup: BeautifulSoup) -> Tuple[str, str]:
    # æè¿°
    desc_el = soup.select_one('p[data-testid="item-description"]')
    description = _clean_text(desc_el.get_text()) if desc_el else ""
    # ç‰¹æ€§ç‚¹
    feats = [ _clean_text(li.get_text())
              for li in soup.select('li[data-translate-id="tov-bullet"]') ]
    feature_str = " | ".join([f for f in feats if f]) if feats else ""
    return description, feature_str

def _parse_dom_sizes_fallback(soup: BeautifulSoup) -> Dict[str, str]:
    # å…œåº•ï¼šç”¨æŒ‰é’® aria-label æå–ï¼ˆ"16 unavailable" / "10 available"...ï¼‰
    size_map = {}
    for b in soup.select('[data-testid="size-chips-button-group"] button[aria-label]'):
        size_txt = _clean_text(b.get_text())
        size = size_txt or re.sub(r"[^\d\.A-Za-z/]", "", b.get("aria-label","").split()[0])
        label = (b.get("aria-label") or "").lower()
        status = "æ— è´§" if "unavailable" in label else "æœ‰è´§"
        if size:
            size_map[size] = status
    return size_map

def _parse_name_from_title(soup: BeautifulSoup) -> str:
    t = (soup.title.string or "") if soup.title else ""
    t = t.replace(" - REISS", "")
    t = re.sub(r"^\s*Reiss\s+", "", t, flags=re.I)  # å»æ‰å‰ç¼€å“ç‰Œ
    return _clean_text(t)

def parse_reiss_product(html: str, url: str) -> Dict:
    soup = BeautifulSoup(html, "html.parser")

    # 1) ä¼˜å…ˆä» __NEXT_DATA__ æ‹¿ç»“æ„åŒ–æ•°æ®ï¼ˆæœ‰ gender / category / sizes ç­‰ï¼‰
    next_data = _parse_next_data(soup)

    # 2) åç§°/é¢œè‰²/æè¿°/ç‰¹æ€§ï¼ˆDOMï¼‰
    product_title = _parse_name_from_title(soup)
    color = _parse_dom_color(soup)
    description, feature_str = _parse_dom_desc_features(soup)

    # 3) ç¼–ç ï¼ˆå…ˆ DOMï¼Œå URL å…œåº•ï¼‰
    product_code = ""
    code_label = soup.find(string=re.compile(r"Product Code", re.I))
    if code_label:
        code_span = (code_label.find_parent().find_next("span")
                     if hasattr(code_label, "find_parent") else None)
        if code_span:
            product_code = _clean_text(code_span.get_text())
    if not product_code:
        pc = _product_code_from_url(url)
        if pc:
            product_code = pc

    # 4) ä»·æ ¼ï¼ˆDOMï¼›å¿…è¦æ—¶ç”¨ JSON sizes çš„ç»Ÿä¸€ä»·å…œåº•ï¼‰
    original, discount = _parse_dom_prices(soup)
    if next_data and discount == 0.0:
        # éƒ¨åˆ†é¡µé¢æ²¡æœ‰å±•ç¤ºæŠ˜åä»·ï¼Œç”¨ options é‡Œçš„ priceUnformatted
        try:
            opts = (next_data.get("options") or {}).get("options") or []
            if opts:
                discount = float(opts[0].get("priceUnformatted") or 0) or discount
                if original == 0.0:
                    original = discount
        except Exception:
            pass

    # 5) æ€§åˆ« / ç±»åˆ«
    gender = ""
    style_category = ""
    if next_data:
        gender = (next_data.get("gender") or "").strip()  # e.g. "Women" / "Men"
        style_category = (next_data.get("category") or "").strip()

    # 6) å°ºç ä¸åº“å­˜
    size_map: Dict[str, str] = {}
    size_detail: Dict[str, Dict] = {}

    if next_data:
        try:
            for o in (next_data.get("options") or {}).get("options", []):
                name = str(o.get("name") or "").strip()  # æ˜¾ç¤ºå°ºç ï¼ˆå¦‚ "10"ï¼‰
                st = (o.get("stockStatus") or "").lower()
                status = "æœ‰è´§" if st in ("instock", "lowstock") else "æ— è´§"
                if name:
                    size_map[name] = status
                    # Reiss è¿™è¾¹æ²¡æœ‰æ˜ç¡®åº“å­˜æ•°/EANï¼Œè¿™é‡Œåªç•™ç©ºç»“æ„ï¼Œå…¼å®¹ txt_parser
                    size_detail[name] = {"stock_count": 0, "ean": ""}
        except Exception:
            pass

    if not size_map:
        size_map = _parse_dom_sizes_fallback(soup)

    # 7) ç‰¹æ€§/æè¿°å…œåº•ï¼ˆè‹¥ä¸ºç©ºç”¨ç®€çŸ­ç»„åˆï¼‰
    if not description and feature_str:
        description = feature_str
    if not feature_str and description:
        # æŠŠæè¿°é¦–å¥å½“ç‰¹æ€§ç®€è¿°
        feature_str = description.split(".")[0].strip()

    # 8) ç»„ç»‡å†™å…¥å­—æ®µï¼ˆå®Œå…¨è´´åˆ txt_writer çš„é”®ï¼‰
    info = {
        "Product Code": product_code or "UNKNOWN",
        "Product Name": product_title or "No Name",
        "Product Description": description or "No Data",
        "Product Gender": gender,                     # å¯ä¸ºç©º
        "Product Color": color,                       # å¯ä¸ºç©º
        "Product Price": str(original or 0),
        "Adjusted Price": str(discount or original or 0),
        "Product Material": "No Data",
        "Style Category": style_category,             # å¯ä¸ºç©ºï¼ˆä¾‹å¦‚ Dressesï¼‰
        "Feature": feature_str or "No Data",
        "SizeMap": size_map,                          # e.g. {"10":"æœ‰è´§","16":"æ— è´§"}
        "SizeDetail": size_detail,                    # ç»“æ„ä¿ç•™ï¼Œå€¼ç¼ºçœ
        "Source URL": url
    }
    return info

# === å…¬å…±å…¥å£ ===

def fetch_one_and_write(url: str, save_dir: Path, brand: str = "reiss") -> Tuple[str, bool, str]:
    try:
        html = _fetch_html(url)
        info = parse_reiss_product(html, url)

        # æ–‡ä»¶åä¼˜å…ˆç”¨ Product Codeï¼›é€€åŒ–ç”¨ URL å°¾æ®µ
        fname = info["Product Code"].replace("/", "_")
        if not fname or fname == "UNKNOWN":
            fname = (_product_code_from_url(url) or re.sub(r"\W+", "_", url))[:80]
        path = Path(save_dir) / f"{fname}.txt"

        format_txt(info, path, brand=brand)
        print(f"âœ… TXT å†™å…¥: {path.name}")
        return url, True, ""
    except Exception as e:
        # é¡µå¤±è´¥ä¹Ÿä¸å½±å“å…¶å®ƒ URLï¼Œä¸”å°½é‡è¾“å‡ºä¸€ä¸ªæœ€å° TXTï¼ˆåªå« URL + å ä½ï¼‰
        try:
            print(f"ğŸ’¥ è§£æå¤±è´¥ï¼ˆç»§ç»­ï¼‰ï¼š{url} -> {e}")
            minimal = {
                "Product Code": _product_code_from_url(url) or "UNKNOWN",
                "Product Name": "No Name",
                "Product Description": "No Data",
                "Product Gender": "",
                "Product Color": "",
                "Product Price": "0",
                "Adjusted Price": "0",
                "Product Material": "No Data",
                "Style Category": "",
                "Feature": "No Data",
                "SizeMap": {},          # ä¸ºç©ºåˆ™ parse_generic ä¸ä¼šå…¥åº“ï¼Œç•™å¾…å¤æŠ“
                "SizeDetail": {},
                "Source URL": url
            }
            # å³ä½¿å¤±è´¥ä¹Ÿå°½é‡ç•™ä¸€ä»½è®°å½•
            fallback_name = minimal["Product Code"] or "UNKNOWN"
            path = Path(save_dir) / f"{fallback_name}.txt"
            format_txt(minimal, path, brand=brand)
            print(f"â†³ å·²å†™å…¥æœ€å° TXTï¼ˆä¾›è¿½è¸ªï¼‰: {path.name}")
        except Exception as ee:
            print(f"â€¼ï¸ æœ€å° TXT å†™å…¥ä¹Ÿå¤±è´¥ï¼š{ee}")
        return url, False, str(e)

def reiss_fetch_all(
    urls_file: Optional[Path] = None,
    save_dir: Optional[Path] = None,
    max_workers: int = 8
):
    if REISS:
        urls_file = urls_file or REISS["LINKS_FILE"]
        save_dir = save_dir or REISS["TXT_DIR"]
    if not urls_file or not save_dir:
        raise ValueError("ç¼ºå°‘ urls_file / save_dirï¼Œè¯·ä¼ å…¥æˆ–åœ¨ config ä¸­é…ç½® REISSã€‚")

    Path(save_dir).mkdir(parents=True, exist_ok=True)

    with open(urls_file, "r", encoding="utf-8") as f:
        urls = [u.strip() for u in f if u.strip()]

    ok = fail = 0
    errs: List[str] = []

    with ThreadPoolExecutor(max_workers=max_workers) as ex:
        futs = [ex.submit(fetch_one_and_write, u, Path(save_dir), "reiss") for u in urls]
        for fut in as_completed(futs):
            _, success, msg = fut.result()
            ok += 1 if success else 0
            fail += 0 if success else 1
            if not success:
                errs.append(msg)

    print(f"ğŸ¯ å®Œæˆï¼šæˆåŠŸ {ok}ï¼Œå¤±è´¥ {fail}ï¼Œè¾“å‡ºç›®å½•ï¼š{save_dir}")
    if fail:
        print("âš ï¸ å¤±è´¥åŸå› ç¤ºä¾‹ï¼š", errs[:3])
