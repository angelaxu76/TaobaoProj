# -*- coding: utf-8 -*-
import os
import re
import json
import time
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin, parse_qs
from config import BARBOUR

# ========== å¯è°ƒå‚æ•° ==========
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36"
}
TIMEOUT = 20
RETRY = 3

# ä½ ç»™çš„ Salsify æ¨¡æ¿ï¼ˆæŠŠ picture_name æ”¾è¿›å»ï¼‰
SALSIFY_TMPL = (
    "https://images.salsify.com/image/upload/"
    "s--i74AAA0n--/c_fill,w_1000,h_1334,f_auto/{name}.jpg"
)

# ========== å·¥å…·å‡½æ•° ==========
def extract_code_and_name(url: str):
    """
    è¾“å…¥: https://www.barbour.com/gb/zola-quilted-jacket-LQU1822CR11.html
    è¾“å‡º: ("LQU1822CR11", "zola-quilted-jacket")
    """
    filename = os.path.basename(urlparse(url).path)  # zola-quilted-jacket-LQU1822CR11.html
    if filename.endswith(".html"):
        filename = filename[:-5]
    parts = filename.split("-")
    code = parts[-1]
    name = "-".join(parts[:-1])
    return code, name

def _expand_srcset(srcset: str):
    """è§£æ srcset -> [(url, width_int)]ï¼ŒæŒ‰å®½åº¦é™åº"""
    items = []
    for part in srcset.split(","):
        cand = part.strip()
        if not cand:
            continue
        pieces = cand.split()
        url = pieces[0]
        width = 0
        if len(pieces) > 1 and pieces[1].endswith("w"):
            try:
                width = int(pieces[1][:-1])
            except Exception:
                width = 0
        items.append((url, width))
    items.sort(key=lambda x: x[1], reverse=True)
    return items

def _basename_no_ext(path: str) -> str:
    """
    å–ä¸å¸¦æ‰©å±•åçš„basenameï¼Œå¹¶å»æ‰ _001/_002 ä¹‹ç±»å°¾ç¼€ã€‚
    e.g. e6760d..._003.webp -> e6760d...
    """
    base = os.path.basename(urlparse(path).path)
    if "." in base:
        base = base[:base.rfind(".")]
    base = re.sub(r"_(\d{3})$", "", base)
    return base

def _is_hash_like(name: str) -> bool:
    """æ˜¯å¦åƒåŠ å¯†åï¼ˆè¾ƒé•¿çš„å­—æ¯æ•°å­—/ä¸‹åˆ’çº¿/çŸ­æ¨ªçº¿ä¸²ï¼‰"""
    return bool(re.fullmatch(r"[A-Za-z0-9_-]{20,}", name))

def _uniq_keep_order(seq):
    seen, out = set(), []
    for x in seq:
        if x and x not in seen:
            seen.add(x)
            out.append(x)
    return out

# ========== æå–å›¾ç‰‡é“¾æ¥ ==========
def extract_image_urls_ldjson(page_content: str):
    """ä¿æŒåŸé€»è¾‘ï¼šåªä» JSON-LD çš„ image æ•°ç»„å–å›¾"""
    soup = BeautifulSoup(page_content, "html.parser")
    script_tag = soup.find("script", type="application/ld+json")
    if not script_tag or not script_tag.string:
        return []
    try:
        data = json.loads(script_tag.string.strip())
        images = data.get("image", [])
        if isinstance(images, list):
            return images
        elif isinstance(images, str):
            return [images]
    except Exception as e:
        print(f"[è§£æå¤±è´¥] JSON-LD é”™è¯¯: {e}")
    return []

def extract_picture_hash_urls(page_content: str):
    """
    æ–°å¢é€»è¾‘ï¼šä» <picture>/<img>/<source> æŠ“åˆ° webp çš„â€œå“ˆå¸Œåâ€ï¼Œ
    æ‹¼æˆ Salsify URL: SALSIFY_TMPL.format(name=<hash>)
    """
    soup = BeautifulSoup(page_content, "html.parser")
    urls = []

    # ä¸»å›¾åŒºåŸŸ
    pictures = soup.select(".picture__wrapper picture")
    if not pictures:
        pictures = soup.find_all("picture")  # å…œåº•

    for pic in pictures:
        # ä¼˜å…ˆ <img src>
        img = pic.find("img")
        if img:
            for attr in ("src", "data-src"):
                val = img.get(attr)
                if val:
                    name = _basename_no_ext(val)
                    if _is_hash_like(name):
                        urls.append(SALSIFY_TMPL.format(name=name))
                    break

        # å†çœ‹ <source srcset> æœ€å¤§å›¾
        for source in pic.find_all("source"):
            srcset = source.get("srcset") or source.get("data-srcset") or ""
            if srcset:
                items = _expand_srcset(srcset)
                if items:
                    first_url = items[0][0]
                    name = _basename_no_ext(first_url)
                    if _is_hash_like(name):
                        urls.append(SALSIFY_TMPL.format(name=name))

    # å¯é€‰ï¼šæ•£è½ img å…œåº•
    if not urls:
        for img in soup.find_all("img"):
            for attr in ("srcset", "data-srcset"):
                if img.get(attr):
                    items = _expand_srcset(img.get(attr))
                    if items:
                        name = _basename_no_ext(items[0][0])
                        if _is_hash_like(name):
                            urls.append(SALSIFY_TMPL.format(name=name))
                            break
            for attr in ("src", "data-src"):
                if img.get(attr):
                    name = _basename_no_ext(img.get(attr))
                    if _is_hash_like(name):
                        urls.append(SALSIFY_TMPL.format(name=name))
                        break

    return _uniq_keep_order(urls)

# ========== â€œå›¾ç‰‡å”¯ä¸€æ ‡è¯†â€ ç”Ÿæˆè§„åˆ™ ==========
def image_identity(url: str) -> str:
    """
    ç”¨äºå»é‡çš„â€œå”¯ä¸€æ ‡è¯†â€ï¼š
    1) Salsifyï¼šæœ€åä¸€æ®µ basenameï¼ˆä¸å¸¦æ‰©å±•åï¼‰ï¼Œå»æ‰ _001 ç­‰åç¼€
    2) media.barbour.com/i/barbour/<id>ï¼šç”¨ <id>ï¼ˆå»æ‰å‚æ•°ã€æ‰©å±•åã€_001ï¼‰
    3) å…¶å®ƒï¼šå¦‚æœ basename åƒå“ˆå¸Œåˆ™ç”¨å“ˆå¸Œï¼›å¦åˆ™å›é€€åˆ°å®Œæ•´ URLï¼ˆé¿å…è¯¯åˆå¹¶ï¼‰
    """
    parsed = urlparse(url)
    path = parsed.path
    host = (parsed.netloc or "").lower()

    base_no_ext = _basename_no_ext(path)

    # Salsifyï¼š/image/upload/.../<hash>.jpg
    if "images.salsify.com" in host:
        return base_no_ext

    # Barbour Scene7ï¼š/i/barbour/<id>...
    if "media.barbour.com" in host and path.startswith("/i/"):
        # å– /i/barbour/<id> çš„ <id> éƒ¨åˆ†
        segs = path.split("/")
        # å®‰å…¨åˆ¤æ–­
        if len(segs) >= 4 and segs[2] == "barbour":
            return _basename_no_ext(segs[3])

    # å…¶å®ƒï¼šå¦‚æœåƒå“ˆå¸Œï¼Œå°±ç”¨å“ˆå¸Œï¼›å¦åˆ™ç”¨å®Œæ•´ URL
    if _is_hash_like(base_no_ext):
        return base_no_ext

    return url  # ä¿å®ˆï¼šä»¥å®Œæ•´ URL ä¸ºæ ‡è¯†ï¼Œé¿å…è¯¯åˆå¹¶

# ========== ä¸‹è½½éƒ¨åˆ† ==========
def _get_with_retry(session: requests.Session, url: str):
    last_err = None
    for _ in range(RETRY):
        try:
            r = session.get(url, headers=HEADERS, timeout=TIMEOUT)
            r.raise_for_status()
            return r
        except Exception as e:
            last_err = e
            time.sleep(0.8)
    raise last_err

def collect_all_image_urls(html: str):
    """
    æ±‡æ€»ï¼šJSON-LD + <picture>ï¼Œè¿”å›æŒ‰å‡ºç°é¡ºåºçš„åˆ—è¡¨ï¼ˆä¸å»é‡ï¼‰
    """
    urls = []
    urls += extract_image_urls_ldjson(html)     # å…ˆ JSON-LDï¼ˆä½ çš„åŸé€»è¾‘ï¼‰
    urls += extract_picture_hash_urls(html)     # å†è¡¥ <picture>
    return [u for u in urls if u]

def dedupe_by_identity(urls):
    """
    æŒ‰â€œå›¾ç‰‡å”¯ä¸€æ ‡è¯†â€å»é‡ï¼Œä¿æŒé¦–æ¬¡å‡ºç°é¡ºåºã€‚
    è¿”å›ï¼š[(identity, url)] åªä¿ç•™æ¯ä¸ª identity çš„ç¬¬ä¸€æ¡ URL
    """
    first_index = {}
    first_url = {}
    for idx, u in enumerate(urls):
        ident = image_identity(u)
        if ident not in first_index:
            first_index[ident] = idx
            first_url[ident] = u
    # æŒ‰é¦–æ¬¡å‡ºç°é¡ºåºæ’åº
    ordered_ids = sorted(first_index.items(), key=lambda x: x[1])
    return [(ident, first_url[ident]) for ident, _ in ordered_ids]

def download_images_for_page(session: requests.Session, page_url: str, out_dir: str, code: str, name: str):
    """
    1) æ”¶é›†å…¨éƒ¨å€™é€‰é“¾æ¥åˆ°å˜é‡
    2) åŸºäºâ€œåŠ å¯†å/å”¯ä¸€æ ‡è¯†â€å»é‡
    3) æŒ‰é¡ºåºä¸‹è½½å¹¶æŒ‰ {code}-{name}_{i}.jpg å‘½å
    """
    html_resp = _get_with_retry(session, page_url)
    html = html_resp.text

    # 1) æ”¶é›†ï¼ˆä½ æƒ³çœ‹ä¹Ÿå¯ä»¥ print å‡ºæ¥ï¼‰
    candidates = collect_all_image_urls(html)

    # 2) å»é‡ï¼ˆåŸºäº identityï¼‰
    unique_list = dedupe_by_identity(candidates)

    if not unique_list:
        print(f"âš ï¸ æœªå‘ç°å›¾ç‰‡: {page_url}")
        return 0

    # 3) ä¸‹è½½
    count = 0
    for i, (ident, img_url) in enumerate(unique_list, 1):
        filename = f"{code}-{name}_{i}.jpg"
        save_path = os.path.join(out_dir, filename)
        try:
            img_resp = _get_with_retry(session, img_url)
            with open(save_path, "wb") as f:
                f.write(img_resp.content)
            count += 1
            print(f"âœ… å·²ä¿å­˜: {filename}  <- {ident}")
        except Exception as e:
            print(f"âŒ ä¸‹è½½å¤±è´¥: {img_url} -> {filename}ï¼Œé”™è¯¯: {e}")
    return count

# ========== ä¸»æµç¨‹ ==========
def download_barbour_images():
    links_file = BARBOUR["LINKS_FILE"]
    image_folder = BARBOUR["IMAGE_DOWNLOAD"]
    os.makedirs(image_folder, exist_ok=True)

    with open(links_file, "r", encoding="utf-8") as f:
        urls = [line.strip() for line in f if line.strip()]

    print(f"ğŸ“¦ å…± {len(urls)} ä¸ªå•†å“é“¾æ¥ï¼Œå¼€å§‹ä¾æ¬¡ä¸‹è½½...")

    with requests.Session() as session:
        for idx, url in enumerate(urls, 1):
            try:
                code, name = extract_code_and_name(url)
                saved = download_images_for_page(session, url, image_folder, code, name)
                print(f"ğŸ‘‰ [{idx}/{len(urls)}] {url} ä¸‹è½½å¼ æ•°: {saved}")
            except Exception as e:
                print(f"âŒ [{idx}/{len(urls)}] å¤±è´¥: {url}ï¼Œé”™è¯¯: {e}")

    print("ğŸ¯ æ‰€æœ‰å›¾ç‰‡å¤„ç†å®Œæ¯•ã€‚")

if __name__ == "__main__":
    download_barbour_images()
