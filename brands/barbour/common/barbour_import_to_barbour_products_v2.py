# barbour_import_to_barbour_products.py
# -*- coding: utf-8 -*-
"""
V3: å¯¼å…¥ç»Ÿä¸€æ¨¡æ¿ TXT åˆ° barbour_productsï¼ˆå…¼å®¹æ—§æ¨¡æ¿ï¼‰
æ”¹åŠ¨ç‚¹ï¼š
- å†™å…¥å­—æ®µä» match_keywords -> match_keywords_l1 / match_keywords_l2
- L1ï¼šä¼˜å…ˆä½¿ç”¨ keyword_lexicon (brand=barbour, level=1) å‘½ä¸­ï¼›è‹¥ L1 å‘½ä¸­ä¸ºç©ºï¼Œåˆ™å›é€€æ—§çš„ extract_match_keywordsï¼ˆå…œåº•ï¼‰
- L2ï¼šä½¿ç”¨ keyword_lexicon (brand=barbour, level=2) å‘½ä¸­ï¼ˆå¯ä¸ºç©ºï¼‰
"""

from __future__ import annotations
from pathlib import Path
from typing import List, Dict, Tuple
import psycopg2
import re
import unicodedata
import sys

from config import PGSQL_CONFIG, BARBOUR

# â€”â€” å¯é€‰ï¼šæ ‡é¢˜ç”Ÿæˆï¼ˆå­˜åœ¨åˆ™ç”¨ï¼Œä¸å­˜åœ¨å¿½ç•¥ï¼‰â€”â€”
try:
    from generate_barbour_taobao_title import generate_barbour_taobao_title
except Exception:
    generate_barbour_taobao_title = None  # æ²¡æœ‰ä¹Ÿä¸å½±å“


# -------------------- åŸºç¡€å·¥å…· --------------------
COMMON_WORDS = {
    "jacket", "coat", "gilet", "vest", "shirt", "top",
    "t-shirt", "pants", "trousers", "shorts", "parka",
    "barbour", "mens", "women", "ladies", "kids"
}

# Barbour ç¼–ç è¯†åˆ«ï¼šå¦‚ MWX0339NY91 / LWX0339OL51 / LBA0400BK111 ç­‰
RE_CODE = re.compile(r'[A-Z]{3}\d{3,4}[A-Z]{2,3}\d{2,3}')


def normalize_text(text: str) -> str:
    return unicodedata.normalize("NFKD", text or "").encode("ascii", "ignore").decode("ascii")


def extract_match_keywords(style_name: str) -> List[str]:
    """
    æ—§é€»è¾‘ï¼šè‡ªç”±æ‹†è¯ + COMMON_WORDS è¿‡æ»¤
    ï¼ˆV3 ä»ä¿ç•™ä½œä¸ºå…œåº•ï¼‰
    """
    style_name = normalize_text(style_name or "")
    cleaned = re.sub(r"[^\w\s]", "", style_name)
    words = [w.lower() for w in cleaned.split() if len(w) >= 3]
    return [w for w in words if w not in COMMON_WORDS]


# -------------------- Lexicon (L1/L2) --------------------
_LEXICON_CACHE: dict[tuple[str, int], set[str]] = {}


def load_lexicon_keywords_from_db(conn, brand: str = "barbour", level: int = 1) -> set[str]:
    """
    ä» keyword_lexicon è¯»å–è¯åº“ï¼ˆlevel=1=L1, level=2=L2ï¼‰ã€‚
    åŒä¸€æ¬¡è¿è¡Œåšç¼“å­˜ï¼šæ¯ä¸ª (brand, level) åªæŸ¥ä¸€æ¬¡ã€‚
    """
    brand = (brand or "barbour").strip().lower()
    level = int(level)
    key = (brand, level)
    if key in _LEXICON_CACHE:
        return _LEXICON_CACHE[key]

    sql = """
    SELECT keyword
    FROM keyword_lexicon
    WHERE brand=%s AND level=%s AND is_active=true
    """
    with conn.cursor() as cur:
        cur.execute(sql, (brand, level))
        words = {str(row[0]).strip().lower() for row in cur.fetchall() if row and row[0]}

    _LEXICON_CACHE[key] = words
    return words


def _tokenize(text: str) -> list[str]:
    t = normalize_text(text or "").lower()
    t = re.sub(r"[^a-z0-9\s]", " ", t)
    return [w for w in t.split() if len(w) >= 3]


def _dedupe_keep_order(words: list[str]) -> list[str]:
    seen = set()
    out = []
    for w in words:
        if w not in seen:
            seen.add(w)
            out.append(w)
    return out


def extract_keywords_by_lexicon(text: str, lex_set: set[str]) -> list[str]:
    """åœ¨æ–‡æœ¬ä¸­å‘½ä¸­ lexicon è¯ï¼ˆå»é‡ä¿åºï¼‰ã€‚"""
    tokens = _tokenize(text)
    hits = [w for w in tokens if w in lex_set]
    return _dedupe_keep_order(hits)


def guess_product_code_from_filename(fp: Path) -> str | None:
    """ä»æ–‡ä»¶åä¸­çŒœæµ‹ product_codeï¼ˆå¦‚ MWX0339NY91.txtï¼‰ã€‚"""
    m = RE_CODE.search(fp.stem.upper())
    return m.group(0) if m else None


def validate_minimal_fields(rec: Dict) -> Tuple[bool, List[str]]:
    # âœ… æ–°è¡¨ï¼šL1 å¿…é¡»æœ‰ï¼ˆå¬å›é”šç‚¹ï¼‰
    required = ["product_code", "style_name", "color", "size", "match_keywords_l1"]
    missing = [k for k in required if not rec.get(k)]
    return (len(missing) == 0, missing)


# -------------------- ç»Ÿä¸€æ¨¡æ¿è§£æ --------------------
def _extract_field(text: str, label_re: str) -> str | None:
    """æå–å•è¡Œå­—æ®µï¼šå¦‚ Product Name: ... / Product Color: ..."""
    m = re.search(rf'{label_re}\s*:\s*([^\n\r]+)', text, flags=re.S)
    return m.group(1).strip() if m else None


def _extract_multiline_field(text: str, label_re: str) -> str | None:
    """æå–å¤šè¡Œå­—æ®µï¼šå¦‚ Product Description: ...ï¼ˆç›´åˆ°ä¸‹ä¸€ä¸ªâ€œWord:â€å­—æ®µï¼‰"""
    m = re.search(rf'{label_re}\s*:\s*(.+)', text, flags=re.S)
    if not m:
        return None
    tail = m.group(1)
    parts = re.split(r'\n\s*[A-Z][A-Za-z ]+:\s*', tail, maxsplit=1)
    return parts[0].strip() if parts else None


def _parse_sizes_from_product_size_line(text: str) -> List[str]:
    """ç»Ÿä¸€æ¨¡æ¿ï¼šProduct Size: 34:æœ‰è´§;36:æ— è´§;M:æœ‰è´§ ..."""
    line = _extract_field(text, r'(?i)Product\s+Size')
    if not line:
        return []
    sizes = []
    for token in line.split(";"):
        token = token.strip()
        if not token:
            continue
        size = token.split(":", 1)[0].strip()
        if size and size not in sizes:
            sizes.append(size)
    return sizes


# -------------------- æ—§æ¨¡æ¿å…¼å®¹ --------------------
def _extract_sizes_from_offer_list_block(text: str) -> list[str]:
    """æ—§ï¼šOffer List: å—ä¸­è§£æç¬¬ä¸€åˆ—å°ºç ï¼ˆsize|price|stock|can_orderï¼‰"""
    sizes = []
    in_block = False
    for line in text.splitlines():
        if not in_block:
            if re.search(r'^\s*Offer\s+List\s*:\s*$', line, flags=re.I):
                in_block = True
            continue
        if not line.strip() or re.match(r'^\s*[A-Z][A-Za-z ]+:\s*', line):
            break
        m = re.match(r'^\s*([^|]+)\|', line)
        if m:
            size = m.group(1).strip()
            size = size.split(":")[0].strip()
            if size and size not in sizes:
                sizes.append(size)
    return sizes


def _extract_sizes_from_sizes_line(text: str) -> list[str]:
    """æ—§ï¼š(Product )?Sizes: è¡Œï¼›é€—å·/åˆ†å·/æ–œæ åˆ†éš”"""
    m = re.search(r'(?i)(?:Product\s+)?Sizes?\s*:\s*(.+)', text)
    if not m:
        return []
    raw = m.group(1)
    parts = re.split(r'[;,/|]', raw)
    sizes = []
    for p in parts:
        p = p.strip()
        if not p:
            continue
        sizes.append(p.split(":")[0].strip())
    return sizes


# -------------------- å¯é€‰å­—æ®µå¢å¼º --------------------
_GENDER_PAT = [
    (r'\b(women|ladies|woman)\b', 'å¥³æ¬¾'),
    (r'\b(men|mens|man)\b',       'ç”·æ¬¾'),
    (r'\b(girl|boy|kid|kids)\b',  'ç«¥æ¬¾'),
]
_CATEGORY_PAT = [
    (r'quilt',             'quilted jacket'),
    (r'\bwax',             'waxed jacket'),
    (r'\bgilet\b|vest',    'gilet'),
    (r'\bparka\b',         'parka'),
    (r'\bliner\b',         'liner'),
    (r'\bfleece\b',        'fleece'),
    (r'\bshirt\b',         'shirt'),
    (r'knit|sweater',      'knitwear'),
]


def infer_gender(text: str) -> str | None:
    t = (text or "").lower()
    for pat, val in _GENDER_PAT:
        if re.search(pat, t):
            return val
    return None


def infer_category(text: str) -> str | None:
    t = (text or "").lower()
    for pat, val in _CATEGORY_PAT:
        if re.search(pat, t):
            return val
    return None


def enrich_record_optional(rec: Dict) -> Dict:
    # ä»…åœ¨ç¼ºå¤±æ—¶è¡¥
    code  = rec.get("product_code") or ""
    name  = rec.get("style_name") or ""
    color = rec.get("color") or ""
    base_text = " ".join([name, rec.get("product_description") or ""])

    if not rec.get("title") and generate_barbour_taobao_title:
        try:
            info = generate_barbour_taobao_title(code, name, color) or {}
            title_cn = info.get("Title")
            if title_cn:
                rec["title"] = title_cn
        except Exception:
            pass

    if not rec.get("gender"):
        g = infer_gender(base_text)
        if g:
            rec["gender"] = g

    if not rec.get("category"):
        c = infer_category(base_text)
        if c:
            rec["category"] = c

    return rec


def _parse_sizes_from_size_detail_line(text: str) -> list[str]:
    """
    ç»Ÿä¸€æ¨¡æ¿å˜ä½“ï¼šProduct Size Detail: 6:0:000...;8:3:000...;...
    ä»…æå–å°ºç ï¼Œä¸ç®¡åé¢çš„æ•°é‡/å ä½ç ã€‚
    """
    line = _extract_field(text, r'(?i)Product\s+Size\s+Detail')
    if not line:
        return []
    sizes = []
    for token in line.split(";"):
        token = token.strip()
        if not token:
            continue
        size = token.split(":", 1)[0].strip()
        if size and size not in sizes:
            sizes.append(size)
    return sizes


# -------------------- TXT è§£æï¼ˆç»Ÿä¸€ + å…¼å®¹ï¼‰ --------------------
def parse_txt_file(filepath: Path, conn) -> List[Dict]:
    """
    è¾“å‡º recordsï¼ˆæ¯å°ºç ä¸€æ¡ï¼‰ï¼š
      product_code, style_name, color, size,
      match_keywords_l1, match_keywords_l2,
      + å¯é€‰ï¼štitle, product_description, gender, category
      + æ¥æºï¼šsource_site, source_url, source_rank
    """
    text = filepath.read_text(encoding="utf-8", errors="ignore")
    info: Dict = {"sizes": []}

    # Product Code
    m = re.search(r'(?i)Product\s+(?:Color\s+)?Code:\s*([A-Z0-9]+|No\s+Data|null)', text)
    code = (m.group(1).strip() if m else None)
    if code and code.lower() not in {"no data", "null"}:
        info["product_code"] = code
    else:
        info["product_code"] = guess_product_code_from_filename(filepath)

    # Product Name
    name = _extract_field(text, r'(?i)Product\s+Name')
    if name:
        info["style_name"] = name

    # Product Color
    color = _extract_field(text, r'(?i)Product\s+Colou?r')
    if color:
        val = re.sub(r'^\-+\s*', '', color).strip()
        info["color"] = val

    # Product Descriptionï¼ˆå¤šè¡Œï¼‰
    desc = _extract_multiline_field(text, r'(?i)Product\s+Description')
    if desc:
        info["product_description"] = desc

    # Product Genderï¼ˆå¯é€‰ï¼‰
    g = _extract_field(text, r'(?i)Product\s+Gender')
    if g:
        gl = g.lower()
        if any(k in gl for k in ["å¥³", "women", "ladies", "woman"]):
            info["gender"] = "å¥³æ¬¾"
        elif any(k in gl for k in ["ç”·", "men", "mens", "man"]):
            info["gender"] = "ç”·æ¬¾"
        elif any(k in gl for k in ["ç«¥", "kid", "kids", "boy", "girl"]):
            info["gender"] = "ç«¥æ¬¾"

    # Style Categoryï¼ˆå¯é€‰ï¼‰
    cat = _extract_field(text, r'(?i)Style\s+Category')
    if cat:
        info["category"] = cat

    # Product Size Detailï¼ˆå”¯ä¸€æ¥æºï¼‰
    sizes = _parse_sizes_from_size_detail_line(text)
    info["sizes"] = sizes

    # æ¥æºï¼ˆç”¨äºå†™å…¥ source_* ä¸ rankï¼‰
    source_site = _extract_field(text, r'(?i)Site\s+Name') or ""
    source_url  = _extract_field(text, r'(?i)Source\s+URL') or ""
    info["source_site"] = source_site.strip()
    info["source_url"]  = source_url.strip()

    # rankï¼šbarbourå®˜ç½‘=0ï¼›æœ‰ç¼–ç ç«™ç‚¹=1ï¼›æ— ç¼–ç ï¼ˆé äººå·¥ï¼‰=2
    if (info["source_site"] or "").lower() == "barbour":
        info["source_rank"] = 0
    elif info.get("product_code"):
        info["source_rank"] = 1
    else:
        info["source_rank"] = 2

    # â€”â€” å…¥åº“å¿…è¦å­—æ®µæ ¡éªŒ â€”â€”ï¼ˆç¼ºå¤±åˆ™è·³è¿‡ï¼‰
    if not info.get("product_code") or not info.get("style_name") or not info.get("color") or not info["sizes"]:
        miss = [k for k in ("product_code", "style_name", "color", "sizes")
                if not info.get(k) or (k == "sizes" and not info["sizes"])]
        print(f"âš ï¸ è·³è¿‡ï¼ˆä¿¡æ¯ä¸å®Œæ•´ï¼‰: {filepath.name} | ç¼ºå¤±: {', '.join(miss)}")
        return []

    # âœ… ç”Ÿæˆ L1 / L2
    l1_set = load_lexicon_keywords_from_db(conn, brand="barbour", level=1)
    l2_set = load_lexicon_keywords_from_db(conn, brand="barbour", level=2)

    # L1ï¼šç”¨ style_name å‘½ä¸­
    match_l1 = extract_keywords_by_lexicon(info["style_name"], l1_set)

    # å…œåº•ï¼šè‹¥ L1 æ²¡å‘½ä¸­ï¼Œå›é€€æ—§é€»è¾‘ï¼ˆä¿è¯ä¸å‡ºç°ç©ºæ•°ç»„ï¼‰
    if not match_l1:
        match_l1 = extract_match_keywords(info["style_name"])

    # L2ï¼šç”¨ style_name + description + gender/categoryï¼ˆæ›´ç¨³ä¸€ç‚¹ï¼‰
    l2_text = " ".join([
        info.get("style_name") or "",   # å°±æ˜¯ä½ è¯´çš„ style title
        info.get("gender") or "",
        info.get("category") or "",
    ])
    match_l2 = extract_keywords_by_lexicon(l2_text, l2_set)


    # â€”â€” ç”Ÿæˆ recordsï¼ˆæ¯å°ºç ä¸€æ¡ï¼‰â€”â€”
    records: List[Dict] = []
    for size in info["sizes"]:
        r = {
            "product_code": info["product_code"],
            "style_name": info["style_name"],
            "color": info["color"],
            "size": size,
            "match_keywords_l1": match_l1,
            "match_keywords_l2": match_l2,
            "source_site": info.get("source_site", ""),
            "source_url": info.get("source_url", ""),
            "source_rank": info.get("source_rank", 999)
        }
        if info.get("product_description"):
            r["product_description"] = info["product_description"]
        if info.get("gender"):
            r["gender"] = info["gender"]
        if info.get("category"):
            r["category"] = info["category"]

        # è½»é‡ enrichï¼ˆä¸è¦†ç›–å·²æœ‰å€¼ï¼‰
        r = enrich_record_optional(r)

        ok, missing = validate_minimal_fields(r)
        if not ok:
            print(f"âš ï¸ è·³è¿‡ï¼ˆè¡Œä¸å®Œæ•´ï¼‰: {filepath.name} {size} | ç¼ºå¤±: {', '.join(missing)}")
            continue
        records.append(r)

    return records


# -------------------- DB å…¥åº“ï¼ˆæŒ‰æ¥æºä¼˜å…ˆçº§ä¿æŠ¤è¦†ç›–ï¼‰ --------------------
def insert_into_products(records: List[Dict], conn):
    """
    UPSERT è§„åˆ™ï¼š
    - å†²çªé”®ï¼š(product_code, size)
    - ä»…å½“ EXCLUDED.source_rank <= ç°æœ‰ source_rank æ—¶ï¼Œå…è®¸è¦†ç›–åŸºç¡€å­—æ®µ
    - titleï¼šè‹¥åº“é‡Œä¸ºç©ºåˆ™å¡«
    - match_keywords_l1/l2ï¼šéš style_name æ›´æ–°ï¼ˆä¹Ÿå— rank ä¿æŠ¤ï¼‰
    """
    sql = """
    INSERT INTO barbour_products
      (product_code, style_name, color, size,
       match_keywords_l1, match_keywords_l2,
       title, product_description, gender, category,
       source_site, source_url, source_rank)
    VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)
    ON CONFLICT (product_code, size) DO UPDATE SET
       style_name = CASE WHEN EXCLUDED.source_rank <= barbour_products.source_rank THEN EXCLUDED.style_name ELSE barbour_products.style_name END,
       color      = CASE WHEN EXCLUDED.source_rank <= barbour_products.source_rank THEN EXCLUDED.color      ELSE barbour_products.color      END,

       match_keywords_l1 = CASE WHEN EXCLUDED.source_rank <= barbour_products.source_rank THEN EXCLUDED.match_keywords_l1 ELSE barbour_products.match_keywords_l1 END,
       match_keywords_l2 = CASE WHEN EXCLUDED.source_rank <= barbour_products.source_rank THEN EXCLUDED.match_keywords_l2 ELSE barbour_products.match_keywords_l2 END,

       -- ä»…å½“åŸ title ä¸ºç©ºæ—¶å†™å…¥ï¼ˆé¿å…è¦†ç›–ä½ æ‰‹å·¥ä¼˜åŒ–è¿‡çš„ä¸­æ–‡æ ‡é¢˜ï¼‰
       title               = COALESCE(barbour_products.title, EXCLUDED.title),
       product_description = CASE WHEN EXCLUDED.source_rank <= barbour_products.source_rank THEN COALESCE(EXCLUDED.product_description, barbour_products.product_description) ELSE barbour_products.product_description END,
       gender              = CASE WHEN EXCLUDED.source_rank <= barbour_products.source_rank THEN COALESCE(EXCLUDED.gender, barbour_products.gender) ELSE barbour_products.gender END,
       category            = CASE WHEN EXCLUDED.source_rank <= barbour_products.source_rank THEN COALESCE(EXCLUDED.category, barbour_products.category) ELSE barbour_products.category END,
       source_site         = CASE WHEN EXCLUDED.source_rank <= barbour_products.source_rank THEN EXCLUDED.source_site ELSE barbour_products.source_site END,
       source_url          = CASE WHEN EXCLUDED.source_rank <= barbour_products.source_rank THEN EXCLUDED.source_url  ELSE barbour_products.source_url  END,
       source_rank         = LEAST(barbour_products.source_rank, EXCLUDED.source_rank);
    """
    with conn.cursor() as cur:
        for r in records:
            try:
                cur.execute(sql, (
                    r.get("product_code"),
                    r.get("style_name"),
                    r.get("color"),
                    r.get("size"),
                    r.get("match_keywords_l1"),
                    r.get("match_keywords_l2"),
                    r.get("title"),
                    r.get("product_description"),
                    r.get("gender"),
                    r.get("category"),
                    r.get("source_site"),
                    r.get("source_url"),
                    int(r.get("source_rank") or 999),
                ))
            except Exception as e:
                print(f"âŒ å…¥åº“å¤±è´¥ï¼ˆè®°å½•çº§ï¼‰: code={r.get('product_code')} size={r.get('size')} | é”™è¯¯: {e}")
                conn.rollback()
                continue
    conn.commit()


# -------------------- ç›®å½•å‘ç° & æ‰¹å¤„ç†å…¥å£ --------------------
_ALIAS = {
    "oac": "outdoorandcountry",
    "outdoor": "outdoorandcountry",
    "allweather": "allweathers",
    "hof": "houseoffraser",
    "pm": "philipmorris",
}


def _discover_txt_paths_by_supplier(supplier: str = "all") -> List[Path]:
    supplier = (supplier or "all").strip().lower()
    supplier = _ALIAS.get(supplier, supplier)

    txt_dirs: Dict[str, str] = BARBOUR.get("TXT_DIRS", {}) or {}
    paths: List[Path] = []

    if supplier == "all":
        for dirpath in txt_dirs.values():
            p = Path(dirpath)
            if p.exists():
                paths.extend(sorted(p.glob("*.txt")))
        if not paths and BARBOUR.get("TXT_DIR"):
            p = Path(BARBOUR["TXT_DIR"])
            if p.exists():
                paths = sorted(p.glob("*.txt"))
        return paths

    # æŒ‡å®šä¾›åº”å•†ç›®å½•
    dirpath = txt_dirs.get(supplier)
    if not dirpath:
        zh_map = {"å®˜ç½‘": "barbour", "barbourå®˜ç½‘": "barbour", "æˆ·å¤–": "outdoorandcountry"}
        supplier = zh_map.get(supplier, supplier)
        dirpath = txt_dirs.get(supplier)

    p = Path(dirpath) if dirpath else None
    if p and p.exists():
        return sorted(p.glob("*.txt"))
    return []


def batch_import_txt_to_barbour_product(supplier: str = "all"):
    files = _discover_txt_paths_by_supplier(supplier)
    if not files:
        print(f"âš ï¸ æœªæ‰¾åˆ°ä»»ä½• TXT æ–‡ä»¶ï¼ˆsupplier='{supplier}'ï¼‰ã€‚è¯·æ£€æŸ¥ BARBOUR['TXT_DIRS'] é…ç½®æˆ–ç›®å½•æ˜¯å¦å­˜åœ¨ã€‚")
        return

    conn = psycopg2.connect(**PGSQL_CONFIG)
    total_rows = 0
    parsed_files = 0

    # é¢„çƒ­ï¼šåŠ è½½ä¸€æ¬¡ L1/L2 lexiconï¼ˆæå‰å‘ç°åº“é‡Œæ²¡æ•°æ®çš„é—®é¢˜ï¼‰
    try:
        l1_set = load_lexicon_keywords_from_db(conn, brand="barbour", level=1)
        l2_set = load_lexicon_keywords_from_db(conn, brand="barbour", level=2)
        if not l1_set:
            print("âš ï¸ keyword_lexicon ä¸­æœªæ‰¾åˆ° barbour çš„ L1 è¯ï¼ˆlevel=1ï¼‰ã€‚match_keywords_l1 å°†å¤§é‡å›é€€æ—§é€»è¾‘ã€‚")
        if not l2_set:
            print("âš ï¸ keyword_lexicon ä¸­æœªæ‰¾åˆ° barbour çš„ L2 è¯ï¼ˆlevel=2ï¼‰ã€‚match_keywords_l2 å°†å¤šæ•°ä¸ºç©ºã€‚")
    except Exception as e:
        print(f"âš ï¸ è¯»å– keyword_lexicon å¤±è´¥ï¼š{e}ï¼ˆL1 å°†ç»§ç»­ç”¨æ—§é€»è¾‘å…œåº•ï¼ŒL2 å¤šæ•°ä¸ºç©ºï¼‰")

    for file in files:
        try:
            records = parse_txt_file(file, conn)
            if not records:
                print(f"â“˜ è·³è¿‡ï¼ˆæ— è®°å½•ï¼‰: {file.name}")
                continue
            insert_into_products(records, conn)
            print(f"âœ… å¯¼å…¥ {file.name} â€” {len(records)} æ¡")
            total_rows += len(records)
            parsed_files += 1
        except Exception as e:
            conn.rollback()
            print(f"âŒ å¯¼å…¥å¤±è´¥ï¼ˆæ–‡ä»¶çº§ï¼‰: {file.name} | é”™è¯¯: {e}")
            continue

    conn.close()
    print(f"\nğŸ‰ å¯¼å…¥å®Œæˆï¼ˆsupplier='{supplier}'ï¼‰ï¼š{parsed_files} ä¸ªæ–‡ä»¶ï¼Œå…± {total_rows} æ¡è®°å½•")


if __name__ == "__main__":
    arg = sys.argv[1] if len(sys.argv) > 1 else "all"
    batch_import_txt_to_barbour_product(arg)
