# -*- coding: utf-8 -*-
"""
Barbour é‡‡é›†å™¨åŸºç±» - ç»Ÿä¸€æ¥å£å’Œå…±äº«é€»è¾‘

ç›®æ ‡: æ¶ˆé™¤ 8 ä¸ªé‡‡é›†è„šæœ¬ 70%+ çš„ä»£ç é‡å¤

ä½¿ç”¨æ–¹å¼:
    class AllweathersFetcher(BaseFetcher):
        def parse_detail_page(self, html, url):
            # åªå®ç°ç«™ç‚¹ç‰¹å®šçš„è§£æé€»è¾‘
            return {...}

    fetcher = AllweathersFetcher("allweathers", links_file, output_dir)
    fetcher.run_batch()
"""

from __future__ import annotations

import logging
import threading
import time
from abc import ABC, abstractmethod
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed

from bs4 import BeautifulSoup

# å¯¼å…¥å…±äº«å·¥å…·æ¨¡å—
from brands.barbour.core.size_normalizer import (
    normalize_size,
    build_size_lines_from_detail,
    format_size_str_simple,
    format_size_detail_simple,
)
from brands.barbour.core.gender_classifier import infer_gender
from brands.barbour.core.html_parser import (
    extract_jsonld,
    extract_meta_tag,
    extract_og_tag,
    extract_price_from_text,
    extract_barbour_code_from_text,
)
from brands.barbour.core.text_utils import (
    clean_text,
    clean_description,
    normalize_url,
    safe_filename,
)

# å¯¼å…¥é€šç”¨æ¨¡å—
from common.core.selenium_utils import get_driver, quit_driver, quit_all_drivers
from common.ingest.txt_writer import format_txt

# é…ç½®
from config import SETTINGS

DEFAULT_STOCK_COUNT = SETTINGS.get("DEFAULT_STOCK_COUNT", 3)


# ================== åŸºç±»å®šä¹‰ ==================

class BaseFetcher(ABC):
    """
    é‡‡é›†å™¨åŸºç±» - æ¨¡æ¿æ–¹æ³•æ¨¡å¼

    æ ¸å¿ƒæ€æƒ³:
    - åŸºç±»æä¾›é€šç”¨é€»è¾‘ (å¹¶å‘ã€é‡è¯•ã€æ—¥å¿—ã€æ–‡ä»¶å†™å…¥)
    - å­ç±»åªå®ç°ç«™ç‚¹ç‰¹å®šçš„è§£æé€»è¾‘ (parse_detail_page)

    ç‰¹æ€§:
    - è‡ªåŠ¨å¹¶å‘ç®¡ç†
    - è‡ªåŠ¨é‡è¯• (æŒ‡æ•°é€€é¿)
    - ç»Ÿä¸€æ—¥å¿—
    - çº¿ç¨‹å®‰å…¨çš„é©±åŠ¨ç®¡ç†
    - ç»Ÿä¸€æ–‡ä»¶å†™å…¥æ ¼å¼
    """

    def __init__(
        self,
        site_name: str,
        links_file: Path | str,
        output_dir: Path | str,
        max_workers: int = 4,
        max_retries: int = 3,
        default_stock: int = DEFAULT_STOCK_COUNT,
        wait_seconds: float = 2.0,
        headless: bool = True,
    ):
        """
        åˆå§‹åŒ–é‡‡é›†å™¨

        Args:
            site_name: ç«™ç‚¹åç§° (å¦‚ 'allweathers', 'barbour')
            links_file: é“¾æ¥æ–‡ä»¶è·¯å¾„
            output_dir: TXTè¾“å‡ºç›®å½•
            max_workers: å¹¶å‘çº¿ç¨‹æ•°
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            default_stock: é»˜è®¤åº“å­˜æ•°é‡
            wait_seconds: é¡µé¢ç­‰å¾…æ—¶é—´ (ç§’)
            headless: æ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼
        """
        self.site_name = site_name
        self.links_file = Path(links_file)
        self.output_dir = Path(output_dir)
        self.max_workers = max_workers
        self.max_retries = max_retries
        self.default_stock = default_stock
        self.wait_seconds = wait_seconds
        self.headless = headless

        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # æ—¥å¿—é…ç½®
        self.logger = logging.getLogger(f"{__name__}.{site_name}")
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                "%(asctime)s [%(levelname)-8s] [%(name)s] %(message)s",
                datefmt="%H:%M:%S",
            )
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
            self.logger.setLevel(logging.INFO)

        # ç»Ÿè®¡
        self._success_count = 0
        self._fail_count = 0
        self._lock = threading.Lock()

    # ================== æŠ½è±¡æ–¹æ³• - å­ç±»å¿…é¡»å®ç° ==================

    @abstractmethod
    def parse_detail_page(self, html: str, url: str) -> Dict[str, Any]:
        """
        è§£æå•†å“è¯¦æƒ…é¡µ - å„ç«™ç‚¹å®ç°è‡ªå·±çš„é€»è¾‘

        Args:
            html: é¡µé¢HTMLæºç 
            url: å•†å“URL

        Returns:
            åŒ…å«ä»¥ä¸‹å­—æ®µçš„å­—å…¸ (å¿…å¡«):
            {
                "Product Code": str,           # Barbourç¼–ç 
                "Product Name": str,           # å•†å“åç§°
                "Product Color": str,          # é¢œè‰²
                "Product Gender": str,         # æ€§åˆ« (Men/Women/Kids)
                "Product Description": str,    # æè¿°
                "Original Price (GBP)": str,   # åŸä»·
                "Discount Price (GBP)": str,   # æŠ˜æ‰£ä»·
                "Product Size": str,           # "S:æœ‰è´§;M:æœ‰è´§;L:æ— è´§"
                "Product Size Detail": str,    # "S:3:EAN001;M:0:EAN002"
            }

        æ³¨æ„:
        - å¯ä»¥ä½¿ç”¨åŸºç±»æä¾›çš„å·¥å…·æ–¹æ³• (self.extract_*, self.parse_*)
        - è¿”å›çš„å­—å…¸ä¼šè‡ªåŠ¨æ·»åŠ  Site Name å’Œ Source URL
        """
        pass

    # ================== é©±åŠ¨ç®¡ç† ==================

    def get_driver(self):
        """
        è·å– Selenium é©±åŠ¨ - çº¿ç¨‹å®‰å…¨

        Returns:
            WebDriverå®ä¾‹
        """
        return get_driver(
            name=self.site_name,
            headless=self.headless,
            window_size="1920,1080",
        )

    def quit_driver(self):
        """å…³é—­é©±åŠ¨"""
        try:
            quit_driver(self.site_name)
        except Exception as e:
            self.logger.warning(f"é©±åŠ¨å…³é—­å¤±è´¥: {e}")

    # ================== æ ¸å¿ƒæµç¨‹ - æ¨¡æ¿æ–¹æ³• ==================

    def fetch_one_product(self, url: str, idx: int, total: int) -> Tuple[str, bool]:
        """
        æŠ“å–å•ä¸ªå•†å“ - å¸¦é‡è¯•å’Œé”™è¯¯å¤„ç†

        Args:
            url: å•†å“URL
            idx: å½“å‰ç´¢å¼• (ä»1å¼€å§‹)
            total: æ€»æ•°

        Returns:
            (url, success: bool)
        """
        for attempt in range(1, self.max_retries + 1):
            try:
                self.logger.info(f"[{idx}/{total}] [{attempt}/{self.max_retries}] æŠ“å–: {url}")

                # Step 1: è·å–HTML
                html = self._fetch_html(url)

                # Step 2: è§£æ
                info = self.parse_detail_page(html, url)

                # Step 3: éªŒè¯å¿…å¡«å­—æ®µ
                self._validate_info(info, url)

                # Step 4: å¡«å……é»˜è®¤å­—æ®µ
                info.setdefault("Site Name", self.site_name)
                info.setdefault("Source URL", url)

                # Step 5: å†™å…¥æ–‡ä»¶
                self._write_output(info)

                code = info.get("Product Code", "N/A")
                self.logger.info(f"âœ… [{idx}/{total}] æˆåŠŸ: {code}")

                with self._lock:
                    self._success_count += 1

                return url, True

            except Exception as e:
                self.logger.error(
                    f"âŒ [{idx}/{total}] å°è¯• {attempt}/{self.max_retries} å¤±è´¥: {url} - {e}",
                    exc_info=(attempt == self.max_retries),  # æœ€åä¸€æ¬¡æ‰æ‰“å°å †æ ˆ
                )

                # æŒ‡æ•°é€€é¿
                if attempt < self.max_retries:
                    wait_time = min(2 ** attempt, 30)  # æœ€å¤šç­‰30ç§’
                    time.sleep(wait_time)

                if attempt == self.max_retries:
                    with self._lock:
                        self._fail_count += 1
                    return url, False

        return url, False

    def _fetch_html(self, url: str) -> str:
        """
        è·å–HTML - é»˜è®¤å®ç°(Selenium)ï¼Œå­ç±»å¯è¦†ç›–

        å¤ç”¨åŒçº¿ç¨‹ driverï¼ˆç”± selenium_utils æŒ‰ thread-id éš”ç¦»ï¼‰ï¼Œ
        ä¸å†æ¯æ¬¡è¯·æ±‚éƒ½åˆ›å»º/é”€æ¯ Chrome è¿›ç¨‹ã€‚

        Args:
            url: å•†å“URL

        Returns:
            HTMLæºç 
        """
        driver = self.get_driver()
        driver.get(url)
        time.sleep(self.wait_seconds)  # ç­‰å¾…æ¸²æŸ“
        return driver.page_source

    def _validate_info(self, info: Dict[str, Any], url: str) -> None:
        """
        éªŒè¯å¿…å¡«å­—æ®µ

        Args:
            info: å•†å“ä¿¡æ¯å­—å…¸
            url: å•†å“URL

        Raises:
            ValueError: å¿…å¡«å­—æ®µç¼ºå¤±
        """
        required_fields = [
            "Product Code",
            "Product Name",
            "Product Color",
            "Product Gender",
            "Product Description",
            "Original Price (GBP)",
            "Discount Price (GBP)",
            "Product Size",
            "Product Size Detail",
        ]

        for field in required_fields:
            if field not in info:
                raise ValueError(f"ç¼ºå¤±å¿…å¡«å­—æ®µ: {field} (URL: {url})")

    def _write_output(self, info: Dict[str, Any]) -> None:
        """
        å†™å…¥TXTæ–‡ä»¶ - ç»Ÿä¸€æ ¼å¼

        Args:
            info: å•†å“ä¿¡æ¯å­—å…¸
        """
        code = info.get("Product Code", "").strip()
        if not code or code in ["Unknown", "No Data", "N/A", ""]:
            # ä½¿ç”¨URLå“ˆå¸Œä½œä¸ºæ–‡ä»¶å
            url = info.get("Source URL", "")
            code = f"NoCode_{abs(hash(url)) & 0xFFFFFFFF:08x}"

        # æ¸…æ´—æ–‡ä»¶å
        safe_code = safe_filename(code)
        txt_path = self.output_dir / f"{safe_code}.txt"

        try:
            format_txt(info, txt_path, brand="barbour")
            self.logger.debug(f"å†™å…¥æ–‡ä»¶: {txt_path}")
        except Exception as e:
            self.logger.error(f"æ–‡ä»¶å†™å…¥å¤±è´¥ {txt_path}: {e}")
            raise

    # ================== æ‰¹é‡å…¥å£ ==================

    def run_batch(self) -> Tuple[int, int]:
        """
        æ‰¹é‡æŠ“å– - ä¸»å…¥å£

        Returns:
            (æˆåŠŸæ•°, å¤±è´¥æ•°)
        """
        # è¯»å–é“¾æ¥
        if not self.links_file.exists():
            self.logger.error(f"é“¾æ¥æ–‡ä»¶ä¸å­˜åœ¨: {self.links_file}")
            return 0, 0

        urls = self._load_urls()
        if not urls:
            self.logger.warning("æ— æœ‰æ•ˆé“¾æ¥")
            return 0, 0

        total = len(urls)
        self.logger.info(f"ğŸ“„ å…± {total} ä¸ªå•†å“å¾…æŠ“å– (å¹¶å‘åº¦: {self.max_workers})")

        # å¹¶å‘æŠ“å–
        try:
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                futures = {
                    executor.submit(self.fetch_one_product, url, idx + 1, total): url
                    for idx, url in enumerate(urls)
                }

                for future in as_completed(futures):
                    url = futures[future]
                    try:
                        future.result()
                    except Exception as e:
                        self.logger.error(f"ä»»åŠ¡å¼‚å¸¸: {url} - {e}")
        finally:
            quit_all_drivers()
            self.logger.info("ğŸ§¹ å·²æ¸…ç†æ‰€æœ‰ driver")

        # ç»Ÿè®¡
        success = self._success_count
        fail = self._fail_count
        self.logger.info(f"âœ… å®Œæˆ: æˆåŠŸ {success}, å¤±è´¥ {fail}")

        return success, fail

    def _load_urls(self) -> List[str]:
        """
        åŠ è½½é“¾æ¥åˆ—è¡¨ - å»é‡

        Returns:
            URLåˆ—è¡¨
        """
        try:
            raw_lines = self.links_file.read_text(encoding="utf-8", errors="ignore").splitlines()
        except Exception as e:
            self.logger.error(f"è¯»å–é“¾æ¥æ–‡ä»¶å¤±è´¥: {e}")
            return []

        seen = set()
        urls = []

        for line in raw_lines:
            url = line.strip()

            # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Š
            if not url or url.startswith("#"):
                continue

            # æ ‡å‡†åŒ–URL (å»é™¤é”šç‚¹)
            url = normalize_url(url)

            # å»é‡
            if url in seen:
                continue

            seen.add(url)
            urls.append(url)

        return urls

    # ================== å·¥å…·æ–¹æ³• (ä¾›å­ç±»ä½¿ç”¨) ==================

    def extract_jsonld(self, soup: BeautifulSoup, target_type: str = "Product") -> Optional[Dict]:
        """æå– JSON-LD æ•°æ®"""
        return extract_jsonld(soup, target_type)

    def extract_meta(self, soup: BeautifulSoup, name: str) -> Optional[str]:
        """æå– meta æ ‡ç­¾"""
        return extract_meta_tag(soup, name)

    def extract_og(self, soup: BeautifulSoup, og_type: str) -> Optional[str]:
        """æå– Open Graph æ ‡ç­¾"""
        return extract_og_tag(soup, og_type)

    def parse_price(self, text: str) -> Optional[float]:
        """ä»æ–‡æœ¬æå–ä»·æ ¼"""
        return extract_price_from_text(text)

    def clean_text(self, text: str, maxlen: int = 500) -> str:
        """æ¸…ç†æ–‡æœ¬"""
        return clean_text(text, maxlen=maxlen)

    def clean_description(self, desc: str) -> str:
        """æ¸…ç†æè¿°"""
        return clean_description(desc)

    def infer_gender(
        self,
        text: str = "",
        url: str = "",
        product_code: str = "",
        output_format: str = "en",
    ) -> str:
        """æ¨æ–­æ€§åˆ«"""
        return infer_gender(
            text=text,
            url=url,
            product_code=product_code,
            output_format=output_format,
        )

    def normalize_size(self, token: str, gender: str) -> Optional[str]:
        """æ ‡å‡†åŒ–å°ºç """
        return normalize_size(token, gender)

    def build_size_lines(
        self,
        size_detail: Dict[str, Dict],
        gender: str,
    ) -> Tuple[str, str]:
        """ä»å°ºç è¯¦æƒ…ç”Ÿæˆ Product Size å’Œ Product Size Detail"""
        return build_size_lines_from_detail(size_detail, gender, self.default_stock)

    def format_size_simple(self, sizes: List[str], gender: str) -> str:
        """ç®€å•æ ¼å¼åŒ–å°ºç  (å…¨éƒ¨æœ‰è´§)"""
        return format_size_str_simple(sizes, gender)

    def format_size_detail_simple(self, sizes: List[str], gender: str) -> str:
        """ç®€å•æ ¼å¼åŒ–å°ºç è¯¦æƒ…"""
        return format_size_detail_simple(sizes, gender, self.default_stock)

    def extract_barbour_code(self, text: str) -> Optional[str]:
        """ä»æ–‡æœ¬æå– Barbour Product Code"""
        return extract_barbour_code_from_text(text)


# ================== è¾…åŠ©å‡½æ•° ==================

def setup_logging(level: int = logging.INFO):
    """
    é…ç½®å…¨å±€æ—¥å¿—

    Args:
        level: æ—¥å¿—çº§åˆ« (DEBUG, INFO, WARNING, ERROR)
    """
    logging.basicConfig(
        level=level,
        format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )
