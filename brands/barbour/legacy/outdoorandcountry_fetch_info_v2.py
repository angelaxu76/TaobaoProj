# -*- coding: utf-8 -*-
"""
Outdoor & Country | Barbour å•†å“æŠ“å–ï¼ˆv4.2 - ç¨³å®šé•¿è·‘ç‰ˆï¼‰
ä¿æŒå¯¹å¤–æ¥å£å…¼å®¹ï¼š
- process_url(url, output_dir)
- outdoorandcountry_fetch_info(max_workers=3)

v4.2 è§£å†³ç‚¹ï¼š
1) driver é•¿è·‘é€€åŒ–ï¼šæ¯çº¿ç¨‹è·‘ N ä¸ªé¡µé¢è‡ªåŠ¨é‡å¯ driverï¼ˆé»˜è®¤ 20ï¼‰
2) é£æ§è¶Šæ¥è¶Šä¸¥ï¼šæŒ‘æˆ˜é¡µ/è¶…æ—¶/å¤±è´¥ -> éšæœºé€€é¿ + æŒ‡æ•°é€€é¿ï¼ˆé¿å…è¶Šè·‘è¶Šæ…¢ï¼‰
3) å¹¶å‘è‡ªé€‚åº”ï¼šå¤±è´¥å¤šæ—¶è‡ªåŠ¨æŠŠæœ‰æ•ˆå¹¶å‘å‹åˆ° 2ï¼ˆå³ä½¿ä½ ä¼ æ›´å¤§ï¼‰
"""

import time
import json
import threading
import re
import random
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urlparse, parse_qs, unquote

from bs4 import BeautifulSoup

from config import BARBOUR, SETTINGS
from brands.barbour.supplier.outdoorandcountry_parse_offer_info import parse_offer_info
from common.ingest.txt_writer import format_txt

# âœ… ä½¿ç”¨ç¨³å®š driver æ± ï¼ˆé”æ­»æœ¬åœ° chromedriver + çº¿ç¨‹éš”ç¦» key + ç¦å›¾ï¼‰
from common.core.selenium_utils import get_driver as _get_driver_v2
from common.core.selenium_utils import quit_all_drivers as _quit_all_drivers_v2

from common.core.size_utils import clean_size_for_barbour
from brands.barbour.core.site_utils import assert_site_or_raise as canon

CANON_SITE = canon("outdoorandcountry")
DEFAULT_STOCK_COUNT = SETTINGS.get("DEFAULT_STOCK_COUNT", 3)

# ======================
# v4.2 å…³é”®å‚æ•°ï¼ˆå¯æŒ‰éœ€å¾®è°ƒï¼‰
# ======================
RESTART_EVERY = 20        # âœ… æ¯çº¿ç¨‹è·‘å¤šå°‘é¡µé¢é‡å¯ driverï¼ˆè¶Šå°è¶Šç¨³ï¼Œä½†ä¼šæ…¢ä¸€ç‚¹ï¼‰
EFFECTIVE_MAX_WORKERS = 2 # âœ… Outdoor å¼ºé£æ§ç«™ç‚¹ï¼šé•¿æœŸç¨³å®šå»ºè®® 2
PAGE_WAIT_TIMEOUT = 10
AFTER_BODY_SLEEP = 0.6

# é€€é¿ç­–ç•¥ï¼ˆæŒ‘æˆ˜é¡µ/å¤±è´¥æ—¶ï¼‰
BACKOFF_BASE = 3.0
BACKOFF_JITTER = (0.8, 1.6)   # ä¹˜æ•°éšæœºæŠ–åŠ¨èŒƒå›´
BACKOFF_CAP = 25.0           # æœ€é•¿é€€é¿ç§’æ•°

# å…¨å±€å¤±è´¥ç»Ÿè®¡ï¼ˆç”¨äºè‡ªé€‚åº”é™å¹¶å‘/æ›´å¼ºé€€é¿ï¼‰
_stats_lock = threading.Lock()
_stats = {
    "ok": 0,
    "fail": 0,
    "challenge": 0,
    "last_fail_ts": 0.0,
}

def _inc_stat(key: str):
    with _stats_lock:
        _stats[key] += 1
        if key in ("fail", "challenge"):
            _stats["last_fail_ts"] = time.time()

def _fail_ratio() -> float:
    with _stats_lock:
        ok = _stats["ok"]
        fail = _stats["fail"] + _stats["challenge"]
        total = ok + fail
        return (fail / total) if total else 0.0

def _compute_backoff(tries: int, kind: str) -> float:
    """
    tries: å½“å‰ url çš„é‡è¯•æ¬¡æ•°ï¼ˆ0,1,2...ï¼‰
    kind: "challenge" or "fail"
    """
    # æŒ‡æ•°é€€é¿ï¼šbase * (2^tries) * jitter
    jitter = random.uniform(*BACKOFF_JITTER)
    sec = BACKOFF_BASE * (2 ** tries) * jitter

    # å¦‚æœå…¨å±€å¤±è´¥ç‡é«˜ï¼Œå¢åŠ é€€é¿ï¼ˆè¶Šè·‘è¶Šæ…¢æ—¶å¾ˆå…³é”®ï¼‰
    ratio = _fail_ratio()
    if ratio >= 0.25:
        sec *= 1.4
    if ratio >= 0.40:
        sec *= 1.8

    # æŒ‘æˆ˜é¡µæ¯”æ™®é€šå¤±è´¥æ›´å¼ºé€€é¿
    if kind == "challenge":
        sec *= 1.5

    return min(sec, BACKOFF_CAP)


# ========== Cookieï¼šæ¯ä¸ª driver åªç‚¹ä¸€æ¬¡ ==========
def accept_cookies(driver, timeout=4):
    if getattr(driver, "_cookies_accepted", False):
        return

    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC

    try:
        btn = WebDriverWait(driver, timeout).until(
            EC.element_to_be_clickable((By.ID, "onetrust-accept-btn-handler"))
        )
        btn.click()
        driver._cookies_accepted = True
        time.sleep(0.2)
    except Exception:
        driver._cookies_accepted = True


# ========== utils ==========
def _normalize_color_from_url(url: str) -> str:
    try:
        qs = parse_qs(urlparse(url).query)
        c = qs.get("c", [None])[0]
        if not c:
            return ""
        c = unquote(c)
        c = c.replace("\\", "/")
        c = re.sub(r"\s*/\s*", " / ", c)
        c = re.sub(r"\s+", " ", c).strip()
        c = " ".join(w.capitalize() for w in c.split(" "))
        return c
    except Exception:
        return ""

def sanitize_filename(name: str) -> str:
    return re.sub(r'[\\/:*?"<>|\s]+', "_", name or "NoName")

def _extract_description(html: str) -> str:
    soup = BeautifulSoup(html, "html.parser")
    tag = soup.find("meta", attrs={"property": "og:description"})
    if tag and tag.get("content"):
        return tag["content"].replace("<br>", "").replace("<br/>", "").replace("<br />", "").strip()
    tab = soup.select_one(".product_tabs .tab_content[data-id='0'] div")
    return tab.get_text(" ", strip=True) if tab else "No Data"

def _extract_features(html: str) -> str:
    soup = BeautifulSoup(html, "html.parser")
    h3 = soup.find("h3", attrs={"title": "Features"})
    if h3:
        ul = h3.find_next("ul")
        if ul:
            items = [li.get_text(" ", strip=True) for li in ul.find_all("li")]
            return "; ".join(items)
    return "No Data"

def _extract_color_code_from_jsonld(html: str) -> str:
    soup = BeautifulSoup(html, "html.parser")
    for script in soup.find_all("script", type="application/ld+json"):
        try:
            raw = (script.string or "").strip()
            if not raw:
                continue
            j = json.loads(raw)
            candidates = j if isinstance(j, list) else [j]
            for obj in candidates:
                if not isinstance(obj, dict) or obj.get("@type") != "Product":
                    continue
                offers = obj.get("offers")
                if not offers:
                    continue
                offers_list = offers if isinstance(offers, list) else [offers]
                for off in offers_list:
                    mpn = (off or {}).get("mpn")
                    if not isinstance(mpn, str):
                        continue
                    mpn = mpn.split("_")[0].strip()

                    if len(mpn) >= 11:
                        maybe_code = mpn[:-2]
                        if re.match(r"^[A-Z]{3}\d{4}[A-Z]{2}\d{2}$", maybe_code):
                            return maybe_code
        except Exception:
            continue
    return ""

def _infer_gender_from_name(name: str) -> str:
    n = (name or "").lower()
    if any(x in n for x in ["women", "women's", "womens", "ladies", "lady"]):
        return "å¥³æ¬¾"
    if any(x in n for x in ["men", "men's", "mens"]):
        return "ç”·æ¬¾"
    if any(x in n for x in ["kids", "kid", "boy", "girl"]):
        return "ç«¥æ¬¾"
    return "ç”·æ¬¾"

def _is_challenge_page(html: str) -> bool:
    low = (html or "").lower()
    return (
        "checking your browser" in low
        or "attention required" in low
        or "cloudflare" in low
        or "access denied" in low
        or "captcha" in low
        or "<title>just a moment" in low
    )


# ========== å°ºç æ•´ç† ==========
WOMEN_NUM = ["6", "8", "10", "12", "14", "16", "18", "20"]
MEN_ALPHA = ["S", "M", "L", "XL", "XXL", "XXXL"]
MEN_NUM = [str(s) for s in range(32, 52, 2)]

def _clean_size(raw: str) -> str:
    raw = (raw or "").strip()
    if not raw:
        return ""
    s = clean_size_for_barbour(raw) or raw
    return s.strip()

def _build_sizes_from_offers(offers, gender: str):
    if not offers:
        return "No Data"

    temp = []
    for size, price, stock_text, can_order in offers:
        size = (size or "").strip()
        if not size:
            continue

        stock = 0
        if (stock_text or "").strip().lower() in ("in stock", "available"):
            stock = DEFAULT_STOCK_COUNT
        if can_order and stock == 0:
            stock = DEFAULT_STOCK_COUNT

        cs = _clean_size(size)
        if not cs:
            continue

        m = re.match(r"^(\d{2})$", cs)
        if m and int(m.group(1)) >= 52:
            continue

        temp.append((cs, stock))

    if not temp:
        return "No Data"

    bucket = {}
    for s, stock in temp:
        bucket[s] = max(bucket.get(s, 0), stock)

    ordered = []
    if "å¥³" in (gender or ""):
        for s in WOMEN_NUM:
            if s in bucket:
                ordered.append(s)
        for s in bucket:
            if s not in ordered:
                ordered.append(s)
    else:
        for s in MEN_ALPHA:
            if s in bucket:
                ordered.append(s)
        for s in MEN_NUM:
            if s in bucket:
                ordered.append(s)
        for s in bucket:
            if s not in ordered:
                ordered.append(s)

    out = []
    for s in ordered:
        qty = DEFAULT_STOCK_COUNT if bucket.get(s, 0) > 0 else 0
        out.append(f"{s}:{qty}:0000000000000")

    return ";".join(out) if out else "No Data"


# ========== v4.2 driver ç®¡ç†ï¼šå®šæœŸé‡å¯ï¼ˆé˜²æ­¢è¶Šè·‘è¶Šæ…¢ï¼‰ ==========
_thread_local = threading.local()

def create_driver(headless: bool = False):
    # å¼ºåˆ¶ headless æ›´çœèµ„æº
    return _get_driver_v2(
        name="outdoorandcountry",
        headless=True,
        window_size="1200,1600",
    )

def get_driver(headless: bool = False):
    d = getattr(_thread_local, "driver", None)
    cnt = getattr(_thread_local, "count", 0)

    if d is None or cnt >= RESTART_EVERY:
        try:
            if d is not None:
                d.quit()
        except Exception:
            pass

        d = create_driver(headless=headless)
        _thread_local.driver = d
        _thread_local.count = 0

    return d

def mark_driver_used():
    _thread_local.count = getattr(_thread_local, "count", 0) + 1

def shutdown_all_drivers():
    _quit_all_drivers_v2()


# ========== æ ¸å¿ƒæŠ“å–ï¼šå¸¦è‡ªé€‚åº”é€€é¿ & å• URL é‡è¯• ==========
def process_url(url, output_dir):
    """
    âœ… å¤–éƒ¨æ¥å£ä¸å˜ï¼šprocess_url(url, output_dir)
    """
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC

    tries = 0
    max_tries = 2  # å• URL æœ€å¤šé‡è¯• 2 æ¬¡ï¼ˆæ€» 3 æ¬¡å°è¯•ï¼‰

    try:
        while True:
            driver = get_driver()
            print(f"\nğŸŒ æ­£åœ¨æŠ“å–: {url} (try={tries})", flush=True)

            try:
                driver.get(url)
                accept_cookies(driver)

                WebDriverWait(driver, PAGE_WAIT_TIMEOUT).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )
                time.sleep(AFTER_BODY_SLEEP)
                html = driver.page_source

                # æŒ‘æˆ˜é¡µ -> é€€é¿ + é‡è¯•
                if _is_challenge_page(html):
                    _inc_stat("challenge")
                    if tries >= max_tries:
                        raise RuntimeError("Challenge page (Cloudflare) - give up")
                    backoff = _compute_backoff(tries, "challenge")
                    print(f"âš ï¸ æŒ‘æˆ˜é¡µï¼Œé€€é¿ {backoff:.1f}s åé‡è¯•", flush=True)
                    time.sleep(backoff)
                    tries += 1
                    continue

                # æ­£å¸¸è§£æ
                info = parse_offer_info(html, url, site_name=CANON_SITE) or {}
                url_color = _normalize_color_from_url(url)

                if info.get("original_price_gbp"):
                    info["Product Price"] = info["original_price_gbp"]
                if info.get("discount_price_gbp"):
                    info["Adjusted Price"] = info["discount_price_gbp"]

                info.setdefault("Brand", "Barbour")
                info.setdefault("Product Name", "No Data")
                info.setdefault("Product Color", url_color or "No Data")
                info.setdefault("Product Description", _extract_description(html))
                info.setdefault("Feature", _extract_features(html))
                info.setdefault("Site Name", CANON_SITE)
                info["Source URL"] = url

                color_code = info.get("Product Color Code") or _extract_color_code_from_jsonld(html)
                if color_code:
                    info["Product Color Code"] = color_code
                    info["Product Code"] = color_code

                if not info.get("Product Gender"):
                    info["Product Gender"] = _infer_gender_from_name(info.get("Product Name", ""))

                offers = info.get("Offers") or []
                info["Product Size Detail"] = _build_sizes_from_offers(offers, info.get("Product Gender") or "")

                if color_code:
                    filename = f"{sanitize_filename(color_code)}.txt"
                else:
                    safe_name = sanitize_filename(info.get("Product Name", "NoName"))
                    safe_color = sanitize_filename(info.get("Product Color", "NoColor"))
                    filename = f"{safe_name}_{safe_color}.txt"

                output_dir.mkdir(parents=True, exist_ok=True)
                txt_path = output_dir / filename
                format_txt(info, txt_path, brand="Barbour")

                _inc_stat("ok")
                print(f"âœ… å†™å…¥: {txt_path.name}", flush=True)
                return

            except Exception as e:
                _inc_stat("fail")
                if tries >= max_tries:
                    raise
                backoff = _compute_backoff(tries, "fail")
                print(f"âš ï¸ å¤±è´¥: {repr(e)}\n   é€€é¿ {backoff:.1f}s åé‡è¯•", flush=True)
                time.sleep(backoff)
                tries += 1
                continue

    except Exception as final_e:
        print(f"âŒ å¤„ç†å¤±è´¥: {url}\n    {repr(final_e)}", flush=True)

    finally:
        # âœ… æ¯ä¸ª URL è®¡æ•°ä¸€æ¬¡ï¼Œç”¨äºå®šæœŸé‡å¯ driver
        mark_driver_used()


def outdoorandcountry_fetch_info(max_workers=3):
    """
    âœ… å¤–éƒ¨æ¥å£ä¸å˜ï¼šoutdoorandcountry_fetch_info(max_workers=3)
    """
    links_file = BARBOUR["LINKS_FILES"]["outdoorandcountry"]
    output_dir = BARBOUR["TXT_DIRS"]["outdoorandcountry"]
    output_dir.mkdir(parents=True, exist_ok=True)

    urls = []
    with open(links_file, "r", encoding="utf-8") as f:
        for line in f:
            u = line.strip()
            if u:
                urls.append(u)

    # v4.2ï¼šæœ‰æ•ˆå¹¶å‘ä¸Šé™ï¼ˆOutdoor å¼ºé£æ§ï¼Œé•¿æœŸè·‘ 2 æœ€ç¨³ï¼‰
    effective = min(int(max_workers), EFFECTIVE_MAX_WORKERS)

    print(f"ğŸ”„ å¯åŠ¨å¤šçº¿ç¨‹æŠ“å–ï¼ˆv4.2ï¼‰ï¼Œæ€»é“¾æ¥æ•°: {len(urls)}ï¼Œè¯·æ±‚å¹¶å‘: {max_workers}ï¼Œæœ‰æ•ˆå¹¶å‘: {effective}", flush=True)

    try:
        with ThreadPoolExecutor(max_workers=effective) as executor:
            futures = [executor.submit(process_url, url, output_dir) for url in urls]
            for fut in as_completed(futures):
                fut.result()
    finally:
        shutdown_all_drivers()


if __name__ == "__main__":
    outdoorandcountry_fetch_info(max_workers=2)
