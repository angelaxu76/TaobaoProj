import re
import time
import random
from pathlib import Path

from bs4 import BeautifulSoup
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

from config import BARBOUR
from common_taobao.core.selenium_utils import get_driver as get_shared_driver, quit_driver

# âœ… ä¸¤ä¸ªå…¥å£ï¼šBarbour & Barbour Internationalï¼ˆç¬¬1é¡µæ— å‚ï¼Œå…¶ä½™ ?dcp=Nï¼‰
BASE_URLS = [
    "https://www.houseoffraser.co.uk/brand/barbour",
    "https://www.houseoffraser.co.uk/brand/barbour-international",
]

OUTPUT_PATH = BARBOUR["LINKS_FILES"]["houseoffraser"]
OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)

# å•†å“è¯¦æƒ…é“¾æ¥çš„é€šç”¨ç‰¹å¾ï¼šç»“å°¾/ç‰‡æ®µä¸­å¸¦ -123456 è¿™ç±»æ•°å­— IDï¼ˆä¸å°‘è¿˜å¸¦ #colcode=ï¼‰
PRODUCT_HREF_PATTERN = re.compile(r"-\d{5,}([/#?]|$)")

# åˆ†é¡µé“¾æ¥ä¸­çš„ dcp=N
DCP_IN_HREF = re.compile(r"[?&]dcp=(\d+)")


def get_driver():
    """
    ä½¿ç”¨ common_taobao.selenium_utils æä¾›çš„å…±äº« driverï¼Œ
    ä¸å†é€šè¿‡ undetected_chromedriver / driver_auto è”ç½‘ä¸‹è½½ã€‚
    """
    return get_shared_driver(
        name="houseoffraser",
        headless=False,             # ä½ å¦‚æœæƒ³é™é»˜å¯ä»¥è‡ªå·±æ”¹ True
        window_size="1920,1080",
    )


def _build_page_url(base_url: str, page: int) -> str:
    """HoF åˆ†é¡µï¼šç¬¬1é¡µæ— å‚ï¼Œå…¶ä½™é¡µä½¿ç”¨ ?dcp=N"""
    if page <= 1:
        return base_url
    sep = "&" if "?" in base_url else "?"
    return f"{base_url}{sep}dcp={page}"


def _soft_scroll(driver, steps=5, pause=0.5):
    """å¤šæ®µæ»šåŠ¨è§¦å‘æ‡’åŠ è½½"""
    for i in range(steps):
        driver.execute_script(
            "window.scrollBy(0, Math.floor(document.body.scrollHeight * 0.30));"
        )
        time.sleep(pause)
    # å›åˆ°é¡¶éƒ¨ï¼Œç»™ DOM ç¨³å®šæ—¶é—´
    driver.execute_script("window.scrollTo(0, 0);")
    time.sleep(0.4)


def _wait_products(driver, timeout=15):
    """
    å…¼å®¹ç­‰å¾…ç­–ç•¥ï¼š
    - ä¼˜å…ˆç­‰å¾… .ProductImageList
    - å…œåº•ç­‰å¾…å¯èƒ½çš„å•†å“ a å…ƒç´ 
    """
    wait_css = ", ".join([
        "a.ProductImageList",
        "a[data-testid*='product']",
        "a[href*='-'][href*='/']",
    ])
    try:
        WebDriverWait(driver, timeout).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, wait_css))
        )
        return True
    except Exception:
        return False


def _discover_total_pages(html: str) -> int:
    """
    ä»ç¬¬1é¡µçš„åˆ†é¡µåŒºåŸŸæŠ“å–æœ€å¤§ dcp å€¼ï¼Œä½œä¸ºæ€»é¡µæ•°ã€‚
    æ‰¾ä¸åˆ°åˆ™è¿”å›ä¸€ä¸ªä¿å®ˆä¸Šé™ï¼ˆ200ï¼‰ã€‚
    """
    soup = BeautifulSoup(html, "html.parser")
    max_page = 1
    for a in soup.select("a[href]"):
        href = a.get("href") or ""
        m = DCP_IN_HREF.search(href)
        if m:
            try:
                n = int(m.group(1))
                if n > max_page:
                    max_page = n
            except ValueError:
                pass
    return max(max_page, 1)


def extract_links_from_html(html: str):
    """
    æå–é€»è¾‘ï¼ˆä¿æŒåŸé€‰æ‹©å™¨ä¼˜å…ˆ + å…œåº•ï¼‰ï¼š
    1) å…ˆèµ° a.ProductImageListï¼ˆä¸ä½ åŸé€»è¾‘ä¸€è‡´ï¼‰
    2) è‹¥ä¸ºç©ºï¼Œå†å¯¹æ‰€æœ‰ a[href] ç”¨æ­£åˆ™ç­›é€‰å« -æ•°å­—(â‰¥5ä½) çš„è¯¦æƒ…é“¾æ¥
    """
    soup = BeautifulSoup(html, "html.parser")
    links = set()

    # 1) åŸé€‰æ‹©å™¨
    for tag in soup.select("a.ProductImageList"):
        href = (tag.get("href") or "").strip()
        if not href:
            continue
        if href.startswith("http"):
            links.add(href)
        elif href.startswith("/"):
            links.add("https://www.houseoffraser.co.uk" + href)

    if links:
        return links

    # 2) å…œåº•
    for tag in soup.select("a[href]"):
        href = (tag.get("href") or "").strip()
        if not href:
            continue
        if href.startswith("/"):
            full = "https://www.houseoffraser.co.uk" + href
        elif href.startswith("http"):
            full = href
        else:
            continue

        if PRODUCT_HREF_PATTERN.search(href):
            links.add(full)

    return links


def _crawl_category(driver, base_url: str, all_links: set):
    """
    æ ¸å¿ƒç­–ç•¥ï¼š
    - ç¬¬1é¡µï¼šåŠ è½½ -> æ»šåŠ¨ -> ç­‰å¾… -> æå–é“¾æ¥ + å‘ç°æ€»é¡µæ•°
    - ä¹‹åé¡µï¼šé€é¡µçˆ¬ï¼Œè¿ç€ 3 é¡µâ€œæ— æ–°å¢â€æ‰åœæ­¢ï¼ˆé˜²æŠ–ï¼‰
    """
    # ç¬¬ 1 é¡µ
    first_url = _build_page_url(base_url, 1)
    print(f"ğŸŒ æŠ“å–ç¬¬ 1 é¡µ: {first_url}")
    driver.get(first_url)

    # é¡µé¢æŒ‚è½½ & æ‡’åŠ è½½
    time.sleep(1.0)
    _soft_scroll(driver, steps=6, pause=0.45)
    _wait_products(driver, timeout=18)

    html = driver.page_source
    total_pages = _discover_total_pages(html)
    print(f"ğŸ” è§£æåˆ°æ€»é¡µæ•°ï¼ˆå¯èƒ½ï¼‰ï¼š{total_pages}")

    first_links = extract_links_from_html(html)
    new_links = [u for u in first_links if u not in all_links]
    print(f"âœ… ç¬¬ 1 é¡µæå– {len(new_links)} ä¸ªæ–°é“¾æ¥")
    all_links.update(new_links)

    # åç»­é¡µ
    consecutive_no_new = 0
    for page in range(2, total_pages + 1):
        page_url = _build_page_url(base_url, page)
        print(f"ğŸŒ æŠ“å–ç¬¬ {page} é¡µ: {page_url}")
        driver.get(page_url)

        # ç»™é¡µé¢æŒ‚è½½æ—¶é—´ + æ‡’åŠ è½½æ»šåŠ¨
        time.sleep(0.8 + random.random() * 0.6)
        _soft_scroll(driver, steps=5, pause=0.4)
        _wait_products(driver, timeout=15)

        html = driver.page_source
        links = extract_links_from_html(html)
        page_new = [u for u in links if u not in all_links]

        if page_new:
            print(f"âœ… ç¬¬ {page} é¡µæå– {len(page_new)} ä¸ªæ–°é“¾æ¥")
            all_links.update(page_new)
            consecutive_no_new = 0
        else:
            consecutive_no_new += 1
            print(f"â„¹ï¸ ç¬¬ {page} é¡µæ— æ–°å¢é“¾æ¥ï¼ˆè¿ç»­ {consecutive_no_new}/3ï¼‰")

        # è¿ç»­ 3 é¡µæ²¡æ–°å¢ â†’ ç»“æŸè¯¥åˆ†ç±»ï¼ˆé˜²æ­¢å¶å‘æŸé¡µåˆ¤ç©ºå°±æå‰é€€å‡ºï¼‰
        if consecutive_no_new >= 3:
            print("ğŸ›‘ è¿ç»­ 3 é¡µæ— æ–°å¢ï¼Œç»“æŸè¯¥åˆ†ç±»")
            break

        # é¡µé—´éšæœºç­‰å¾…ï¼Œé™ä½é£æ§
        time.sleep(0.7 + random.random() * 0.8)


def houseoffraser_get_links():
    print("ğŸš€ å¼€å§‹æŠ“å– House of Fraser å•†å“é“¾æ¥ï¼ˆBarbour & Barbour Internationalï¼‰")
    OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)

    driver = get_driver()

    try:
        # âœ… åªåœ¨é¦–æ¬¡åŠ è½½ç¬¬ä¸€ä¸ªåˆ†ç±»å‰åœç•™ 10 ç§’æ‰‹åŠ¨ç‚¹å‡» Cookieï¼ˆåç»­å¤ç”¨åŒä¸€å®ä¾‹ï¼‰
        print("ğŸ•’ å·²æ‰“å¼€æµè§ˆå™¨ï¼Œå°†æ‰“å¼€é¦–åˆ†ç±»ç¬¬1é¡µã€‚è¯·åœ¨ 10 ç§’å†…æ‰‹åŠ¨ç‚¹å‡» Cookie çš„ 'Allow all' æŒ‰é’®...")
        driver.get(BASE_URLS[0])
        time.sleep(10)
        print("âœ… å·²ç­‰å¾… 10 ç§’ï¼Œå¼€å§‹æ­£å¼æŠ“å–")

        all_links = set()

        # âœ… ä¾æ¬¡æŠ“å–ä¸¤ä¸ªåˆ†ç±»ï¼ˆBarbour / Barbour Internationalï¼‰
        for base_url in BASE_URLS:
            print(f"\n===== ğŸ§­ å½“å‰åˆ†ç±»ï¼š{base_url} =====")
            _crawl_category(driver, base_url, all_links)

        # âœ… å†™å…¥ txtï¼ˆè¦†ç›–å†™ï¼‰
        links_sorted = sorted(all_links)
        OUTPUT_PATH.write_text("\n".join(links_sorted), encoding="utf-8")

        print(f"\nâœ… æŠ“å–å®Œæˆï¼šå…± {len(links_sorted)} æ¡é“¾æ¥")
        print(f"ğŸ“ å·²å†™å…¥: {OUTPUT_PATH}")

    finally:
        quit_driver(driver)
