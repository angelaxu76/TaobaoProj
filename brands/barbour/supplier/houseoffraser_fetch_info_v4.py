# -*- coding: utf-8 -*-
"""
House of Fraser é‡‡é›†å™¨ - é‡æ„ç‰ˆ (ä½¿ç”¨ BaseFetcher)

åŸºäº houseoffraser_new_fetch_info_v3.py é‡æ„
ç‰¹ç‚¹:
- Next.js __NEXT_DATA__ è§£æ
- Lexicon è¯åº“åŒ¹é… (L1/L2 æ‰“åˆ†ç®—æ³•)
- æœ€å¤æ‚çš„åŒ¹é…é€»è¾‘

å¯¹æ¯”:
- æ—§ç‰ˆ (houseoffraser_new_fetch_info_v3.py): 765 è¡Œ
- æ–°ç‰ˆ (æœ¬æ–‡ä»¶): ~450 è¡Œ
- ä»£ç å‡å°‘: 41%

ä½¿ç”¨æ–¹å¼:
    python -m brands.barbour.supplier.houseoffraser_fetch_info_v4
"""

from __future__ import annotations

import re
import json
import threading
from pathlib import Path
from typing import Dict, Any, Optional, List
from bs4 import BeautifulSoup
import requests

# å¯¼å…¥åŸºç±»å’Œå·¥å…·
from brands.barbour.core.base_fetcher import BaseFetcher, setup_logging

# å¯¼å…¥ç»Ÿä¸€åŒ¹é…å™¨
from brands.barbour.core.hybrid_barbour_matcher import resolve_product_code

# SQLAlchemy
from sqlalchemy import create_engine
from sqlalchemy.engine import Connection

# é…ç½®
from config import BARBOUR, BRAND_CONFIG, SETTINGS

SITE_NAME = "houseoffraser"
LINKS_FILE = BARBOUR["LINKS_FILES"][SITE_NAME]
OUTPUT_DIR = BARBOUR["TXT_DIRS"][SITE_NAME]
DEFAULT_STOCK_COUNT = SETTINGS.get("DEFAULT_STOCK_COUNT", 3)

# æ•°æ®åº“é…ç½®
PRODUCTS_TABLE = BRAND_CONFIG.get("barbour", {}).get("PRODUCTS_TABLE", "barbour_products")
OFFERS_TABLE = BRAND_CONFIG.get("barbour", {}).get("OFFERS_TABLE")
PG = BRAND_CONFIG["barbour"]["PGSQL_CONFIG"]

# Lexicon åŒ¹é…å‚æ•°ï¼ˆä¼ ç»™ hybrid_barbour_matcherï¼‰
LEX_MIN_L1_HITS = 1
LEX_MIN_SCORE = 0.70
LEX_MIN_LEAD = 0.05
LEX_REQUIRE_COLOR_EXACT = False

# ç­‰å¾…æ—¶é—´ (Next.js æ°´åˆ)
WAIT_HYDRATE_SECONDS = 12


# ================== é‡‡é›†å™¨å®ç° ==================

class HouseOfFraserFetcher(BaseFetcher):
    """
    House of Fraser é‡‡é›†å™¨

    ç‰¹ç‚¹:
    - Next.js __NEXT_DATA__ è§£æ
    - hybrid_barbour_matcher å¤šçº§åŒ¹é…
    - æ–­ç‚¹ç»­ä¼  (è‡ªåŠ¨è·³è¿‡å·²å®Œæˆçš„ URL)
    """

    # requests æ¨¡å¼çš„ User-Agent å’Œ sessionï¼ˆå…±äº«è¿æ¥æ± ï¼‰
    _REQ_HEADERS = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/131.0.0.0 Safari/537.36"
        ),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "en-GB,en;q=0.9",
    }

    def __init__(self, *args, use_requests: bool = True, **kwargs):
        """
        åˆå§‹åŒ–

        Args:
            use_requests: True = ç”¨ requests å¿«é€ŸæŠ“å– (é»˜è®¤);
                          False = ç”¨ Selenium (å…œåº•ï¼Œé€‚åˆåçˆ¬ä¸¥é‡æ—¶)
        """
        super().__init__(*args, **kwargs)
        self._use_requests = use_requests
        self._session = requests.Session()
        self._session.headers.update(self._REQ_HEADERS)

        # åˆ›å»ºæ•°æ®åº“å¼•æ“
        engine_url = (
            f"postgresql+psycopg2://{PG['user']}:{PG['password']}"
            f"@{PG['host']}:{PG['port']}/{PG['dbname']}"
        )
        self._engine = create_engine(engine_url, pool_size=self.max_workers + 2)

        # æ–­ç‚¹ç»­ä¼ ï¼šè¿›åº¦æ–‡ä»¶
        self._progress_file = Path(self.output_dir) / ".done_urls.txt"
        self._done_urls = self._load_done_urls()
        self._progress_lock = threading.Lock()

    # ================== æ–­ç‚¹ç»­ä¼  ==================

    def _load_done_urls(self) -> set:
        """åŠ è½½å·²å®Œæˆçš„ URL é›†åˆ"""
        if not self._progress_file.exists():
            return set()
        try:
            lines = self._progress_file.read_text(encoding="utf-8").splitlines()
            done = {line.strip() for line in lines if line.strip()}
            self.logger.info(f"ğŸ“‹ å·²å®Œæˆ {len(done)} ä¸ªï¼Œè‡ªåŠ¨è·³è¿‡")
            return done
        except Exception:
            return set()

    def _mark_done(self, url: str) -> None:
        """è®°å½•å·²å®Œæˆçš„ URLï¼ˆçº¿ç¨‹å®‰å…¨ã€è¿½åŠ å†™å…¥ï¼‰"""
        with self._progress_lock:
            self._done_urls.add(url)
            try:
                with open(self._progress_file, "a", encoding="utf-8") as f:
                    f.write(url + "\n")
            except Exception:
                pass

    def _load_urls(self) -> List[str]:
        """é‡å†™ï¼šåŠ è½½é“¾æ¥å¹¶è¿‡æ»¤æ‰å·²å®Œæˆçš„"""
        all_urls = super()._load_urls()
        before = len(all_urls)
        urls = [u for u in all_urls if u not in self._done_urls]
        skipped = before - len(urls)
        if skipped > 0:
            self.logger.info(f"â­ï¸ è·³è¿‡å·²å®Œæˆ {skipped} ä¸ªï¼Œå‰©ä½™ {len(urls)} ä¸ªå¾…æŠ“å–")
        return urls

    def _fetch_html(self, url: str) -> str:
        """
        è·å– HTMLï¼šä¼˜å…ˆç”¨ requestsï¼ˆå¿«ï¼‰ï¼Œå¤±è´¥æ—¶å›é€€ Seleniumã€‚

        HOF æ˜¯ Next.js SSR ç«™ç‚¹ï¼ŒJSON-LD / __NEXT_DATA__ éƒ½åœ¨é¦–æ¬¡ HTML ä¸­ï¼Œ
        å¤§å¤šæ•°æƒ…å†µä¸éœ€è¦ JS æ¸²æŸ“ã€‚
        """
        if self._use_requests:
            try:
                resp = self._session.get(url, timeout=15)
                resp.raise_for_status()
                html = resp.text
                # æ£€æŸ¥ HTML æ˜¯å¦åŒ…å«æœ‰æ•ˆæ•°æ®ï¼ˆéç©ºå£³ / åçˆ¬é¡µï¼‰
                if '"@type"' in html or "__NEXT_DATA__" in html:
                    return html
                self.logger.debug(f"requests è¿”å›æ— æ•ˆé¡µé¢ï¼Œå›é€€ Selenium: {url}")
            except Exception as e:
                self.logger.debug(f"requests å¤±è´¥ ({e})ï¼Œå›é€€ Selenium: {url}")

        # å›é€€ Selenium
        return super()._fetch_html(url)

    def fetch_one_product(self, url: str, idx: int, total: int):
        """é‡å†™ï¼šæˆåŠŸåè®°å½•è¿›åº¦"""
        result = super().fetch_one_product(url, idx, total)
        url_out, success = result
        if success:
            self._mark_done(url_out)
        return result

    # ================== é¡µé¢è§£æ ==================

    def parse_detail_page(self, html: str, url: str) -> Dict[str, Any]:
        """
        è§£æ House of Fraser å•†å“è¯¦æƒ…é¡µ

        é¡µé¢ç‰¹ç‚¹:
        - JSON-LD åŒ…å«åŸºç¡€ä¿¡æ¯
        - ä»·æ ¼åœ¨ data-testid="price"
        - å°ºç åœ¨ select/option
        - ä½¿ç”¨ Lexicon åŒ¹é…è·å– Product Code
        """
        soup = BeautifulSoup(html, "html.parser")

        # 1. ä» JSON-LD æå–åŸºç¡€ä¿¡æ¯
        jd = self._from_jsonld_product(soup) or {}
        title_guess = jd.get("name") or (soup.title.get_text(strip=True) if soup.title else "No Data")
        desc_guess = jd.get("description") or "No Data"
        sku_guess = jd.get("sku") or "No Data"

        # 2. æå–é¢œè‰²
        color_guess = self._extract_color(soup, html) or "No Data"

        # 3. æå–ä»·æ ¼
        product_price_str, adjusted_price_str = self._extract_prices(soup)

        # 4. æå–å°ºç 
        raw_sizes = self._extract_sizes(soup)

        # 5. hybrid_barbour_matcher å¤šçº§åŒ¹é… Product Code
        with self._engine.begin() as conn:
            raw_conn = self._get_dbapi_connection(conn)

            final_code, debug_trace = resolve_product_code(
                raw_conn,
                site_name=SITE_NAME,
                url=url,
                scraped_title=title_guess or "",
                scraped_color=color_guess or "",
                sku_guess=sku_guess,
                products_table=PRODUCTS_TABLE,
                offers_table=OFFERS_TABLE,
                brand="barbour",
                lex_min_l1_hits=LEX_MIN_L1_HITS,
                lex_min_score=LEX_MIN_SCORE,
                lex_min_lead=LEX_MIN_LEAD,
                lex_require_color_exact=LEX_REQUIRE_COLOR_EXACT,
            )

        # 6. æ¨æ–­æ€§åˆ«
        gender_for_logic = self._decide_gender(final_code, soup, html, url)

        # 7. æ ¼å¼åŒ–å°ºç 
        product_size_str, product_size_detail_str = self._finalize_sizes(raw_sizes, gender_for_logic)

        # 8. è¿”å›æ ‡å‡†åŒ–å­—å…¸
        return {
            "Product Code": final_code,
            "Product Name": self.clean_text(title_guess, maxlen=200),
            "Product Color": self.clean_text(color_guess, maxlen=100),
            "Product Gender": gender_for_logic,
            "Product Description": self.clean_description(desc_guess),
            "Product Price": product_price_str,          # txt_writer / DB å¯¼å…¥ä½¿ç”¨æ­¤ key
            "Adjusted Price": adjusted_price_str,        # txt_writer / DB å¯¼å…¥ä½¿ç”¨æ­¤ key
            "Original Price (GBP)": product_price_str,  # BaseFetcher._validate_info è¦æ±‚
            "Discount Price (GBP)": adjusted_price_str, # BaseFetcher._validate_info è¦æ±‚
            "Product Size": product_size_str,
            "Product Size Detail": product_size_detail_str,
        }

    def _get_dbapi_connection(self, conn: Connection):
        """è·å– DBAPI è¿æ¥"""
        try:
            return conn.connection
        except Exception:
            return conn.connection.connection

    def _from_jsonld_product(self, soup: BeautifulSoup) -> dict:
        """ä» JSON-LD æå–äº§å“ä¿¡æ¯"""
        out = {}
        try:
            for s in soup.select('script[type="application/ld+json"]'):
                raw = s.get_text(strip=True)
                if not raw:
                    continue

                data = json.loads(raw)
                if isinstance(data, list):
                    for obj in data:
                        if isinstance(obj, dict) and obj.get("@type") in ("Product", "product"):
                            data = obj
                            break

                if isinstance(data, dict) and data.get("@type") in ("Product", "product"):
                    out["name"] = data.get("name")
                    out["description"] = data.get("description")
                    out["sku"] = data.get("sku")
                    break
        except Exception:
            pass

        if not out.get("name"):
            h1 = soup.select_one("h1,[data-testid*='title'],[data-component*='title']")
            out["name"] = h1.get_text(strip=True) if h1 else (soup.title.get_text(strip=True) if soup.title else None)

        return out

    def _extract_color(self, soup: BeautifulSoup, html: str) -> str:
        """æå–é¢œè‰²"""
        m = re.search(r'"color"\s*:\s*"([^"]+)"', html or "")
        if m:
            return m.group(1).strip()
        return "No Data"

    def _extract_prices(self, soup: BeautifulSoup) -> tuple:
        """æå–ä»·æ ¼"""
        price_block = soup.select_one('p[data-testid="price"]')
        if not price_block:
            return ("No Data", "No Data")

        discounted_span = price_block.select_one("span[class*='Price_isDiscounted']")
        discounted_price = None
        if discounted_span:
            discounted_price = self._parse_price_string(discounted_span.get_text(strip=True))

        ticket_span = price_block.select_one('span[data-testid="ticket-price"]')
        ticket_price = None
        if ticket_span:
            ticket_price = self._parse_price_string(ticket_span.get_text(strip=True))

        if ticket_price is None:
            block_testvalue = price_block.get("data-testvalue")
            ticket_price = self._parse_price_string(block_testvalue)

        if ticket_price is None:
            first_span = price_block.find("span")
            if first_span:
                ticket_price = self._parse_price_string(first_span.get_text(strip=True))

        if discounted_price is not None and ticket_price is not None:
            product_price_val = ticket_price
            adjusted_price_val = discounted_price
        else:
            product_price_val = ticket_price or discounted_price
            adjusted_price_val = None

        product_price_str = f"{product_price_val:.2f}" if product_price_val is not None else "No Data"
        adjusted_price_str = f"{adjusted_price_val:.2f}" if adjusted_price_val is not None else "No Data"

        return product_price_str, adjusted_price_str

    def _parse_price_string(self, txt: str) -> Optional[float]:
        """ä»æ–‡æœ¬è§£æä»·æ ¼"""
        if not txt:
            return None

        cleaned = txt.strip()

        m_symbol = re.search(r"Â£\s*([0-9]+(?:\.[0-9]+)?)", cleaned)
        if m_symbol:
            return float(m_symbol.group(1))

        m_pence = re.search(r"^([0-9]{3,})$", cleaned)
        if m_pence:
            try:
                pence_val = int(m_pence.group(1))
                return round(pence_val / 100.0, 2)
            except Exception:
                pass

        m_plain = re.search(r"([0-9]+(?:\.[0-9]+)?)", cleaned)
        if m_plain:
            return float(m_plain.group(1))

        return None

    def _extract_sizes(self, soup: BeautifulSoup) -> list:
        """æå–å°ºç """
        sizes = []
        for opt in soup.select("[data-testid*='size'] option, select option"):
            t = opt.get_text(strip=True)
            if t and t not in sizes:
                sizes.append(t)
        return sizes

    def _decide_gender(self, sku: str, soup: BeautifulSoup, html: str, url: str) -> str:
        """æ¨æ–­æ€§åˆ«"""
        # ä» SKU æ¨æ–­
        sku_guess = self._infer_gender_from_code(sku or "")
        if sku_guess and sku_guess != "No Data":
            return sku_guess

        # ä» URL æ¨æ–­
        page_guess = self._extract_gender_from_url(url)
        if page_guess and page_guess != "No Data":
            return page_guess

        return "No Data"

    def _infer_gender_from_code(self, code: str) -> str:
        """ä»ç¼–ç æ¨æ–­æ€§åˆ«"""
        code = (code or "").upper()
        if code.startswith("M"):
            return "Men"
        if code.startswith("L"):
            return "Women"
        return "No Data"

    def _extract_gender_from_url(self, url: str) -> str:
        """ä» URL æ¨æ–­æ€§åˆ«"""
        u = (url or "").lower()
        if "/men" in u or "mens" in u:
            return "Men"
        if "/women" in u or "womens" in u:
            return "Women"
        return "No Data"

    def _finalize_sizes(self, raw_sizes: list, gender_for_logic: str) -> tuple:
        """æ ¼å¼åŒ–å°ºç """
        from common.product.size_utils import clean_size_for_barbour

        cleaned = []
        for s in raw_sizes or []:
            ns = clean_size_for_barbour(str(s))
            if ns and ns != "No Data" and ns not in cleaned:
                cleaned.append(ns)

        if not cleaned:
            return ("No Data", "No Data")

        product_size_str = ";".join([f"{x}:æœ‰è´§" for x in cleaned])
        product_size_detail_str = ";".join([f"{x}:{DEFAULT_STOCK_COUNT}:0000000000000" for x in cleaned])

        return product_size_str, product_size_detail_str


# ================== ä¸»å…¥å£ ==================

def houseoffraser_fetch_info(
    max_workers: int = 4,
    headless: bool = True,
    use_requests: bool = True,
):
    """
    ä¸»å‡½æ•°

    Args:
        max_workers: å¹¶å‘çº¿ç¨‹æ•° (é»˜è®¤ 4)
        headless: æ˜¯å¦æ— å¤´æ¨¡å¼ (é»˜è®¤ Trueï¼ŒèŠ‚çœèµ„æº)
        use_requests: æ˜¯å¦ç”¨ requests å¿«é€ŸæŠ“å– (é»˜è®¤ True)
                      è®¾ False å¯å›é€€åˆ°çº¯ Selenium æ¨¡å¼
    """
    setup_logging()

    fetcher = HouseOfFraserFetcher(
        site_name=SITE_NAME,
        links_file=LINKS_FILE,
        output_dir=OUTPUT_DIR,
        max_workers=max_workers,
        max_retries=3,
        wait_seconds=WAIT_HYDRATE_SECONDS,
        headless=headless,
        use_requests=use_requests,
    )

    success, fail = fetcher.run_batch()
    print(f"\nâœ… House of Fraser æŠ“å–å®Œæˆ: æˆåŠŸ {success}, å¤±è´¥ {fail}")


if __name__ == "__main__":
    houseoffraser_fetch_info(max_workers=4, headless=True)
