# -*- coding: utf-8 -*-
"""
Philip Morris Direct | Barbour å•†å“æŠ“å–ï¼ˆæ”¯æŒå¤šé¢œè‰²ã€å¤š TXTã€é¢œè‰²ç¼–ç æ˜ å°„ï¼‰

æ ¸å¿ƒé€»è¾‘ï¼š
1. Selenium æ‰“å¼€å•†å“é¡µï¼Œè¯†åˆ«æ‰€æœ‰é¢œè‰²é€‰é¡¹ï¼ˆlabel.form-option.label-imgï¼‰
2. å¯¹æ¯ä¸ªé¢œè‰²ï¼š
   - ç‚¹å‡»è¯¥é¢œè‰²
   - æŠ“å–å½“å‰é¡µé¢ä¸Šçš„å°ºç  & åº“å­˜ï¼ˆæœ‰è´§/æ— è´§ï¼‰
   - ç»„åˆæˆ Product Size: "S:æœ‰è´§;M:æ— è´§;..."
3. ä» HTML ä¸­æå– Barbour æ¬¾å¼ç¼–ç ï¼ˆå¦‚ MQU0281ï¼‰
4. ç”¨ é¢œè‰²è‹±æ–‡ -> é¢œè‰²ç®€å†™ï¼ˆBK/NY/OL/SG...ï¼‰å¾—åˆ°ç»„åˆå‰ç¼€ï¼šMQU0281OL
5. å» PostgreSQL çš„ barbour_products è¡¨æŸ¥ï¼š
   SELECT product_code
   FROM barbour_products
   WHERE product_code ILIKE 'MQU0281OL%'
   LIMIT 1
   è‹¥æ‰¾åˆ°ï¼Œç”¨è¯¥ product_code ä½œä¸ºï¼š
       - TXT æ–‡ä»¶åï¼ˆMQU0281OL51.txtï¼‰
       - TXT ä¸­çš„ Product Code
   è‹¥æ‰¾ä¸åˆ°ï¼Œåˆ™é™çº§ç”¨ MQU0281OL ä½œä¸º Product Code
6. æ‰€æœ‰å­—æ®µç»Ÿä¸€å†™å…¥ info dictï¼Œæœ€åç”¨ txt_writer.format_txt(info, filepath, brand="Barbour")
"""

import re
import time
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed

import undetected_chromedriver as uc
from bs4 import BeautifulSoup
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import psycopg2
import tempfile
from config import BARBOUR
from common_taobao.ingest.txt_writer import format_txt
from selenium import webdriver
try:
    from selenium_stealth import stealth
except ImportError:
    def stealth(*args, **kwargs):
        return

# ========== é…ç½® ==========
LINKS_FILE: Path = BARBOUR["LINKS_FILES"]["philipmorris"]
TXT_DIR: Path = BARBOUR["TXT_DIRS"]["philipmorris"]
SITE_NAME = "Philip Morris"
PGSQL_CONFIG = BARBOUR["PGSQL_CONFIG"]
COLOR_CODE_MAP = BARBOUR.get("BARBOUR_COLOR_CODE_MAP", {})

TXT_DIR.mkdir(parents=True, exist_ok=True)


# ========== æµè§ˆå™¨ ==========

from common_taobao.core.selenium_utils import get_driver as get_shared_driver

def get_driver(headless: bool = True):
    """
    ä½¿ç”¨é¡¹ç›®ç»Ÿä¸€çš„ selenium_utils.get_driverï¼ˆå…¨å±€å…±äº« chromedriverï¼‰
    å·²ç»ä¸å†ä½¿ç”¨ build_uc_driver / undetected_chromedriverã€‚
    """
    print("ğŸš— [get_driver] è°ƒç”¨å…¨å±€ selenium_utils.get_driver() ...")
    driver = get_shared_driver(
        name="philipmorris",
        headless=headless,
        window_size="1920,1080"
    )
    return driver



def accept_cookies(driver, timeout: int = 8):
    """å°½é‡ç‚¹æ‰å¼¹å‡ºçš„ cookie å¼¹çª—ï¼Œä¸å½±å“æ­£å¸¸è¿è¡Œï¼Œå¤±è´¥å°±å¿½ç•¥ã€‚"""
    try:
        WebDriverWait(driver, timeout).until(
            EC.element_to_be_clickable((By.CSS_SELECTOR, "button#onetrust-accept-btn-handler"))
        ).click()
        time.sleep(1)
    except Exception:
        # æœ‰äº›é¡µé¢å¯èƒ½æ˜¯åˆ«çš„ cookie æ ·å¼ï¼Œå…ˆå¿½ç•¥
        pass


# ========== å·¥å…·å‡½æ•° ==========

def sanitize_filename(name: str) -> str:
    return re.sub(r"[\\/:*?\"<>|'\s]+", "_", (name or "")).strip("_")


def extract_style_code(html: str) -> str | None:
    """ä»æ•´é¡µ HTML ä¸­æå– Barbour æ¬¾å¼ç¼–ç ï¼šä¸‰å­—æ¯ + å››æ•°å­—ï¼Œå¦‚ MQU0281ã€‚"""
    m = re.search(r"\b[A-Z]{3}\d{4}\b", html)
    return m.group(0) if m else None


def _clean_price_text(text: str) -> str:
    """ä» 'Â£179.00' ä¹‹ç±»çš„å­—ç¬¦ä¸²ä¸­æå–å‡ºæ•°å­—éƒ¨åˆ† '179.00'ã€‚"""
    if not text:
        return ""
    t = text.strip()
    m = re.search(r"([0-9]+(?:\.[0-9]{1,2})?)", t.replace(",", ""))
    return m.group(1) if m else ""


def extract_prices_pmd(soup: BeautifulSoup) -> tuple[str, str]:
    """
    è¿”å› (åŸä»·, æ‰“æŠ˜ä»·)ï¼Œéƒ½ä¸ºå­—ç¬¦ä¸²ï¼Œä¾‹å¦‚: ('179.00', '142.95')
    å¦‚æœæ²¡æœ‰æ‰“æŠ˜ï¼Œåˆ™ä¸¤ä¸ªå€¼ç›¸åŒï¼›å¦‚æœæŸä¸ªå–ä¸åˆ°ï¼Œç”¨å¦ä¸€ä¸ªå…œåº•ã€‚
    """
    sale = ""   # æ‰“æŠ˜åä»·
    orig = ""   # åŸä»· / RRP

    # æ‰“æŠ˜åä»·ï¼šspan.price.price--withTax
    for span in soup.select("span.price.price--withTax"):
        val = _clean_price_text(span.get_text())
        if val:
            sale = val
            break

    # åŸä»· / RRPï¼šspan.price.price--rrp
    for span in soup.select("span.price.price--rrp"):
        val = _clean_price_text(span.get_text())
        if val:
            orig = val
            break

    # å…œåº•ï¼šå¦‚æœæ‰“æŠ˜ä»·æ²¡æ‹¿åˆ°ï¼Œç”¨ meta é‡Œçš„ price amount
    if not sale:
        meta = soup.find("meta", {"property": "product:price:amount"})
        if meta and meta.get("content"):
            sale = meta["content"].strip()

    # å†å…œåº•ï¼šå¦‚æœåŸä»·æ²¡æ‹¿åˆ°ï¼Œå°±ç­‰äºå½“å‰å”®ä»·
    if not orig:
        orig = sale

    return orig, sale


# def extract_price(soup: BeautifulSoup) -> str:
#     """ä» meta æ ‡ç­¾æå–ä»·æ ¼ï¼ŒGBP é‡‘é¢ï¼Œæ‰¾ä¸åˆ°è¿”å› '0.00'ã€‚"""
#     meta = soup.find("meta", {"property": "product:price:amount"})
#     if meta and meta.get("content"):
#         return meta["content"].strip()
#     return "0.00"


def extract_product_name(soup: BeautifulSoup) -> str:
    h1 = soup.find("h1", class_="productView-title")
    if h1 and h1.text.strip():
        return h1.text.strip()
    # å…œåº•ç”¨ <title>
    title = soup.find("title")
    if title and title.text:
        return title.text.split("|")[0].strip()
    return "No Data"


def extract_description(soup: BeautifulSoup) -> str:
    """å°½é‡å– Description tab çš„æ–‡å­—ï¼›å¤±è´¥å°±è¿”å› 'No Data'ã€‚"""
    desc = soup.find("div", id="tab-description")
    if not desc:
        desc = soup.find("div", class_="productView-description")
    if not desc:
        return "No Data"
    text = " ".join(desc.stripped_strings)
    return text or "No Data"


def infer_gender(product_name: str) -> str:
    """æç®€æ€§åˆ«æ¨æ–­ï¼šè¯†åˆ« Men's / Ladies / Women'sï¼Œå¦åˆ™é»˜è®¤ Menã€‚"""
    name = (product_name or "").lower()
    if any(w in name for w in ["women", "woman", "ladies", "lady", "women's", "woman's"]):
        return "Women"
    if any(w in name for w in ["men", "men's", "man's"]):
        return "Men"
    return "Men"  # Barbour å¤–å¥—é»˜è®¤æŒ‰ç”·æ¬¾å…œåº•ï¼Œå¯¹ä½ å½±å“ä¸å¤§


def parse_sizes_from_html(html: str) -> list[tuple[str, str]]:
    """
    ä»å½“å‰é¢œè‰²å¯¹åº”é¡µé¢è§£æå°ºç  + åº“å­˜çŠ¶æ€
    è¿”å›åˆ—è¡¨: [(size_text, 'æœ‰è´§'/'æ— è´§'), ...]
    """
    soup = BeautifulSoup(html, "html.parser")
    labels = soup.select("label.form-option")
    sizes: list[tuple[str, str]] = []

    for label in labels:
        classes = label.get("class", [])
        # è¿‡æ»¤é¢œè‰²æŒ‰é’®ï¼ˆæœ‰ label-imgï¼‰
        if "label-img" in classes:
            continue
        span = label.find("span", class_="form-option-variant")
        if not span:
            continue
        size_text = span.text.strip()
        stock_status = "æ— è´§" if "unavailable" in classes else "æœ‰è´§"
        sizes.append((size_text, stock_status))
    return sizes


def build_product_size_str(sizes: list[tuple[str, str]]) -> str:
    """
    æŠŠ [(size, status), ...] èšåˆæˆ:
        "S:æœ‰è´§;M:æ— è´§;..."
    åŒä¸€å°ºç å¤šæ¬¡å‡ºç°æ—¶ï¼Œåªè¦æœ‰ä¸€ä¸ªâ€œæœ‰è´§â€å°±ç®—æœ‰è´§ã€‚
    """
    agg = {}
    order = []
    for size, status in sizes:
        if size not in agg:
            agg[size] = status
            order.append(size)
        else:
            # åªè¦ä»»æ„ä¸€ä¸ªæœ‰è´§ï¼Œå°±è®¤ä¸ºæœ‰è´§
            if status == "æœ‰è´§" or agg[size] == "æœ‰è´§":
                agg[size] = "æœ‰è´§"
            else:
                agg[size] = "æ— è´§"

    tokens = [f"{s}:{agg[s]}" for s in order]
    return ";".join(tokens)


def map_color_to_code(color_name: str) -> str | None:
    """
    ç”¨ BARBOUR_COLOR_CODE_MAP æŠŠè‹±æ–‡é¢œè‰²æ˜ å°„åˆ°ç®€å†™ï¼š
    - ä¾‹å¦‚ 'Black' -> 'BK'
    - æ”¯æŒ 'Beige/Antique White' è¿™ç§ï¼Œå–ç¬¬ä¸€ä¸ªé¢œè‰²
    """
    if not color_name:
        return None
    s = color_name.strip().lower()
    # ç»„åˆè‰²åªå–ç¬¬ä¸€ä¸ª
    if "/" in s:
        s = s.split("/")[0].strip()

    for code, names in COLOR_CODE_MAP.items():
        en = (names.get("en") or "").lower()
        if not en:
            continue
        # å…¨ç­‰ / åŒ…å«ä»»æ„ä¸€ç§æƒ…å†µéƒ½ç®—åŒ¹é…
        if s == en or s in en or en in s:
            return code
    return None


def find_product_code_in_db(style_code: str, color_name: str, conn) -> str | None:
    """
    é€šè¿‡ æ¬¾å¼ç¼–ç  + é¢œè‰²è‹±æ–‡ï¼Œä» barbour_products ä¸­æ‰¾åˆ°â€œçœŸæ­£çš„å•†å“ç¼–ç â€ï¼š
        style_code + color_code_abbr + å°ºç åç¼€
    ä¾‹å¦‚: MQU0281 + OL -> åŒ¹é… MQU0281OL51
    """
    if not style_code or not color_name or conn is None:
        return None

    color_abbr = map_color_to_code(color_name)
    if not color_abbr:
        print(f"âš ï¸ æœªæ‰¾åˆ°é¢œè‰²ç®€å†™æ˜ å°„ï¼šstyle={style_code}, color={color_name}")
        return None

    sql = """
        SELECT product_code
        FROM barbour_products
        WHERE product_code ILIKE %s
        ORDER BY product_code
        LIMIT 1
    """

    # ===== ç¬¬ä¸€æ¬¡ï¼šç”¨æ­£å¸¸çš„é¢œè‰²ç®€å†™ï¼Œä¾‹å¦‚ Sage -> SG =====
    prefix = f"{style_code}{color_abbr}"
    with conn.cursor() as cur:
        cur.execute(sql, (prefix + "%",))
        row = cur.fetchone()
        if row and row[0]:
            return row[0]

    # ===== ç‰¹ä¾‹å¤„ç†ï¼šSage å…ˆ SGï¼Œæ²¡å‘½ä¸­å†è¯• GN =====
    # é€»è¾‘ï¼šPowell è¿™ç§éæ²¹èœ¡ï¼ŒBarbour å®˜æ–¹ç”¨ GNï¼›æ²¹èœ¡æ¬¾æ‰æ›´å¤šç”¨ SG
    if color_name.strip().lower() == "sage" and color_abbr.upper() == "SG":
        alt_abbr = "GN"
        alt_prefix = f"{style_code}{alt_abbr}"
        with conn.cursor() as cur:
            cur.execute(sql, (alt_prefix + "%",))
            row = cur.fetchone()
            if row and row[0]:
                print(f"ğŸ” Sage é¢œè‰²ä½¿ç”¨ GN å¤‡é€‰ç®€å†™å‘½ä¸­: {alt_prefix} -> {row[0]}")
                return row[0]

    # ===== ä»ç„¶æ²¡æ‰¾åˆ°ï¼Œæœ€åå…œåº•ï¼šè¿”å›åŸæ¥çš„å‰ç¼€ =====
    print(f"âš ï¸ æ•°æ®åº“ä¸­æœªåŒ¹é…åˆ° product_codeï¼Œä½¿ç”¨å‰ç¼€ä»£æ›¿: {prefix}")
    return prefix



# ========== å•ä¸ªé“¾æ¥å¤„ç† ==========

def process_url(url: str, output_dir: Path):
    driver = get_driver(headless=True)

    if driver is None:
        print(f"âŒ æ— æ³•åˆ›å»ºæµè§ˆå™¨ï¼Œè·³è¿‡æ­¤é“¾æ¥ï¼š{url}")
        return
    try:
        print(f"\nğŸŒ æ­£åœ¨æŠ“å–: {url}")
        driver.get(url)
        accept_cookies(driver)
        time.sleep(3)

        html0 = driver.page_source
        soup0 = BeautifulSoup(html0, "html.parser")

        # ---- å…¬å…±ä¿¡æ¯ï¼ˆå¯¹æ‰€æœ‰é¢œè‰²é€šç”¨ï¼‰----
        style_code = extract_style_code(html0) or ""
        product_name = extract_product_name(soup0)
        product_desc = extract_description(soup0)
        base_orig_price, base_sale_price = extract_prices_pmd(soup0)  # ğŸ†•
        gender = infer_gender(product_name)

        # ---- æ‰¾åˆ°æ‰€æœ‰é¢œè‰²æŒ‰é’® ----
        color_elems = driver.find_elements(By.CSS_SELECTOR, "label.form-option.label-img")
        if not color_elems:
            print("âš ï¸ æœªæ‰¾åˆ°é¢œè‰²é€‰é¡¹ï¼ŒåªæŒ‰å•ä¸€é¢œè‰²å¤„ç†")
        variants: list[dict] = []

        if color_elems:
            color_count = len(color_elems)
            for idx in range(color_count):
                # æ¯è½®é‡æ–°è·å–å…ƒç´ ï¼Œé¿å… stale element
                color_elems = driver.find_elements(By.CSS_SELECTOR, "label.form-option.label-img")
                elem = color_elems[idx]
                color_name = elem.text.strip()
                if not color_name:
                    # æœ‰äº›æ ·å¼å¯èƒ½æ”¾åœ¨ title é‡Œ
                    color_name = (elem.get_attribute("title") or "").strip() or "No Data"

                print(f"  ğŸ¨ é¢œè‰² {idx + 1}/{color_count}: {color_name}")

                if color_name == "No Data":
                    print(f"  âš ï¸ è·³è¿‡æ— æ•ˆé¢œè‰²é€‰é¡¹ï¼ˆindex={idx + 1}ï¼‰")
                    continue

                # ç‚¹å‡»è¯¥é¢œè‰²ï¼Œè®©é¡µé¢åˆ·æ–°å½“å‰åº“å­˜
                driver.execute_script("arguments[0].click();", elem)
                time.sleep(1.5)

                html_color = driver.page_source
                soup_color = BeautifulSoup(html_color, "html.parser")

                # å½“å‰é¢œè‰²ä»·æ ¼ï¼ˆåŒæ—¶å–åŸä»· & æŠ˜åä»·ï¼‰
                orig_price, sale_price = extract_prices_pmd(soup_color)

                # å…œåº•ï¼šå¦‚æœè¿™æ¬¡æ²¡å–åˆ°ï¼Œç”¨å…¨å±€çš„
                if not sale_price:
                    sale_price = base_sale_price
                if not orig_price:
                    orig_price = base_orig_price or sale_price

                sizes = parse_sizes_from_html(html_color)
                product_size_str = build_product_size_str(sizes)

                # å†³å®šå†™å…¥ TXT çš„ä¸¤ä¸ªä»·æ ¼å­—æ®µï¼š
                # - Product Price = åŸä»·
                # - Adjusted Price = æŠ˜åä»·ï¼ˆåªæœ‰æŠ˜æ‰£æ—¶å¡«å†™ï¼‰
                adjusted_price = ""
                if sale_price and orig_price and sale_price != orig_price:
                    adjusted_price = sale_price

                info = {
                    "Brand": "Barbour",
                    "Product Name": product_name,
                    "Product Description": product_desc,
                    "Product Gender": gender,
                    "Product Color": color_name,
                    "Product Price": orig_price or sale_price or "0.00",
                    "Adjusted Price": adjusted_price,
                    "Product Material": "",
                    "Style Category": "",
                    "Feature": "",
                    "Product Size": product_size_str,
                    "Site Name": SITE_NAME,
                    "Source URL": url,
                    "_style_code": style_code,
                }

                variants.append(info)
        else:
            # æ²¡æœ‰é¢œè‰²é€‰é¡¹æ—¶ï¼ŒæŒ‰å•ä¸€é¢œè‰²å¤„ç†
            color_name = "No Data"
            sizes = parse_sizes_from_html(html0)
            product_size_str = build_product_size_str(sizes)

            orig_price, sale_price = base_orig_price, base_sale_price
            adjusted_price = ""
            if sale_price and orig_price and sale_price != orig_price:
                adjusted_price = sale_price

            info = {
                "Brand": "Barbour",
                "Product Name": product_name,
                "Product Description": product_desc,
                "Product Gender": gender,
                "Product Color": color_name,
                "Product Price": orig_price or sale_price or "0.00",
                "Adjusted Price": adjusted_price,
                "Product Material": "",
                "Style Category": "",
                "Feature": "",
                "Product Size": product_size_str,
                "Site Name": SITE_NAME,
                "Source URL": url,
                "_style_code": style_code,
            }


            variants.append(info)

              # ---- ç¬¬äºŒé˜¶æ®µï¼šæ ¹æ® style + é¢œè‰² å»æ•°æ®åº“æ‰¾â€œçœŸæ­£å•†å“ç¼–ç â€ï¼Œå†ç»Ÿä¸€å†™ TXT ----
        if not variants:
            print("âŒ æœªè§£æåˆ°ä»»ä½•é¢œè‰²å˜ä½“ï¼Œè·³è¿‡æ­¤é“¾æ¥")
            return

        conn = None
        try:
            # å…ˆå°è¯•è¿æ¥æ•°æ®åº“
            try:
                conn = psycopg2.connect(**PGSQL_CONFIG)
                print("âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œå°†è·³è¿‡ç¼–ç ç²¾ç¡®åŒ¹é…ï¼š{e}")
                conn = None

            for info in variants:
                style_code = info.get("_style_code") or ""
                color_name = info.get("Product Color") or ""
                product_code = None

                # åªæœ‰åœ¨ style_code å’Œ conn éƒ½å­˜åœ¨æ—¶æ‰å» DB é‡ŒæŸ¥
                if style_code and conn is not None:
                    product_code = find_product_code_in_db(style_code, color_name, conn)

                # æŸ¥ä¸åˆ° / DB ä¸é€š éƒ½å…œåº•ç”¨ style_code
                if not product_code:
                    product_code = style_code or "UNKNOWN"

                info["Product Code"] = product_code
                # æ¸…ç†å†…éƒ¨å­—æ®µ
                info.pop("_style_code", None)

                filename = sanitize_filename(product_code) + ".txt"
                txt_path = output_dir / filename
                format_txt(info, txt_path, brand="Barbour")
                print(f"  âœ… å†™å…¥ TXT: {txt_path.name}")
        finally:
            if conn is not None:
                conn.close()


    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {url}\n    {e}")
    finally:
        driver.quit()


# ========== å¤šçº¿ç¨‹å…¥å£ ==========

def philipmorris_fetch_info(max_workers: int = 3):
    print(f"LINKS_FILE = {LINKS_FILE}")
    print(f"TXT_DIR    = {TXT_DIR}")

    urls: list[str] = []
    with open(LINKS_FILE, "r", encoding="utf-8") as f:
        for line in f:
            url = line.strip()
            if url:
                urls.append(url)

    print(f"ğŸš€ Philip Morris æŠ“å–å¯åŠ¨ï¼Œæ€»é“¾æ¥æ•°: {len(urls)}ï¼Œå¹¶å‘çº¿ç¨‹æ•°: {max_workers}")

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(process_url, url, TXT_DIR) for url in urls]
        for _ in as_completed(futures):
            pass


# å…¼å®¹ä¹‹å‰å¯èƒ½ä½¿ç”¨çš„å‡½æ•°å
def fetch_all():
    philipmorris_fetch_info(max_workers=1)


if __name__ == "__main__":
    philipmorris_fetch_info(max_workers=1)
