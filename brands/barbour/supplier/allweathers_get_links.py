import time
from pathlib import Path

from bs4 import BeautifulSoup
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

from config import BARBOUR
from common.browser.selenium_utils import get_driver, quit_driver

# ========= ç±»ç›®é…ç½®ï¼šåœ¨è¿™é‡Œå¢åˆ ç±»ç›®é“¾æ¥ =========
CATEGORY_URLS = [
    "https://www.allweathers.co.uk/collections/barbour",
    "https://www.allweathers.co.uk/collections/barbour-quilted-jackets-1",
    "https://www.allweathers.co.uk/collections/barbour-coats-long",
    "https://www.allweathers.co.uk/collections/barbour-waxed-jackets",
    "https://www.allweathers.co.uk/collections/barbour-waterproof-jackets",
    "https://www.allweathers.co.uk/collections/barbour-gilets",
    "https://www.allweathers.co.uk/collections/barbour-liners",
    "https://www.allweathers.co.uk/collections/barbour-fleece-gilets",
    "https://www.allweathers.co.uk/collections/barbour-knitwear",
    "https://www.allweathers.co.uk/collections/barbour-sweatshirts-hoodies",
]

# è¾“å‡ºè·¯å¾„ä»ç„¶å¤ç”¨åŸæ¥çš„é…ç½®
OUTPUT_PATH = BARBOUR["LINKS_FILES"]["allweathers"]
OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)


def get_allweathers_driver():
    """
    ç»Ÿä¸€ä» common.selenium_utils è·å– driverï¼Œ
    åå­—ç”¨ 'allweathers'ï¼Œæ–¹ä¾¿ä»¥åå¤ç”¨æˆ–å•ç‹¬å…³é—­ã€‚
    """
    return get_driver(
        name="allweathers",
        headless=False,           # éœ€è¦å¯è§†åŒ–å°±å…³æ‰ headless
        window_size="1200,2000",
    )


def extract_links_from_html(html: str):
    soup = BeautifulSoup(html, "html.parser")
    links = set()
    for tag in soup.select("a.product-title.h6"):
        href = tag.get("href", "").strip()
        if not href:
            continue
        if href.startswith("http"):
            links.add(href)
        elif href.startswith("/"):
            links.add("https://www.allweathers.co.uk" + href)
    return links


def build_page_url(base_url: str, page: int) -> str:
    """æ ¹æ®æ˜¯å¦å·²æœ‰ ? æ„é€ åˆ†é¡µ URLï¼Œé¿å… ?page= æ‹¼é”™"""
    if "?" in base_url:
        return f"{base_url}&page={page}"
    else:
        return f"{base_url}?page={page}"


def allweathers_get_links():
    print("ğŸš€ å¼€å§‹æŠ“å– Allweathers å¤šç±»ç›®å•†å“é“¾æ¥")
    driver = get_allweathers_driver()
    all_links = set()

    try:
        for idx, base_url in enumerate(CATEGORY_URLS, start=1):
            print("\n============================")
            print(f"ğŸ“‚ ç±»ç›® {idx}/{len(CATEGORY_URLS)}: {base_url}")
            page = 1

            while True:
                url = build_page_url(base_url, page)
                print(f"ğŸŒ æŠ“å–ç¬¬ {page} é¡µ: {url}")
                driver.get(url)

                try:
                    WebDriverWait(driver, 10).until(
                        EC.presence_of_element_located(
                            (By.CSS_SELECTOR, "a.product-title.h6")
                        )
                    )
                except Exception:
                    print(f"âš ï¸ ç¬¬ {page} é¡µåŠ è½½è¶…æ—¶æˆ–æ— å•†å“ï¼Œç»ˆæ­¢è¯¥ç±»ç›®åˆ†é¡µ")
                    break

                html = driver.page_source
                links = extract_links_from_html(html)

                if not links:
                    print(f"âš ï¸ ç¬¬ {page} é¡µæœªæå–åˆ°é“¾æ¥ï¼Œç»ˆæ­¢è¯¥ç±»ç›®åˆ†é¡µ")
                    break

                print(f"âœ… ç¬¬ {page} é¡µæå– {len(links)} ä¸ªå•†å“é“¾æ¥")
                all_links.update(links)

                page += 1
                time.sleep(1)

    finally:
        # ä½¿ç”¨å…¬å…±å·¥å…·å…³é—­ 'allweathers' è¿™ä¸ª driver
        quit_driver("allweathers")

    # å†™å…¥æ–‡ä»¶ï¼ˆå»é‡åçš„æ€»é›†åˆï¼‰
    OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)
    OUTPUT_PATH.write_text("\n".join(sorted(all_links)), encoding="utf-8")

    print("\nğŸ¯ æŠ“å–å®Œæˆ")
    print(f"ğŸ“¦ å…±æå– {len(all_links)} æ¡å•†å“é“¾æ¥ï¼ˆå¤šç±»ç›®å»é‡åæ€»æ•°ï¼‰")
    print(f"ğŸ’¾ å·²ä¿å­˜è‡³ï¼š{OUTPUT_PATH}")


if __name__ == "__main__":
    allweathers_get_links()
