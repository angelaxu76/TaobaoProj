import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from bs4 import BeautifulSoup
from pathlib import Path
import time
from config import BARBOUR
from pathlib import Path
import re
import json
from bs4 import BeautifulSoup

# === é…ç½®è·¯å¾„ ===
from config import BARBOUR

# âœ… é¡µé¢é…ç½®ï¼šä¸Šè¡£ç±»ï¼ˆç”· + å¥³ï¼‰
TARGET_URLS = [
    ("men", "https://www.outdoorandcountry.co.uk/shop-by-brand/barbour/barbour-menswear/all-barbour-mens-clothing-footwear.sub?s=i&pt=Coats+%26+Jackets%2cGilets+%26+Waistcoats%2cKnitwear"),
    ("women", "https://www.outdoorandcountry.co.uk/shop-by-brand/barbour/barbour-womenswear/womens-barbour-clothing-footwear-accessories.sub?s=i&pt=Coats+%26+Jackets%2cGilets+%26+Waistcoats%2cKnitwear"),
    ("international", "https://www.outdoorandcountry.co.uk/shop-by-brand/barbour-international/all-barbour-international.sub")
]

BASE_DOMAIN = "https://www.outdoorandcountry.co.uk"
OUTPUT_FILE = BARBOUR["LINKS_FILES"]["outdoorandcountry"]
DEBUG_DIR = OUTPUT_FILE.parent / "debug_pages"

SCROLL_STEP = 1200
SCROLL_PAUSE = 1.5
STABLE_THRESHOLD = 10



TXT_DIR = BARBOUR["TXT_DIRS"]["outdoorandcountry"]
LINKS_FILE = BARBOUR["LINKS_FILES"]["outdoorandcountry"]


# === æå–å‡½æ•° ===
def extract_js_object(js_text: str, var_name: str):
    pattern = re.compile(rf"window\.{re.escape(var_name)}\s*=\s*(\{{.*?\}});", re.DOTALL)
    match = pattern.search(js_text)
    if match:
        try:
            return json.loads(match.group(1))
        except Exception:
            print(f"âš ï¸ å˜é‡ {var_name} è§£æå¤±è´¥")
            return {}
    return {}

def parse_outdoor_product_page(html: str, url: str) -> list:
    soup = BeautifulSoup(html, "html.parser")

    # ä¿å­˜é¡µé¢è°ƒè¯•ï¼ˆå¯é€‰ï¼‰
    with open("debug.html", "w", encoding="utf-8") as f:
        f.write(html)

    # å•†å“åç§°
    title_tag = soup.find("title")
    if not title_tag:
        return []
    product_name = title_tag.text.strip()

    # URL ä¸­æå–é¢œè‰²
    color = "Unknown"
    if "?c=" in url:
        color = url.split("?c=")[-1].strip().replace("%20", " ").capitalize()

    # æå– JS å˜é‡
    js_text = soup.text
    colours = extract_js_object(js_text, "Colours")
    sizes = extract_js_object(js_text, "Sizes")
    stock_info = extract_js_object(js_text, "stockInfo")

    if not stock_info:
        print(f"âš ï¸ é¡µé¢æ—  stockInfo: {url}")
        return []

    results = []
    for k, v in stock_info.items():
        try:
            size_id, color_id = k.split("-")
            size = sizes.get(size_id, size_id)
            color_name = colours.get(color_id, color)
            stock_status = v.get("stockLevelMessage", "").lower()
            price = v.get("priceGbp", 0)

            results.append({
                "Product Name": product_name,
                "Product Color": color_name,
                "Product Size": size,
                "Product URL": url,
                "Stock Status": stock_status,
                "Price": f"{price:.2f}"
            })
        except Exception as e:
            print(f"âŒ è§£æå•é¡¹å¤±è´¥: {k} -> {e}")

    return results


# === å†™å…¥ TXT ===
def write_txt(filepath: Path, items: list):
    if not items:
        return
    main = items[0]
    lines = [
        f"Product Name: {main['Product Name']}",
        f"Product Color: {main['Product Color']}",
        f"Product URL: {main['Product URL']}",
        f"Product Size: " + "; ".join(f"{i['Product Size']}: {i['Stock Status']}" for i in items),
        f"Product Price: {main['Price']}"
    ]
    with open(filepath, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))


# === æ‰¹é‡å¤„ç†é€»è¾‘ ===
def outdoorandcountry_fetch_info():
    TXT_DIR.mkdir(parents=True, exist_ok=True)
    urls = set()
    with open(LINKS_FILE, "r", encoding="utf-8") as f:
        for line in f:
            url = line.strip()
            if url:
                urls.add(url)

    # âœ… ä½¿ç”¨ä¸æŠ“é“¾æ¥å®Œå…¨ä¸€æ ·çš„ uc.Chrome æ–¹å¼
    options = uc.ChromeOptions()
    options.add_argument("--start-maximized")
    driver = uc.Chrome(options=options)  # âŒ ä¸åŠ  headless

    for url in sorted(urls):
        try:
            print(f"\nğŸŒ æ‰“å¼€å•†å“è¯¦æƒ…é¡µ: {url}")
            driver.get(url)
            accept_cookies(driver)
            time.sleep(3)  # âœ… ç­‰å¾… JS æ¸²æŸ“

            html = driver.page_source

            # âœ… å¯é€‰ï¼šè°ƒè¯•é¡µé¢æˆªå›¾å’Œ HTML
            with open("debug_product.html", "w", encoding="utf-8") as f:
                f.write(html)
            driver.save_screenshot("debug_product.png")

            items = parse_outdoor_product_page(html, url)
            if items:
                product_name = items[0]['Product Name']
                color = items[0]['Product Color']
                filename = f"{product_name.replace(' ', '_')}_{color}.txt"
                filepath = TXT_DIR / filename
                write_txt(filepath, items)
                print(f"âœ… å†™å…¥: {filepath.name}")
            else:
                print(f"âš ï¸ è·³è¿‡ï¼ˆæ— åº“å­˜ä¿¡æ¯ï¼‰: {url}")
        except Exception as e:
            print(f"âŒ é”™è¯¯: {url}\n    {e}")

    driver.quit()


def accept_cookies(driver, timeout=8):
    try:
        button = WebDriverWait(driver, timeout).until(
            EC.element_to_be_clickable((By.ID, "onetrust-accept-btn-handler"))
        )
        button.click()
        print("ğŸª å·²è‡ªåŠ¨ç‚¹å‡» Accept Cookies")
        time.sleep(1)
    except:
        print("âš ï¸ æœªå‡ºç° Cookie æ¥å—æŒ‰é’®ï¼Œå¯èƒ½å·²æ¥å—æˆ–è¢«è·³è¿‡")

def scroll_like_mouse_until_loaded(
    driver,
    step=SCROLL_STEP,
    pause=SCROLL_PAUSE,
    stable_threshold=STABLE_THRESHOLD,
    max_scrolls=200,            # âœ… ç¡¬ä¸Šé™ï¼Œé˜²æ­¢æ— é™å¾ªç¯
    max_seconds=120             # âœ… æ€»æ—¶é•¿ä¸Šé™ï¼ˆç§’ï¼‰
):
    """
    è¿ç»­æ»šåŠ¨ç›´åˆ°ï¼š
      1) å¯è§é“¾æ¥æ•°åœ¨ stable_threshold æ¬¡æ£€æŸ¥ä¸­ä¸å†å¢åŠ ï¼›æˆ–
      2) é¡µé¢æ»šåŠ¨é«˜åº¦åœ¨ stable_threshold æ¬¡æ£€æŸ¥ä¸­ä¸å†å¢åŠ ï¼›æˆ–
      3) å·²åˆ°é¡µé¢åº•éƒ¨å¹¶ä¸”ç­‰å¾…è‹¥å¹²æ¬¡ä»æ— æ–°å¢ï¼›æˆ–
      4) è§¦å‘ç¡¬ä¸Šé™ï¼ˆmax_scrolls / max_secondsï¼‰
    """
    print("âš¡ å¼€å§‹æ»šåŠ¨ç›´åˆ°å•†å“å…¨éƒ¨åŠ è½½...")
    start_ts = time.time()

    last_link_count = 0
    last_scroll_height = 0
    stable_count = 0
    total_scrolls = 0

    while True:
        # 1) å…ˆæ¨¡æ‹Ÿé¼ æ ‡æ»šåŠ¨ä¸€æ­¥
        driver.execute_script("window.scrollBy(0, arguments[0]);", step)
        time.sleep(pause)

        # 2) è§£æå½“å‰é¡µé¢çš„â€œå”¯ä¸€å•†å“é“¾æ¥æ•°â€ï¼ˆæ›´ç¨³å®šï¼‰
        html = driver.page_source
        current_links = collect_links_from_html(html)
        link_count = len(current_links)

        # 3) è·å–æ»šåŠ¨é«˜åº¦ï¼ˆåˆ¤æ–­æ˜¯å¦è¿˜åœ¨å¢é•¿ï¼‰
        scroll_height = driver.execute_script("return document.body.scrollHeight;")
        viewport_bottom = driver.execute_script("return window.scrollY + window.innerHeight;")
        at_bottom = viewport_bottom >= scroll_height - 5  # å…è®¸å¾®å°è¯¯å·®

        print(f"ğŸŒ€ æ»šåŠ¨ {total_scrolls+1} æ¬¡ | é“¾æ¥: {link_count} | é«˜åº¦: {scroll_height} | at_bottom={at_bottom}")

        # 4) åˆ¤æ–­æ˜¯å¦â€œç¨³å®šä¸å˜â€
        no_new_links = (link_count == last_link_count)
        no_new_height = (scroll_height == last_scroll_height)

        if no_new_links and (no_new_height or at_bottom):
            stable_count += 1
        else:
            stable_count = 0
            last_link_count = link_count
            last_scroll_height = scroll_height

        # 5) æ»¡è¶³ä»»ä½•ä¸€ç§åœæ­¢æ¡ä»¶å°±é€€å‡º
        if stable_count >= stable_threshold:
            print(f"âœ… å·²ç¨³å®š {stable_count} æ¬¡ï¼Œåœæ­¢æ»šåŠ¨ï¼ˆé“¾æ¥ {link_count}ï¼‰")
            break

        if total_scrolls >= max_scrolls:
            print(f"â¹ï¸ è¾¾åˆ°æœ€å¤§æ»šåŠ¨æ¬¡æ•° {max_scrolls}ï¼Œåœæ­¢")
            break

        if time.time() - start_ts >= max_seconds:
            print(f"â¹ï¸ è¾¾åˆ°æœ€é•¿ç­‰å¾… {max_seconds}sï¼Œåœæ­¢")
            break

        total_scrolls += 1

    # æœ€åï¼Œå†å°è¯•ä¸€æ¬¡æ»šåˆ°åº•ï¼ˆæœ‰äº›ç«™ç‚¹éœ€è¦ï¼‰
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
    time.sleep(1.0)


def collect_links_from_html(html):
    soup = BeautifulSoup(html, "html.parser")
    links = set()
    for a in soup.select("a.image"):
        href = a.get("href", "").strip()
        if href:
            full_url = href if href.startswith("http") else BASE_DOMAIN + href
            links.add(full_url)
    return links

def outdoorandcountry_fetch_and_save_links():
    DEBUG_DIR.mkdir(parents=True, exist_ok=True)
    OUTPUT_FILE.parent.mkdir(parents=True, exist_ok=True)

    options = uc.ChromeOptions()
    options.add_argument("--start-maximized")
    driver = uc.Chrome(options=options, headless=False)

    all_links = set()

    for label, url in TARGET_URLS:
        print(f"\nğŸš€ æ‰“å¼€é¡µé¢ [{label}]: {url}")
        driver.get(url)
        time.sleep(3)

        accept_cookies(driver)
        scroll_like_mouse_until_loaded(driver)

        print(f"ğŸŸ¡ è¯·æ£€æŸ¥é¡µé¢ [{label}] æ˜¯å¦åŠ è½½å®Œæ•´ï¼Œå¯æ‰‹åŠ¨å†æ»šåŠ¨å‡ æ¬¡")
        input(f"â¸ï¸ ç¡®è®¤ [{label}] é¡µé¢åŠ è½½å®Œæˆåï¼ŒæŒ‰å›è½¦ç»§ç»­ >>> ")

        html = driver.page_source
        links = collect_links_from_html(html)
        all_links.update(links)

        debug_file = DEBUG_DIR / f"debug_{label}_auto_scroll.html"
        with debug_file.open("w", encoding="utf-8") as f:
            f.write(html)
        print(f"âœ… [{label}] é“¾æ¥æå–: {len(links)} æ¡ï¼Œé¡µé¢å¿«ç…§ä¿å­˜: {debug_file}")

    # ä¿å­˜æ€»é“¾æ¥
    with OUTPUT_FILE.open("w", encoding="utf-8") as f:
        for link in sorted(all_links):
            f.write(link + "\n")

    print(f"\nğŸ‰ å…±æå–å•†å“é“¾æ¥: {len(all_links)} æ¡")
    print(f"ğŸ“„ é“¾æ¥å†™å…¥: {OUTPUT_FILE}")
    driver.quit()
