from selenium.webdriver.common.by import By
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from bs4 import BeautifulSoup
from common.core.driver_auto import build_uc_driver
from pathlib import Path
import time
from config import BARBOUR
from pathlib import Path
import re
import json
from bs4 import BeautifulSoup

# === é…ç½®è·¯å¾„ ===
from config import BARBOUR

# âœ… é¡µé¢é…ç½®ï¼šä¸Šè¡£ç±»ï¼ˆç”· + å¥³ï¼‰
TARGET_URLS = [
    ("men", "https://www.outdoorandcountry.co.uk/shop-by-brand/barbour/barbour-menswear/all-barbour-mens-clothing-footwear.sub?s=i&pt=Coats+%26+Jackets%2cGilets+%26+Waistcoats%2cKnitwear"),
    ("women", "https://www.outdoorandcountry.co.uk/shop-by-brand/barbour/barbour-womenswear/womens-barbour-clothing-footwear-accessories.sub?s=i&pt=Coats+%26+Jackets%2cGilets+%26+Waistcoats%2cKnitwear"),
    ("international", "https://www.outdoorandcountry.co.uk/shop-by-brand/barbour-international/all-barbour-international.sub")
]

BASE_DOMAIN = "https://www.outdoorandcountry.co.uk"
OUTPUT_FILE = BARBOUR["LINKS_FILES"]["outdoorandcountry"]
DEBUG_DIR = OUTPUT_FILE.parent / "debug_pages"

SCROLL_STEP = 1200
SCROLL_PAUSE = 1.5
STABLE_THRESHOLD = 10



TXT_DIR = BARBOUR["TXT_DIRS"]["outdoorandcountry"]
LINKS_FILE = BARBOUR["LINKS_FILES"]["outdoorandcountry"]


# -*- coding: utf-8 -*-
import os, re, shutil, subprocess, time
import undetected_chromedriver as uc
from selenium.common.exceptions import SessionNotCreatedException, WebDriverException

def _detect_chrome_major_on_windows():
    # 1) æ³¨å†Œè¡¨è¯»å–
    try:
        import winreg
        for root in (winreg.HKEY_CURRENT_USER, winreg.HKEY_LOCAL_MACHINE):
            try:
                key = winreg.OpenKey(root, r"SOFTWARE\Google\Chrome\BLBeacon")
                version, _ = winreg.QueryValueEx(key, "version")  # e.g. "141.0.7390.125"
                m = re.match(r"(\d+)\.", version)
                if m:
                    return int(m.group(1))
            except OSError:
                pass
    except Exception:
        pass

    # 2) é€šè¿‡ chrome.exe --version
    candidates = [
        r"C:\Program Files\Google\Chrome\Application\chrome.exe",
        r"C:\Program Files (x86)\Google\Chrome\Application\chrome.exe",
    ]
    for exe in candidates:
        if os.path.exists(exe):
            try:
                out = subprocess.check_output([exe, "--version"], stderr=subprocess.STDOUT, timeout=5)
                out = out.decode("utf-8", "ignore")
                # e.g. "Google Chrome 141.0.7390.125"
                m = re.search(r"\b(\d+)\.", out)
                if m:
                    return int(m.group(1))
            except Exception:
                pass

    return None


def _clear_uc_cache():
    # é€šå¸¸ç¼“å­˜ç›®å½•
    candidates = [
        os.path.join(os.path.expanduser("~"), "AppData", "Roaming", "undetected_chromedriver"),
        os.path.join(os.path.expanduser("~"), ".undetected_chromedriver"),
    ]
    for p in candidates:
        shutil.rmtree(p, ignore_errors=True)


def build_uc_driver(headless=False, extra_options=None, retries=2, verbose=True):
    """
    è‡ªåŠ¨é€‚é…æœ¬æœº Chrome ä¸»ç‰ˆæœ¬ï¼Œæ„å»º undetected_chromedriverã€‚
    - headless: æ˜¯å¦æ— å¤´
    - extra_options: é¢å¤–çš„ ChromeOptions å‚æ•°ï¼ˆlist[str]ï¼‰
    - retries: å¤±è´¥åï¼ˆæ¸…ç¼“å­˜ï¼‰é‡è¯•æ¬¡æ•°
    """
    options = uc.ChromeOptions()
    options.add_argument("--start-maximized")
    if extra_options:
        for arg in extra_options:
            options.add_argument(arg)

    major = _detect_chrome_major_on_windows()
    if verbose:
        print(f"ğŸ” Detected Chrome major version: {major if major else 'UNKNOWN'}")

    # ä¼ å…¥ version_main å¯ç¡®ä¿ä¸‹è½½åŒ¹é…çš„é©±åŠ¨
    kwargs = dict(options=options, headless=headless)
    if major:
        kwargs["version_main"] = major

    last_err = None
    for attempt in range(1, retries + 1):
        try:
            if verbose:
                print(f"ğŸš— Creating uc.Chrome (attempt {attempt}/{retries}) with {kwargs} ...")
            driver = build_uc_driver(headless=False, extra_options=None, retries=2, verbose=True)
            if verbose:
                print("âœ… uc.Chrome started successfully.")
            return driver
        except SessionNotCreatedException as e:
            last_err = e
            if verbose:
                print(f"âš ï¸ SessionNotCreatedException: {e}\nğŸ§¹ Clearing uc cache and retrying ...")
            _clear_uc_cache()
            time.sleep(1.5)
        except WebDriverException as e:
            last_err = e
            if "only supports Chrome version" in str(e):
                if verbose:
                    print(f"âš ï¸ Driver version mismatch: {e}\nğŸ§¹ Clearing uc cache and retrying ...")
                _clear_uc_cache()
                time.sleep(1.5)
            else:
                raise

    # è‹¥åˆ°è¿™é‡Œä»å¤±è´¥ï¼ŒæŠ›å‡ºæœ€åä¸€æ¬¡çš„é”™è¯¯
    raise last_err if last_err else RuntimeError("Failed to start uc.Chrome with auto version match.")

# === æå–å‡½æ•° ===
def extract_js_object(js_text: str, var_name: str):
    pattern = re.compile(rf"window\.{re.escape(var_name)}\s*=\s*(\{{.*?\}});", re.DOTALL)
    match = pattern.search(js_text)
    if match:
        try:
            return json.loads(match.group(1))
        except Exception:
            print(f"âš ï¸ å˜é‡ {var_name} è§£æå¤±è´¥")
            return {}
    return {}

def parse_outdoor_product_page(html: str, url: str) -> list:
    soup = BeautifulSoup(html, "html.parser")

    # ä¿å­˜é¡µé¢è°ƒè¯•ï¼ˆå¯é€‰ï¼‰
    with open("debug.html", "w", encoding="utf-8") as f:
        f.write(html)

    # å•†å“åç§°
    title_tag = soup.find("title")
    if not title_tag:
        return []
    product_name = title_tag.text.strip()

    # URL ä¸­æå–é¢œè‰²
    color = "Unknown"
    if "?c=" in url:
        color = url.split("?c=")[-1].strip().replace("%20", " ").capitalize()

    # æå– JS å˜é‡
    js_text = soup.text
    colours = extract_js_object(js_text, "Colours")
    sizes = extract_js_object(js_text, "Sizes")
    stock_info = extract_js_object(js_text, "stockInfo")

    if not stock_info:
        print(f"âš ï¸ é¡µé¢æ—  stockInfo: {url}")
        return []

    results = []
    for k, v in stock_info.items():
        try:
            size_id, color_id = k.split("-")
            size = sizes.get(size_id, size_id)
            color_name = colours.get(color_id, color)
            stock_status = v.get("stockLevelMessage", "").lower()
            price = v.get("priceGbp", 0)

            results.append({
                "Product Name": product_name,
                "Product Color": color_name,
                "Product Size": size,
                "Product URL": url,
                "Stock Status": stock_status,
                "Price": f"{price:.2f}"
            })
        except Exception as e:
            print(f"âŒ è§£æå•é¡¹å¤±è´¥: {k} -> {e}")

    return results


# === å†™å…¥ TXT ===
def write_txt(filepath: Path, items: list):
    if not items:
        return
    main = items[0]
    lines = [
        f"Product Name: {main['Product Name']}",
        f"Product Color: {main['Product Color']}",
        f"Product URL: {main['Product URL']}",
        f"Product Size: " + "; ".join(f"{i['Product Size']}: {i['Stock Status']}" for i in items),
        f"Product Price: {main['Price']}"
    ]
    with open(filepath, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))


# === æ‰¹é‡å¤„ç†é€»è¾‘ ===
def outdoorandcountry_fetch_info():
    TXT_DIR.mkdir(parents=True, exist_ok=True)
    urls = set()
    with open(LINKS_FILE, "r", encoding="utf-8") as f:
        for line in f:
            url = line.strip()
            if url:
                urls.add(url)

    # âœ… ä½¿ç”¨ä¸æŠ“é“¾æ¥å®Œå…¨ä¸€æ ·çš„ uc.Chrome æ–¹å¼
    from common.core.driver_auto import build_uc_driver

    driver = build_uc_driver(headless=False, extra_options=None, retries=2, verbose=True)


    for url in sorted(urls):
        try:
            print(f"\nğŸŒ æ‰“å¼€å•†å“è¯¦æƒ…é¡µ: {url}")
            driver.get(url)
            accept_cookies(driver)
            time.sleep(3)  # âœ… ç­‰å¾… JS æ¸²æŸ“

            html = driver.page_source

            # âœ… å¯é€‰ï¼šè°ƒè¯•é¡µé¢æˆªå›¾å’Œ HTML
            with open("debug_product.html", "w", encoding="utf-8") as f:
                f.write(html)
            driver.save_screenshot("debug_product.png")

            items = parse_outdoor_product_page(html, url)
            if items:
                product_name = items[0]['Product Name']
                color = items[0]['Product Color']
                filename = f"{product_name.replace(' ', '_')}_{color}.txt"
                filepath = TXT_DIR / filename
                write_txt(filepath, items)
                print(f"âœ… å†™å…¥: {filepath.name}")
            else:
                print(f"âš ï¸ è·³è¿‡ï¼ˆæ— åº“å­˜ä¿¡æ¯ï¼‰: {url}")
        except Exception as e:
            print(f"âŒ é”™è¯¯: {url}\n    {e}")

    driver.quit()


def accept_cookies(driver, timeout=8):
    try:
        button = WebDriverWait(driver, timeout).until(
            EC.element_to_be_clickable((By.ID, "onetrust-accept-btn-handler"))
        )
        button.click()
        print("ğŸª å·²è‡ªåŠ¨ç‚¹å‡» Accept Cookies")
        time.sleep(1)
    except:
        print("âš ï¸ æœªå‡ºç° Cookie æ¥å—æŒ‰é’®ï¼Œå¯èƒ½å·²æ¥å—æˆ–è¢«è·³è¿‡")

def scroll_like_mouse_until_loaded(
    driver,
    step=SCROLL_STEP,
    pause=SCROLL_PAUSE,
    stable_threshold=STABLE_THRESHOLD,
    max_scrolls=200,            # âœ… ç¡¬ä¸Šé™ï¼Œé˜²æ­¢æ— é™å¾ªç¯
    max_seconds=120             # âœ… æ€»æ—¶é•¿ä¸Šé™ï¼ˆç§’ï¼‰
):
    """
    è¿ç»­æ»šåŠ¨ç›´åˆ°ï¼š
      1) å¯è§é“¾æ¥æ•°åœ¨ stable_threshold æ¬¡æ£€æŸ¥ä¸­ä¸å†å¢åŠ ï¼›æˆ–
      2) é¡µé¢æ»šåŠ¨é«˜åº¦åœ¨ stable_threshold æ¬¡æ£€æŸ¥ä¸­ä¸å†å¢åŠ ï¼›æˆ–
      3) å·²åˆ°é¡µé¢åº•éƒ¨å¹¶ä¸”ç­‰å¾…è‹¥å¹²æ¬¡ä»æ— æ–°å¢ï¼›æˆ–
      4) è§¦å‘ç¡¬ä¸Šé™ï¼ˆmax_scrolls / max_secondsï¼‰
    """
    print("âš¡ å¼€å§‹æ»šåŠ¨ç›´åˆ°å•†å“å…¨éƒ¨åŠ è½½...")
    start_ts = time.time()

    last_link_count = 0
    last_scroll_height = 0
    stable_count = 0
    total_scrolls = 0

    while True:
        # 1) å…ˆæ¨¡æ‹Ÿé¼ æ ‡æ»šåŠ¨ä¸€æ­¥
        driver.execute_script("window.scrollBy(0, arguments[0]);", step)
        time.sleep(pause)

        # 2) è§£æå½“å‰é¡µé¢çš„â€œå”¯ä¸€å•†å“é“¾æ¥æ•°â€ï¼ˆæ›´ç¨³å®šï¼‰
        html = driver.page_source
        current_links = collect_links_from_html(html)
        link_count = len(current_links)

        # 3) è·å–æ»šåŠ¨é«˜åº¦ï¼ˆåˆ¤æ–­æ˜¯å¦è¿˜åœ¨å¢é•¿ï¼‰
        scroll_height = driver.execute_script("return document.body.scrollHeight;")
        viewport_bottom = driver.execute_script("return window.scrollY + window.innerHeight;")
        at_bottom = viewport_bottom >= scroll_height - 5  # å…è®¸å¾®å°è¯¯å·®

        print(f"ğŸŒ€ æ»šåŠ¨ {total_scrolls+1} æ¬¡ | é“¾æ¥: {link_count} | é«˜åº¦: {scroll_height} | at_bottom={at_bottom}")

        # 4) åˆ¤æ–­æ˜¯å¦â€œç¨³å®šä¸å˜â€
        no_new_links = (link_count == last_link_count)
        no_new_height = (scroll_height == last_scroll_height)

        if no_new_links and (no_new_height or at_bottom):
            stable_count += 1
        else:
            stable_count = 0
            last_link_count = link_count
            last_scroll_height = scroll_height

        # 5) æ»¡è¶³ä»»ä½•ä¸€ç§åœæ­¢æ¡ä»¶å°±é€€å‡º
        if stable_count >= stable_threshold:
            print(f"âœ… å·²ç¨³å®š {stable_count} æ¬¡ï¼Œåœæ­¢æ»šåŠ¨ï¼ˆé“¾æ¥ {link_count}ï¼‰")
            break

        if total_scrolls >= max_scrolls:
            print(f"â¹ï¸ è¾¾åˆ°æœ€å¤§æ»šåŠ¨æ¬¡æ•° {max_scrolls}ï¼Œåœæ­¢")
            break

        if time.time() - start_ts >= max_seconds:
            print(f"â¹ï¸ è¾¾åˆ°æœ€é•¿ç­‰å¾… {max_seconds}sï¼Œåœæ­¢")
            break

        total_scrolls += 1

    # æœ€åï¼Œå†å°è¯•ä¸€æ¬¡æ»šåˆ°åº•ï¼ˆæœ‰äº›ç«™ç‚¹éœ€è¦ï¼‰
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
    time.sleep(1.0)


def collect_links_from_html(html):
    soup = BeautifulSoup(html, "html.parser")
    links = set()
    for a in soup.select("a.image"):
        href = a.get("href", "").strip()
        if href:
            full_url = href if href.startswith("http") else BASE_DOMAIN + href
            links.add(full_url)
    return links

def outdoorandcountry_fetch_and_save_links():
    DEBUG_DIR.mkdir(parents=True, exist_ok=True)
    OUTPUT_FILE.parent.mkdir(parents=True, exist_ok=True)

    from common.core.driver_auto import build_uc_driver
    driver = build_uc_driver(headless=False, extra_options=None, retries=2, verbose=True)

    all_links = set()

    for label, url in TARGET_URLS:
        print(f"\nğŸš€ æ‰“å¼€é¡µé¢ [{label}]: {url}")
        driver.get(url)
        time.sleep(3)

        accept_cookies(driver)
        scroll_like_mouse_until_loaded(driver)

        print(f"ğŸŸ¡ è¯·æ£€æŸ¥é¡µé¢ [{label}] æ˜¯å¦åŠ è½½å®Œæ•´ï¼Œå¯æ‰‹åŠ¨å†æ»šåŠ¨å‡ æ¬¡")
        input(f"â¸ï¸ ç¡®è®¤ [{label}] é¡µé¢åŠ è½½å®Œæˆåï¼ŒæŒ‰å›è½¦ç»§ç»­ >>> ")

        html = driver.page_source
        links = collect_links_from_html(html)
        all_links.update(links)

        debug_file = DEBUG_DIR / f"debug_{label}_auto_scroll.html"
        with debug_file.open("w", encoding="utf-8") as f:
            f.write(html)
        print(f"âœ… [{label}] é“¾æ¥æå–: {len(links)} æ¡ï¼Œé¡µé¢å¿«ç…§ä¿å­˜: {debug_file}")

    # ä¿å­˜æ€»é“¾æ¥
    with OUTPUT_FILE.open("w", encoding="utf-8") as f:
        for link in sorted(all_links):
            f.write(link + "\n")

    print(f"\nğŸ‰ å…±æå–å•†å“é“¾æ¥: {len(all_links)} æ¡")
    print(f"ğŸ“„ é“¾æ¥å†™å…¥: {OUTPUT_FILE}")
    driver.quit()
