# barbour/supplier/allweathers_fetch_info.py
# -*- coding: utf-8 -*-

import re
import time
import tempfile
from concurrent.futures import ThreadPoolExecutor, as_completed

import demjson3
from bs4 import BeautifulSoup
from selenium import webdriver

from config import BARBOUR, SETTINGS
from brands.barbour.core.site_utils import assert_site_or_raise as canon
from common_taobao.core.selenium_utils import get_driver as shared_get_driver, quit_driver

# âœ… ç»Ÿä¸€å†™å…¥ï¼šä½¿ç”¨ä½ çš„ txt_writerï¼Œä¿è¯ä¸å…¶å®ƒç«™ç‚¹åŒæ¨¡æ¿
from common_taobao.ingest.txt_writer import format_txt  # ä¸é¡¹ç›®å½“å‰ç”¨æ³•ä¿æŒä¸€è‡´

# å¯é€‰çš„ selenium_stealthï¼ˆæ— åˆ™è·³è¿‡ï¼‰
try:
    from selenium_stealth import stealth
except ImportError:
    def stealth(*args, **kwargs):
        return

# â€”â€” æ–°å¢ï¼šæ€§åˆ«ä¿®æ­£ï¼ˆBarbour ç¼–ç å‰ç¼€ä¼˜å…ˆï¼‰â€”â€”
try:
    from common_taobao.core.size_normalizer import infer_gender_for_barbour
except Exception:
    infer_gender_for_barbour = None  # è‹¥æœªæä¾›å…±äº«æ¨¡å—ï¼Œä¸‹é¢ä¼šç”¨æœ¬åœ°å…œåº•

# -------- å…¨å±€é…ç½® --------
CANON_SITE = canon("allweathers")  # è¿™é‡Œæ˜¯æ³¨é‡Šï¼Œä¸è¦å†™æˆï¼ˆ= "allweathers"ï¼‰
LINK_FILE = BARBOUR["LINKS_FILES"]["allweathers"]
TXT_DIR = BARBOUR["TXT_DIRS"]["allweathers"]
TXT_DIR.mkdir(parents=True, exist_ok=True)

MAX_WORKERS = 6

DEFAULT_STOCK_COUNT = SETTINGS.get("DEFAULT_STOCK_COUNT", 3)
# ============ æŠ½å–è¾…åŠ©å‡½æ•°ï¼ˆä¸æˆ·å¤–ç«™å®é™…é¡µé¢é€‚é…ï¼‰ ============

def _clean_text(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "").strip())


def _infer_gender_from_title(title_or_name: str) -> str:
    t = (title_or_name or "").lower()
    if re.search(r"\b(women|woman|women's|ladies)\b", t):
        return "å¥³æ¬¾"
    if re.search(r"\b(men|men's|man)\b", t):
        return "ç”·æ¬¾"
    if re.search(r"\b(kids?|boys?|girls?)\b", t):
        return "ç«¥æ¬¾"
    return "æœªçŸ¥"


def _extract_name_and_color(soup: BeautifulSoup) -> tuple[str, str]:
    # ä¼˜å…ˆ og:titleï¼šå½¢å¦‚ "Barbour Acorn Women's Waxed Jacket | Olive"
    og = soup.find("meta", {"property": "og:title"})
    if og and og.get("content"):
        txt = og["content"].strip()
        if "|" in txt:
            name, color = map(str.strip, txt.split("|", 1))
            return name, color
        return txt, "Unknown"

    # å…¶æ¬¡ document.title
    if soup.title and soup.title.string:
        t = soup.title.string.strip()
        t = t.split("|", 1)[0].strip()
        if "â€“" in t:
            name, color = map(str.strip, t.split("â€“", 1))
            return name, color
        return t, "Unknown"

    return "Unknown", "Unknown"


def _extract_description(soup: BeautifulSoup) -> str:
    # 1) twitter:description
    m = soup.find("meta", attrs={"name": "twitter:description"})
    if m and m.get("content"):
        desc = _clean_text(m["content"])
        # å»æ‰ â€œKey Features â€¦â€ ç­‰å°¾æ³¨
        desc = re.split(r"(Key\s*Features|Materials\s*&\s*Technical)", desc, flags=re.I)[0].strip(" -â€“|,")
        return desc

    # 2) og:description
    m = soup.find("meta", attrs={"property": "og:description"})
    if m and m.get("content"):
        return _clean_text(m["content"])

    # 3) JSON-LD ProductGroup.description
    for s in soup.find_all("script", {"type": "application/ld+json"}):
        try:
            j = demjson3.decode(s.string)
        except Exception:
            continue
        if isinstance(j, dict) and j.get("@type") in ("ProductGroup", "Product"):
            desc = _clean_text(j.get("description") or "")
            if desc:
                desc = re.split(r"(Key\s*Features|Materials\s*&\s*Technical)", desc, flags=re.I)[0].strip(" -â€“|,")
                return desc
    return "No Data"


def _extract_features(soup: BeautifulSoup) -> str:
    # å¯»æ‰¾ â€œKey Features & Benefitsâ€ æ ‡é¢˜åçš„åˆ—è¡¨
    h = soup.find(["h2", "h3"], string=re.compile(r"Key\s*Features", re.I))
    if h:
        ul = h.find_next("ul")
        if ul:
            items = []
            for li in ul.find_all("li"):
                txt = _clean_text(li.get_text(" ", strip=True))
                if txt:
                    items.append(txt)
            if items:
                return " | ".join(items)

    # é€€å› JSON-LD description é‡Œçš„ â€œKey Features â€¦ï¼ˆåˆ° Materials ä¹‹å‰ï¼‰â€
    for s in soup.find_all("script", {"type": "application/ld+json"}):
        try:
            j = demjson3.decode(s.string)
        except Exception:
            continue
        if isinstance(j, dict) and j.get("@type") in ("ProductGroup", "Product"):
            desc = j.get("description") or ""
            if "Key" in desc:
                m = re.search(
                    r"Key\s*Features.*?:\s*(.+?)\s*(Materials\s*&\s*Technical|Frequently|$)",
                    desc, flags=re.I | re.S
                )
                if m:
                    block = m.group(1)
                    parts = [_clean_text(p) for p in re.split(r"[\r\n]+|â€¢|- ", block)]
                    parts = [p for p in parts if p]
                    if parts:
                        return " | ".join(parts)
    return "No Data"


def _extract_material_outer(soup: BeautifulSoup) -> str:
    # é¡µé¢ H2 â€œMaterials & Technical Specificationsâ€ åˆ—è¡¨ä¸­çš„ Outer
    h = soup.find(["h2", "h3"], string=re.compile(r"Materials\s*&\s*Technical", re.I))
    if h:
        ul = h.find_next("ul")
        if ul:
            for li in ul.find_all("li"):
                txt = _clean_text(li.get_text(" ", strip=True))
                m = re.match(r"Outer:\s*(.+)", txt, flags=re.I)
                if m:
                    return m.group(1)

    # é€€å› JSON-LD description
    for s in soup.find_all("script", {"type": "application/ld+json"}):
        try:
            j = demjson3.decode(s.string)
        except Exception:
            continue
        if isinstance(j, dict) and j.get("@type") in ("ProductGroup", "Product"):
            desc = j.get("description") or ""
            m = re.search(r"Outer:\s*(.+)", desc, flags=re.I)
            if m:
                outer_line = _clean_text(m.group(1))
                outer_line = re.split(r"[\r\n;]+", outer_line)[0].strip()
                return outer_line
    return "No Data"


def _extract_header_price(soup: BeautifulSoup) -> float | None:
    # Shopify å¸¸è§çš„ meta ä»·æ ¼
    m = soup.find("meta", {"property": "product:price:amount"})
    if m and m.get("content"):
        try:
            return float(m["content"])
        except Exception:
            pass
    return None

# â€”â€” æ–°å¢ï¼šä»ä¸»å•†å“åŒºæˆå¯¹æŠ½å–ï¼ˆåŸä»·/ç°ä»·ï¼‰ â€”â€” 
def _extract_price_pair_from_dom(soup: BeautifulSoup):
    """
    è¿”å› (original_price, current_price)ã€‚
    ä»…ä»ä¸»å•†å“å— <price-list class="price-list--product"> é‡ŒæŠ“ï¼š
      <sale-price>Â£ç°ä»·</sale-price>
      <compare-at-price>Â£åŸä»·</compare-at-price>
    è‹¥å–ä¸åˆ°ï¼Œè¿”å› (None, None) äº¤ç»™ä¸Šå±‚ç”¨ _extract_header_price å…œåº•ã€‚
    """
    block = soup.find("price-list", class_=re.compile(r"\bprice-list--product\b"))
    if not block:
        return (None, None)

    def _to_float(x: str):
        try:
            return float(re.search(r"([0-9]+(?:\.[0-9]+)?)", x.replace(",", "")).group(1))
        except Exception:
            return None

    sale_el = block.find("sale-price")
    comp_el = block.find("compare-at-price")
    sale = _to_float(sale_el.get_text(" ", strip=True)) if sale_el else None
    comp = _to_float(comp_el.get_text(" ", strip=True)) if comp_el else None

    if sale and comp:
        return (comp, sale)           # (åŸä»·, ç°ä»·)
    if sale and not comp:
        return (sale, sale)           # æ— åŸä»·èŠ‚ç‚¹ â†’ è§†ä¸ºæ— æŠ˜æ‰£
    if comp and not sale:
        return (comp, comp)           # æå°‘æ•°ä¸»é¢˜å†™å â†’ å…œåº•
    return (None, None)

# ============ è§£æè¯¦æƒ…é¡µä¸ºç»Ÿä¸€ infoï¼ˆçˆ¬å–é€»è¾‘ä¿æŒä¸å˜ï¼‰ ============

def parse_detail_page(html: str, url: str) -> dict:
    soup = BeautifulSoup(html, "html.parser")

    # åç§° & é¢œè‰²
    name, color = _extract_name_and_color(soup)

    # JSON-LDï¼ˆShopify ProductGroupï¼‰
    script = soup.find("script", {"type": "application/ld+json"})
    if not script:
        raise ValueError("æœªæ‰¾åˆ° JSON-LD æ•°æ®æ®µ")

    data = demjson3.decode(script.string)
    variants = data.get("hasVariant", []) if isinstance(data, dict) else []
    if not variants:
        raise ValueError("âŒ æœªæ‰¾åˆ°å°ºç å˜ä½“")

    # å•†å“ç¼–ç ï¼ˆè‰²ç å«åœ¨ç¼–ç æœ«å°¾ï¼‰ï¼šå¦‚ LWX0752OL51-16 â†’ LWX0752OL51
    first_sku = (variants[0].get("sku") or "")
    base_sku = first_sku.split("-")[0] if first_sku else "Unknown"

    # å°ºç /ä»·æ ¼/åº“å­˜
    size_detail = {}
    for item in variants:
        sku = item.get("sku", "")
        offer = item.get("offers") or {}
        try:
            price = float(offer.get("price", 0.0))
        except Exception:
            price = 0.0
        availability = (offer.get("availability") or "").lower()
        can_order = "instock" in availability
        # UK å°ºç åœ¨ sku å°¾éƒ¨ï¼šLWX0752OL51-16 â†’ UK 16
        size_tail = sku.split("-")[-1] if "-" in sku else "Unknown"
        size = f"UK {re.sub(r'\\s+', ' ', size_tail)}"
        size_detail[size] = {
            "stock_count": DEFAULT_STOCK_COUNT if can_order else 0,  # ç»Ÿä¸€ä¸Šæ¶é‡ç­–ç•¥
            "ean": "0000000000000",                # å ä½ EAN
        }

    gender = _infer_gender_from_title(name)
    description = _extract_description(soup)
    features = _extract_features(soup)
    material_outer = _extract_material_outer(soup)
    # ä»·æ ¼ï¼šä¼˜å…ˆ DOM æˆå¯¹ä»·ï¼›ç¼ºå¤±åˆ™å›é€€ headerï¼ˆç°ä»·ï¼‰
    price_header = _extract_header_price(soup)  # å¸¸ç­‰äºç°ä»·
    orig, curr = _extract_price_pair_from_dom(soup)
    original_price = orig or price_header
    current_price = curr or price_header



    info = {
        "Product Code": base_sku,
        "Product Name": name,
        "Product Description": description,
        "Product Gender": gender,
        "Product Color": color,
        "Product Price": original_price,   # âœ… åŸä»·
        "Adjusted Price": current_price,   # âœ… ç°ä»·/æŠ˜åä»·
        "Product Material": material_outer,
        # "Style Category": ç•™ç©ºè®©å†™å…¥å™¨è‡ªåŠ¨ inferï¼ˆå¦‚ä½ å·²å‡çº§åˆ†ç±»å™¨ï¼‰
        "Feature": features,
        "SizeDetail": size_detail,       # æ¯ç åº“å­˜/å ä½ EANï¼ˆå†™å…¥å‰å†è½¬ä¸¤è¡Œï¼‰
        "Source URL": url,
        "Site Name": CANON_SITE,
    }
    return info


# ============ åå¤„ç†ï¼ˆä¸æ”¹çˆ¬å–ï¼Œåªæ•´ç†å†™å…¥å­—æ®µï¼‰ ============

WOMEN_ORDER = ["4","6","8","10","12","14","16","18","20"]
MEN_ALPHA_ORDER = ["2XS","XS","S","M","L","XL","2XL","3XL"]
MEN_NUM_ORDER = [str(n) for n in range(30, 52, 2)]  # 30..50ï¼ˆæŒ‰ä½ è¦æ±‚ï¼šä¸åŒ…å« 52ï¼‰

ALPHA_MAP = {
    "XXXS": "2XS", "2XS": "2XS",
    "XXS": "XS", "XS": "XS",
    "S": "S", "SMALL": "S",
    "M": "M", "MEDIUM": "M",
    "L": "L", "LARGE": "L",
    "XL": "XL", "X-LARGE": "XL",
    "XXL": "2XL", "2XL": "2XL",
    "XXXL": "3XL", "3XL": "3XL",
}

def _choose_full_order_for_gender(gender: str, present: set[str]) -> list[str]:
    """ç”·æ¬¾åœ¨ã€å­—æ¯ç³»ã€‘ä¸ã€æ•°å­—ç³»ã€‘äºŒé€‰ä¸€ï¼›å¥³æ¬¾å›ºå®š 4â€“20ã€‚"""
    g = (gender or "").lower()
    if "å¥³" in g:
        return WOMEN_ORDER[:]  # 4..20

    has_num   = any(k in MEN_NUM_ORDER   for k in present)
    has_alpha = any(k in MEN_ALPHA_ORDER for k in present)
    if has_num and not has_alpha:
        return MEN_NUM_ORDER[:]          # 30..50ï¼ˆä¸å«52ï¼‰
    if has_alpha and not has_num:
        return MEN_ALPHA_ORDER[:]        # 2XS..3XL
    if has_num or has_alpha:
        num_count   = sum(1 for k in present if k in MEN_NUM_ORDER)
        alpha_count = sum(1 for k in present if k in MEN_ALPHA_ORDER)
        return MEN_NUM_ORDER[:] if num_count >= alpha_count else MEN_ALPHA_ORDER[:]
    # å®åœ¨åˆ¤ä¸å‡ºæ¥ï¼Œé»˜è®¤ç”¨å­—æ¯ç³»
    return MEN_ALPHA_ORDER[:]


def _normalize_size(token: str, gender: str) -> str | None:
    """å°† 'UK 36' / '36' / 'XL' å½’ä¸€åˆ°ä½ çš„æ ‡å‡†ï¼Œå¹¶è¿‡æ»¤ç”·æ¬¾ 52ã€‚"""
    s = (token or "").strip().upper()
    s = s.replace("UK ", "").replace("EU ", "").replace("US ", "")
    s = re.sub(r"\s*\(.*?\)\s*", "", s)
    s = re.sub(r"\s+", " ", s)

    # å…ˆæ•°å­—
    m = re.findall(r"\d{1,3}", s)
    if m:
        n = int(m[0])
        if gender == "å¥³æ¬¾" and n in {4,6,8,10,12,14,16,18,20}:
            return str(n)
        if gender == "ç”·æ¬¾":
            # ç”·æ•°å­—ï¼š30..50ï¼ˆå¶æ•°ï¼‰ï¼Œä¸”æ˜¾å¼æ’é™¤ 52
            if 30 <= n <= 50 and n % 2 == 0:
                return str(n)
            # å®¹é”™è´´è¿‘ï¼šå¯¹ 28..54 å–å°±è¿‘å¶æ•°å¹¶è£å‰ªåˆ° 30..50
            if 28 <= n <= 54:
                cand = n if n % 2 == 0 else n-1
                cand = max(30, min(50, cand))
                return str(cand)
        # å…¶å®ƒåœºæ™¯ï¼šä¸è¿”å›
        return None

    # å†å­—æ¯
    key = s.replace("-", "").replace(" ", "")
    return ALPHA_MAP.get(key)

def _sort_sizes(keys: list[str], gender: str) -> list[str]:
    if gender == "å¥³æ¬¾":
        return [k for k in WOMEN_ORDER if k in keys]
    # ç”·æ¬¾ï¼šå­—æ¯ä¼˜å…ˆï¼Œå†æ•°å­—
    ordered = [k for k in MEN_ALPHA_ORDER if k in keys] + [k for k in MEN_NUM_ORDER if k in keys]
    return ordered

def _build_size_lines_from_sizedetail(size_detail: dict, gender: str) -> tuple[str, str]:
    bucket_status: dict[str, str] = {}
    bucket_stock: dict[str, int] = {}

    # 1) æ±‡æ€»é¡µé¢å‡ºç°çš„å°ºç ï¼ˆæœ‰è´§ä¼˜å…ˆï¼‰
    for raw_size, meta in (size_detail or {}).items():
        norm = _normalize_size(raw_size, gender or "ç”·æ¬¾")
        if not norm:
            continue
        stock = int(meta.get("stock_count", 0) or 0)
        status = "æœ‰è´§" if stock > 0 else "æ— è´§"
        prev = bucket_status.get(norm)
        if prev is None or (prev == "æ— è´§" and status == "æœ‰è´§"):
            bucket_status[norm] = status
            bucket_stock[norm] = DEFAULT_STOCK_COUNT if stock > 0 else 0

    # 2) é€‰æ‹©â€œå•ä¸€å°ºç ç³»â€çš„å®Œæ•´é¡ºåºè¡¨ï¼ˆç”·æ¬¾äºŒé€‰ä¸€ï¼›å¥³æ¬¾å›ºå®šï¼‰
    present_keys = set(bucket_status.keys())
    full_order = _choose_full_order_for_gender(gender or "ç”·æ¬¾", present_keys)

    # â˜…â˜…â˜… 2.5) å…ˆæŠŠâ€œå¦ä¸€å¥—ç³»â€çš„é”®æ¸…æ‰ï¼ˆé˜²æ­¢åç»­è¢«ä¸‹æ¸¸å†æ‹¼å›å»ï¼‰
    for k in list(bucket_status.keys()):
        if k not in full_order:
            bucket_status.pop(k, None)
            bucket_stock.pop(k, None)

    # 3) ä»…åœ¨é€‰å®šé‚£ä¸€ç³»å†…è¡¥é½æœªå‡ºç°çš„å°ºç ä¸º 0
    for size in full_order:
        if size not in bucket_status:
            bucket_status[size] = "æ— è´§"
            bucket_stock[size] = 0

    # 4) æŒ‰é€‰å®šç³»å›ºå®šé¡ºåºè¾“å‡º
    ordered = list(full_order)
    ps  = ";".join(f"{k}:{bucket_status[k]}" for k in ordered)
    psd = ";".join(f"{k}:{bucket_stock[k]}:0000000000000" for k in ordered)
    return ps, psd


# ============ æŠ“å–å¹¶å†™å…¥ TXTï¼ˆpipeline ç­¾åä¿æŒä¸å˜ï¼‰ ============

def fetch_one_product(url: str, idx: int, total: int):
    print(f"[{idx}/{total}] æŠ“å–: {url}")
    driver_name = f"allweathers_{idx}"    # æ¯ä¸ªä»»åŠ¡å•ç‹¬ä¸€ä¸ª driver å

    try:
        driver = shared_get_driver(
            name=driver_name,
            headless=True,
            window_size="1920,1080",
        )
        driver.get(url)
        time.sleep(2.5)
        html = driver.page_source

        # â€”â€” ä¸æ”¹çˆ¬å–é€»è¾‘ï¼šä¿ç•™ä½ åŸæœ‰çš„è§£æ â€”â€”
        info = parse_detail_page(html, url)

        # â€”â€” å†™å…¥å‰çš„è§„èŒƒåŒ–ï¼šåªåšå­—æ®µæ¸…æ´—ï¼Œä¸è§¦ç¢°æŠ“å–æµç¨‹ â€”â€”
        # å“ç‰Œä¸ç«™ç‚¹ä¿¡æ¯
        info.setdefault("Brand", "Barbour")
        info.setdefault("Site Name", CANON_SITE)
        info.setdefault("Source URL", url)

        # æ€§åˆ«ä¿®æ­£ï¼ˆä¼˜å…ˆ Barbour ç¼–ç å‰ç¼€ï¼›å†çœ‹æ ‡é¢˜/æè¿°ï¼›å¦åˆ™ç”¨åŸå€¼ï¼‰
        if infer_gender_for_barbour:
            info["Product Gender"] = infer_gender_for_barbour(
                product_code=info.get("Product Code"),
                title=info.get("Product Name"),
                description=info.get("Product Description"),
                given_gender=info.get("Product Gender"),
            ) or info.get("Product Gender") or "ç”·æ¬¾"

        # ç”± SizeDetail ç”Ÿæˆä¸¤è¡Œï¼ˆä¸è¾“å‡º SizeMapï¼›å¹¶è¿‡æ»¤ç”·æ¬¾ 52ï¼‰
        if info.get("SizeDetail") and (not info.get("Product Size") or not info.get("Product Size Detail")):
            ps, psd = _build_size_lines_from_sizedetail(info["SizeDetail"], info.get("Product Gender", "ç”·æ¬¾"))
            info["Product Size"] = info.get("Product Size") or ps
            info["Product Size Detail"] = info.get("Product Size Detail") or psd

        # æ–‡ä»¶åï¼šç”¨ Product Code
        code = info.get("Product Code") or "Unknown"
        safe_code = re.sub(r"[^A-Za-z0-9_-]+", "_", code)
        txt_path = TXT_DIR / f"{safe_code}.txt"

        # âœ… ç»Ÿä¸€å†™å…¥ï¼ˆåŒå…¶å®ƒç«™ç‚¹ï¼‰
        format_txt(info, txt_path, brand="Barbour")
        return (url, "âœ… æˆåŠŸ")
    except Exception as e:
        return (url, f"âŒ å¤±è´¥: {e}")
    finally:
        quit_driver(driver_name)


def allweathers_fetch_info(max_workers: int = MAX_WORKERS):
    print(f"ğŸš€ å¯åŠ¨ Allweathers å¤šçº¿ç¨‹å•†å“è¯¦æƒ…æŠ“å–ï¼ˆçº¿ç¨‹æ•°: {max_workers}ï¼‰")
    links = LINK_FILE.read_text(encoding="utf-8").splitlines()
    links = [u.strip() for u in links if u.strip()]
    total = len(links)

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [
            executor.submit(fetch_one_product, url, idx + 1, total)
            for idx, url in enumerate(links)
        ]
        for future in as_completed(futures):
            url, status = future.result()
            print(f"{status} - {url}")

    print("\nâœ… æ‰€æœ‰å•†å“æŠ“å–å®Œæˆ")


if __name__ == "__main__":
    allweathers_fetch_info()
