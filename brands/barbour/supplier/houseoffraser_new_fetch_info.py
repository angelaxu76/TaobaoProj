# -*- coding: utf-8 -*-
"""
House of Fraser | Barbour - æ–°ç‰ˆ Next.js PDP è§£æ
- ä¸å†å°è¯•æ—§ç‰ˆï¼›ç»Ÿä¸€æŒ‰æ–°æ ˆ(JSON-LD + DOM)è§£æ
- å•å®ä¾‹ Seleniumï¼ˆundetected-chromedriverï¼‰ï¼Œé¦–ä¸ªå•†å“é¡µç­‰å¾…10ç§’æ‰‹åŠ¨ç‚¹ Cookie
- è¾“å‡ºæ²¿ç”¨æ—§æœ‰ KV æ–‡æœ¬æ¨¡æ¿ï¼Œä¸æ”¹å­—æ®µå/é¡ºåºï¼Œä¿è¯ä¸‹æ¸¸å…¼å®¹
"""

from __future__ import annotations

import os, time, tempfile, threading, html as ihtml
from pathlib import Path
from typing import Optional, Dict, Any
from collections import OrderedDict

# ---- ä¾èµ– ----
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from sqlalchemy import create_engine
from sqlalchemy.engine import Connection

# ---- é¡¹ç›®å†…æ¨¡å—ï¼ˆä¿æŒä¸å˜ï¼‰----
from config import BARBOUR, BRAND_CONFIG
from brands.barbour.core.site_utils import assert_site_or_raise as canon
from brands.barbour.core.sim_matcher import match_product, choose_best
from common_taobao.core.size_utils import clean_size_for_barbour as _norm_size  # å°ºç æ¸…æ´—
from common_taobao.core.driver_auto import build_uc_driver

# ================== å¸¸é‡/è·¯å¾„ ==================
SITE_NAME = canon("houseoffraser")
LINKS_FILE: str = BARBOUR["LINKS_FILES"]["houseoffraser"]
TXT_DIR: Path = BARBOUR["TXT_DIRS"]["houseoffraser"]
TXT_DIR.mkdir(parents=True, exist_ok=True)
DEBUG_DIR: Path = TXT_DIR / "_debug"
DEBUG_DIR.mkdir(parents=True, exist_ok=True)

PRODUCTS_TABLE: str = BRAND_CONFIG.get("barbour", {}).get("PRODUCTS_TABLE", "barbour_products")
OFFERS_TABLE: Optional[str] = BRAND_CONFIG.get("barbour", {}).get("OFFERS_TABLE")
PG = BRAND_CONFIG["barbour"]["PGSQL_CONFIG"]

WAIT_HYDRATE_SECONDS = 22
DEFAULT_DELAY = 0.0
MAX_WORKERS_DEFAULT = 1  # å»ºè®®ä¸²è¡Œæœ€ç¨³ï¼›å¹¶å‘è¯·æ”¹ä¸ºâ€œæ¯çº¿ç¨‹1ä¸ªdriverâ€æ–¹æ¡ˆ
MIN_SCORE = 0.72
MIN_LEAD = 0.04
NAME_WEIGHT = 0.75
COLOR_WEIGHT = 0.25

# ================== URLâ†’Code ç¼“å­˜ ==================
URL_CODE_CACHE: Dict[str, str] = {}
_URL_CODE_CACHE_READY = False


import json
from bs4 import BeautifulSoup


WOMEN_ORDER = ["4","6","8","10","12","14","16","18","20"]
MEN_ALPHA_ORDER = ["2XS","XS","S","M","L","XL","2XL","3XL"]
MEN_NUM_ORDER = [str(n) for n in range(30, 52, 2)]  # 30..50ï¼Œç‰¹æ„ä¸åŒ…å«52

ALPHA_MAP = {
    "XXXS": "2XS", "2XS": "2XS",
    "XXS": "XS",  "XS": "XS",
    "S": "S", "SMALL":"S",
    "M": "M", "MEDIUM":"M",
    "L": "L", "LARGE":"L",
    "XL": "XL", "X-LARGE":"XL",
    "XXL":"2XL","2XL":"2XL",
    "XXXL":"3XL","3XL":"3XL",
}


def _normalize_size_token_for_barbour(raw_token: str, gender: str) -> str | None:
    """
    æŠŠç«™ç‚¹å°ºå¯¸ï¼ˆå¯èƒ½æ˜¯ '12(M)' / '16 (XL)' / 'UK 14' / 'XL' / '32'ï¼‰æ¸…æ´—æˆæ ‡å‡†å†…éƒ¨ç ï¼š
      - å¥³æ¬¾: æ•°å­— 4,6,8,10,12,14,16,18,20
      - ç”·æ¬¾ä¸Šè¡£: 2XS,XS,S,M,L,XL,2XL,3XL
      - ç”·æ¬¾ä¸‹è£…: 30,32,34,...,50 (å¶æ•°)
    å¦‚æœä¸æ˜¯åˆæ³•å°ºç ï¼Œè¿”å› Noneã€‚
    """

    if not raw_token:
        return None

    s = raw_token.strip().upper()

    # å»æ‰æ‹¬å·éƒ¨åˆ†ï¼Œæ¯”å¦‚ "12(M)" -> "12"
    s = re.sub(r"\(.*?\)", "", s)
    s = s.strip()

    # å»æ‰ "UK 12" / "UK12" / "EU 40" è¿™ç§å‰ç¼€
    s = re.sub(r"^(UK|EU|US)\s*", "", s)

    # å‹ç´§ç©ºæ ¼ã€è¿å­—ç¬¦
    s = re.sub(r"\s+", "", s)
    s = s.replace("-", "")

    # --- å¥³æ¬¾é€»è¾‘ ---
    # æ³¨æ„ï¼šä¸ç”¨å†åªçœ‹ä¸­æ–‡â€œå¥³â€ï¼Œè€Œæ˜¯ç”¨ _is_female_gender()
    if _is_female_gender(gender):
        m = re.match(r"^(\d{1,2})$", s)
        if m:
            n = int(m.group(1))
            if n in {4,6,8,10,12,14,16,18,20}:
                return str(n)
        # å¥³è£…ä¸è¾“å‡º S/M/L ä¹‹ç±»ï¼Œç›´æ¥ä¸¢å¼ƒ
        return None

    # --- ç”·æ¬¾é€»è¾‘ ---

    # æ•°å­—è£¤è£…å°ºç ï¼Œæ¯”å¦‚ 32, 34, 36...ï¼ˆè…°å›´ï¼‰
    m = re.match(r"^(\d{2})$", s)
    if m:
        n = int(m.group(1))
        if 30 <= n <= 50 and n % 2 == 0:
            return str(n)

    # ä¸Šè£…å­—æ¯ç ï¼Œæ¯”å¦‚ XL, XXL, 3XL...
    mapped = ALPHA_MAP.get(s)
    if mapped:
        return mapped

    return None



def _choose_size_family_for_gender(gender: str, observed_sizes: set[str]) -> list[str]:
    """
    å†³å®šæˆ‘ä»¬è¦è¾“å‡º/è¡¥å…¨çš„â€œè¿™ä¸€å¥—å°ºç ä½“ç³»â€çš„é¡ºåºåˆ—è¡¨ã€‚
    - å¥³æ¬¾ï¼šå›ºå®š 4..20
    - ç”·æ¬¾ï¼šåœ¨å­—æ¯ç³»(2XS..3XL) å’Œ æ•°å­—ç³»(30..50) ä¹‹é—´äºŒé€‰ä¸€
      - å¦‚æœä¸¤ç§éƒ½æœ‰ï¼Œåˆ™çœ‹å“ªä¸ªå‡ºç°çš„æ•°é‡æ›´å¤š
    """

    g = (gender or "").lower()
    if "å¥³" in g:
        return WOMEN_ORDER[:]

    has_num   = any(k in MEN_NUM_ORDER   for k in observed_sizes)
    has_alpha = any(k in MEN_ALPHA_ORDER for k in observed_sizes)

    if has_num and not has_alpha:
        return MEN_NUM_ORDER[:]
    if has_alpha and not has_num:
        return MEN_ALPHA_ORDER[:]

    if has_num or has_alpha:
        num_count   = sum(1 for k in observed_sizes if k in MEN_NUM_ORDER)
        alpha_count = sum(1 for k in observed_sizes if k in MEN_ALPHA_ORDER)
        return MEN_NUM_ORDER[:] if num_count >= alpha_count else MEN_ALPHA_ORDER[:]

    # å®åœ¨çœ‹ä¸å‡ºæ¥ï¼Œå°±é»˜è®¤å­—æ¯ç³»
    return MEN_ALPHA_ORDER[:]


# ---------- helpers: æå– next/å†…åµŒ JSON æ–‡æœ¬ ----------
def _extract_next_json_chunks(html: str) -> str:
    # ç›´æ¥ç”¨æ­£åˆ™åœ¨ HTML æ–‡æœ¬é‡Œæ‰¾åŒ…å«å…³é”®å­—æ®µçš„å¤§å— JSON
    # å°½é‡ä¿å®ˆï¼šç”¨æœ€å°åŒ¹é… + å…³é”®é”šç‚¹
    m = re.search(r'"gender":"(?:Mens|Womens|Girls|Boys)".{0,2000}?"variants":\[(.*?)\]\}', html, re.S)
    if m: 
        return m.group(0)
    # å…œåº•ï¼šæ‰¾åŒ…å« sizes.allSizes çš„å—
    m = re.search(r'"sizes":\{"allSizes":\[(.*?)\]\}', html, re.S)
    return m.group(0) if m else ""

def _extract_color_from_og_alt(soup: BeautifulSoup) -> str:
    # å–ç¬¬ä¸€ä¸ª og:image:altï¼Œå½¢å¦‚ "Black BK11 - Barbour International - xxx"
    for tag in soup.find_all("meta", {"property": "og:image:alt"}):
        content = (tag.get("content") or "").strip()
        if content:
            # é¢œè‰²å = ç¬¬ä¸€ä¸ª " - " ä¹‹å‰ï¼Œå†å»æ‰æœ«å°¾çš„è‰²ç 
            first = content.split(" - ", 1)[0]  # e.g. "Black BK11"
            # å»æ‰ç±»ä¼¼ BK11/NY91 è¿™ç±»è‰²ç 
            first = re.sub(r"\b[A-Z]{2}\d{2,}\b", "", first).strip()
            if first:
                return first
    return ""

def _extract_all_sizes(html: str):
    """
    è¿”å› list[str]ï¼Œæ¥è‡ª sizes.allSizes[].sizeã€‚ä¼šå¯¹ size åš _norm_size æ¸…æ´—å¹¶å»é‡ä¿åºã€‚
    """
    sizes = []
    ms = re.search(r'"sizes"\s*:\s*\{\s*"allSizes"\s*:\s*(\[[^\]]*\])', html, re.S | re.I)
    if ms:
        try:
            arr = json.loads(ms.group(1))
            seen = set()
            for it in arr:
                raw = (it.get("size") or "").strip()
                if not raw:
                    continue
                size = _norm_size(raw) or raw   # â† æ¸…æ´—
                if size not in seen:
                    seen.add(size)
                    sizes.append(size)
        except Exception:
            pass
    return sizes



def _extract_sizes_dom_fallback(soup: BeautifulSoup):
    """
    ä»æ¸²æŸ“åçš„å°ºå¯¸æŒ‰é’®å®¹å™¨æŠ“å–ï¼š
    - data-testid="swatch-button-enabled" => æœ‰è´§
    - data-testid="swatch-button-disabled" => æ— è´§
    ä¼šå¯¹ size åš _norm_size æ¸…æ´—å¹¶å»é‡ã€‚
    """
    enabled = []
    disabled = []

    for btn in soup.select('[data-testid="swatch-button-enabled"]'):
        raw = (btn.get("value") or btn.get_text() or "").strip()
        if not raw: 
            continue
        size = _norm_size(raw) or raw  # â† æ¸…æ´—
        enabled.append(size)

    for btn in soup.select('[data-testid="swatch-button-disabled"], button[disabled][data-testid*="swatch"]'):
        raw = (btn.get("value") or btn.get_text() or "").strip()
        if not raw:
            continue
        size = _norm_size(raw) or raw  # â† æ¸…æ´—
        disabled.append(size)

    if not enabled and not disabled:
        return []

    # å»é‡ä¿åºï¼›æœ‰è´§ä¼˜å…ˆ
    seen = set()
    entries = []
    for s in enabled:
        if s not in seen:
            seen.add(s); entries.append((s, "æœ‰è´§"))
    for s in disabled:
        if s not in seen:
            seen.add(s); entries.append((s, "æ— è´§"))

    return entries




def _extract_color_gender_from_json(html: str) -> (str, str):
    block = _extract_next_json_chunks(html)
    color = ""
    gender = ""
    if block:
        mc = re.search(r'"color"\s*:\s*"([^"]+)"', block)
        if mc:
            raw = mc.group(1)  # e.g. "Black BK11"
            color = re.sub(r"\b[A-Z]{2}\d{2,}\b", "", raw).strip()
        mg = re.search(r'"gender"\s*:\s*"(Mens|Womens|Girls|Boys)"', block)
        if mg:
            g = mg.group(1).lower()
            # ç»Ÿä¸€åˆ°ä½ åŸæ¥ TXT ä¹ æƒ¯ï¼ˆmen / womenï¼‰ï¼Œä¹Ÿå¯è¾“å‡º mens/womens
            mapping = {"mens":"men", "womens":"women", "girls":"women", "boys":"men"}
            gender = mapping.get(g, g)
    return color, gender

def _extract_color_new(soup: BeautifulSoup, html: str) -> str:
    color, _ = _extract_color_gender_from_json(html)
    if color:
        return color
    alt_color = _extract_color_from_og_alt(soup)
    return alt_color or "No Data"

def _infer_gender_from_code(code: Optional[str]) -> str:
    """
    ä»…åœ¨å·²æ‹¿åˆ°äº§å“ç¼–ç æ—¶å¯ç”¨ã€‚
    å¸¸è§å‰ç¼€ï¼ˆBarbour/Internationalï¼‰ï¼š
      ç”·æ¬¾ï¼šMQU, MWX, MSH, MKN, MGL, MFL, MGI, MLI, MSW, MCA...
      å¥³æ¬¾ï¼šLQU, LWX, LSH, LKN, LGL, LFL, LGI, LLI, LSW, LCA...
    è¿”å›ï¼šmen / women / No Data
    """
    if not code:
        return "No Data"
    c = code.strip().upper()
    # å…ˆçœ‹é¦–å­—æ¯
    if c.startswith("M"):
        return "men"
    if c.startswith("L"):
        return "women"
    # å†çœ‹å¸¸è§ 3 ä½å‰ç¼€ï¼ˆæ›´ç²¾ç¡®ï¼‰
    male3  = ("MQU", "MWX", "MSH", "MKN", "MGL", "MFL", "MGI", "MLI", "MSW", "MCA")
    female3= ("LQU", "LWX", "LSH", "LKN", "LGL", "LFL", "LGI", "LLI", "LSW", "LCA")
    pre3 = c[:3]
    if pre3 in male3:
        return "men"
    if pre3 in female3:
        return "women"
    return "No Data"

def _gender_to_cn(g: str) -> str:
    if not g:
        return "No Data"
    g = g.strip().lower()
    if g in ("men", "mens"):
        return "ç”·æ¬¾"
    if g in ("women", "womens"):
        return "å¥³æ¬¾"
    # å¦‚ä½ ä»¥åæƒ³åŒºåˆ†ç«¥æ¬¾ï¼Œå¯åœ¨è¿™æ‰©å±•ï¼š
    if g in ("boys", "boy"):
        return "ç”·æ¬¾"   # æˆ–è€…è¿”å› "ç«¥æ¬¾-ç”·"
    if g in ("girls", "girl"):
        return "å¥³æ¬¾"   # æˆ–è€…è¿”å› "ç«¥æ¬¾-å¥³"
    return "No Data"


def _extract_gender_new(soup: BeautifulSoup, html: str, url: str) -> str:
    """
    ä¼˜å…ˆä»æ•´é¡µ JSON ä¸­æŠ½å– "gender":"Mens|Womens|Boys|Girls"ï¼›
    è‹¥æ²¡æœ‰ï¼Œå†ä»é¢åŒ…å±‘/æ ‡é¢˜/æè¿°é‡Œæ¨æ–­ï¼›æœ€åç”¨ URL å…œåº•ã€‚
    è¿”å›ï¼šmen / women / No Data
    """
    # 1) JSONï¼ˆæ•´é¡µä»»æ„ä½ç½®ï¼‰
    m = re.search(r'"gender"\s*:\s*"(Mens|Womens|Girls|Boys)"', html, re.I)
    if m:
        g = m.group(1).lower()
        mapping = {"mens": "men", "womens": "women", "girls": "women", "boys": "men"}
        return mapping.get(g, g)

    # 2) é¢åŒ…å±‘/æ ‡é¢˜/æè¿° æ¨æ–­
    # ï¼ˆå°½é‡ä¸ä¾èµ–ä½ å…¶ä»–å‡½æ•°ï¼Œé¿å…å‘½åå†²çªï¼‰
    bc = soup.select("nav[aria-label*=breadcrumb] a, ol[aria-label*=breadcrumb] a")
    bc_txt = " ".join(a.get_text(" ", strip=True) for a in bc) if bc else ""
    title = soup.title.get_text(strip=True) if soup.title else ""
    meta_desc = soup.find("meta", {"name": "description"})
    desc = (meta_desc.get("content") or "") if meta_desc else ""
    blob = " ".join([bc_txt.lower(), title.lower(), desc.lower()])

    if any(w in blob for w in ("womens", "women", "ladies", "women's", "lady")):
        return "women"
    if any(w in blob for w in ("mens", "men", "men's", "man")):
        return "men"

    # 3) URL å…œåº•
    ul = (url or "").lower()
    if "/women" in ul or "womens" in ul:
        return "women"
    if "/men" in ul or "mens" in ul:
        return "men"

    return "No Data"



# ---------- sizes & availability ----------
def _extract_sizes_from_variants(html: str):
    """
    è¿”å› list[(size, status)]ï¼Œä»…åŒ…å« variants é‡Œå‡ºç°çš„å°ºç ï¼›
    status = "æœ‰è´§"/"æ— è´§"ï¼ˆä»¥ isOnStock ä¸ºå‡†ï¼‰ã€‚ä¼šå¯¹ size åš _norm_size æ¸…æ´—ã€‚
    """
    entries = []
    patt = r'"size"\s*:\s*"([^"]+?)".{0,4000}?"isOnStock"\s*:\s*(true|false)'
    for m in re.finditer(patt, html, re.S | re.I):
        raw = m.group(1).strip()
        size = _norm_size(raw) or raw  # â† æ¸…æ´—
        avail = (m.group(2).lower() == "true")
        if size:
            entries.append((size, "æœ‰è´§" if avail else "æ— è´§"))
    return entries


def _extract_sizes_from_allSizes(html: str):
    # å…œåº•ï¼šæ²¡æœ‰ isOnStockï¼Œå°±è®¤ä¸ºé¡µé¢åœ¨å”® â†’ è®°ä¸ºæœ‰è´§
    entries = []
    ms = re.search(r'"sizes"\s*:\s*\{\s*"allSizes"\s*:\s*(\[[^\]]*\])', html, re.S)
    if ms:
        try:
            arr = json.loads(ms.group(1))
            for it in arr:
                size = (it.get("size") or "").strip()
                if size:
                    entries.append((size, "æœ‰è´§"))
        except Exception:
            pass
    return entries

import re
from typing import Tuple, Dict, Optional
from bs4 import BeautifulSoup

OOS_WORDS = ["out of stock", "sold out", "unavailable", "no stock", "oos"]

def _is_number_size(label: str) -> bool:
    # çº¯æ•°å­—ï¼Œæ¯”å¦‚ "44", "46", "32", "30"
    return bool(re.fullmatch(r"\d+", label.strip()))

def _is_letter_size(label: str) -> bool:
    # å¸¸è§è¡£æœå­—æ¯ç ï¼ŒåŒ…æ‹¬ 3XL / 4XL / XXL / XXXL ç­‰
    norm = label.strip().upper()
    # æ˜ç¡®å¸¸è§é›†åˆ
    COMMON = [
        "2XS","XXS","XS",
        "S","M","L",
        "XL","XXL","2XL",
        "XXXL","3XL","4XL","5XL"
    ]
    return norm in COMMON

def _extract_sizes_new(soup: BeautifulSoup) -> Dict[str, Dict[str, int]]:
    """
    æŠ“ House of Fraser / Flannels é¡µé¢ä¸Šçš„å°ºç æŒ‰é’®ï¼Œè¿”å›ä¸­é—´ç»“æ„:
    {
        "12": {"stock": 3},
        "14": {"stock": 3},
        "16": {"stock": 3},
        "XS": {"stock": 0},
        ...
    }
    æ³¨æ„ï¼šè¿™é‡Œä¸åšæ¸…æ´—/ä¸åšè¡¥å…¨/ä¸ç®¡ç”·å¥³ï¼Œåªè´Ÿè´£åŸå§‹æŠ“å–ã€‚
    åé¢ä¼šç”± normalize+è¡¥ç  çš„æµç¨‹æ¥ç”Ÿæˆæœ€ç»ˆ Product Size / Product Size Detailã€‚
    """

    results: Dict[str, Dict[str, int]] = {}

    # swatch æŒ‰é’®æ˜¯æˆ‘ä»¬æœ€å¯é çš„æ¥æº
    for btn in soup.select("button[data-testid='swatch-button-enabled'], button[data-testid='swatch-button-disabled']"):
        # label å°ºç åï¼Œæ¯”å¦‚ "12 (M)" / "16(XL)" / "XL" / "2XL"
        label = (btn.get("value") or btn.get_text() or "").strip()
        if not label:
            continue

        # åˆ¤æ–­åº“å­˜
        data_tid = btn.get("data-testid", "")
        in_stock = "enabled" in data_tid  # enabled = å¯é€‰ï¼Œæœ‰è´§ï¼›disabled = æ— è´§
        stock_qty = 3 if in_stock else 0

        # è®°å½•ï¼ˆåé¢ä¼šç»§ç»­æ¸…æ´—ï¼‰
        if label not in results:
            results[label] = {"stock": stock_qty}
        else:
            # å¤šæ¬¡å‡ºç°ï¼Œä¿ç•™æœ‰è´§é‚£ä¸ª
            if results[label]["stock"] == 0 and stock_qty > 0:
                results[label]["stock"] = stock_qty

    return results



def _is_female_gender(g: str) -> bool:
    """
    åˆ¤æ–­æ˜¯ä¸æ˜¯å¥³æ¬¾:
    - åŒ…å«ä¸­æ–‡"å¥³"
    - æˆ–è‹±æ–‡ women / womens / girl / girls / ladies / female
    """
    if not g:
        return False
    gl = g.strip().lower()
    if "å¥³" in gl:
        return True
    female_keys = ["women", "womens", "woman", "girl", "girls", "ladies", "lady", "female"]
    return any(k in gl for k in female_keys)


def _is_male_gender(g: str) -> bool:
    """
    åˆ¤æ–­æ˜¯ä¸æ˜¯ç”·æ¬¾:
    - åŒ…å«ä¸­æ–‡"ç”·"
    - æˆ–è‹±æ–‡ men / mens / boy / boys / male
    """
    if not g:
        return False
    gl = g.strip().lower()
    if "ç”·" in gl:
        return True
    male_keys = ["men", "mens", "man", "boy", "boys", "male"]
    return any(k in gl for k in male_keys)


def _choose_size_family_for_gender(gender: str, observed_sizes: set[str]) -> list[str]:
    """
    å†³å®šè¾“å‡ºå“ªå¥—å®Œæ•´å°ºç åºåˆ—ã€‚
    - å¥³æ¬¾ï¼šå›ºå®šå¥³è£…æ•°å­—ç  ["4","6","8","10","12","14","16","18","20"]
    - ç”·æ¬¾ï¼šåœ¨ç”·è£…å­—æ¯ç³»(2XS..3XL) å’Œ ç”·è£…æ•°å­—è…°å›´ç³»(30..50 å¶æ•°)é‡ŒäºŒé€‰ä¸€
      ï¼ˆè°å‡ºç°å¾—å¤šå°±ç”¨è°ï¼‰ï¼Œç„¶åæˆ‘ä»¬ä¼šåœ¨åé¢è¡¥å…¨æ²¡å‡ºç°çš„ç ã€‚
    """
    if _is_female_gender(gender):
        return WOMEN_ORDER[:]

    # ç”·æ¬¾ / æœªçŸ¥ é»˜è®¤æŒ‰ç”·é€»è¾‘
    has_num   = any(k in MEN_NUM_ORDER   for k in observed_sizes)
    has_alpha = any(k in MEN_ALPHA_ORDER for k in observed_sizes)

    if has_num and not has_alpha:
        return MEN_NUM_ORDER[:]
    if has_alpha and not has_num:
        return MEN_ALPHA_ORDER[:]

    if has_num or has_alpha:
        num_count   = sum(1 for k in observed_sizes if k in MEN_NUM_ORDER)
        alpha_count = sum(1 for k in observed_sizes if k in MEN_ALPHA_ORDER)
        return MEN_NUM_ORDER[:] if num_count >= alpha_count else MEN_ALPHA_ORDER[:]

    # å®åœ¨çœ‹ä¸å‡ºæ¥ï¼Œå…œåº•ç”¨ç”·æ¬¾å­—æ¯ç³»
    return MEN_ALPHA_ORDER[:]


def _finalize_sizes_for_hof(raw_size_dict: Dict[str, Dict[str, int]], gender: str) -> Tuple[str, str]:
    """
    raw_size_dict å½¢å¦‚:
        { "12(M)": {"stock":3}, "14(L)": {"stock":3}, "16(XL)": {"stock":3} }

    gender: ç°åœ¨å¯èƒ½æ˜¯ "women"/"men"/"No Data"/"ç”·æ¬¾"/"å¥³æ¬¾" ç­‰ç­‰

    è¾“å‡º:
      Product Size:        "10:æ— è´§;12:æœ‰è´§;14:æœ‰è´§;16:æœ‰è´§;18:æ— è´§;20:æ— è´§"
      Product Size Detail: "10:0:000...;12:3:000...;14:3:000...;16:3:000...;18:0:000...;20:0:000..."
    """

    # 1. æ¸…æ´—æˆæˆ‘ä»¬æ ‡å‡†ç ï¼Œå¹¶è®°å½•åº“å­˜
    normalized_stock: Dict[str, int] = {}
    for raw_label, meta in (raw_size_dict or {}).items():
        norm = _normalize_size_token_for_barbour(raw_label, gender or "")
        if not norm:
            continue
        stock_qty = int(meta.get("stock", 0))
        if norm not in normalized_stock:
            normalized_stock[norm] = stock_qty
        else:
            # å¦‚æœä¹‹å‰æ˜¯æ— è´§ï¼Œç°åœ¨å‘ç°æœ‰è´§ï¼Œå°±æ›´æ–°
            if normalized_stock[norm] == 0 and stock_qty > 0:
                normalized_stock[norm] = stock_qty

    # 2. é€‰å®šåº”è¯¥ç”¨å“ªä¸€å¥—å®Œæ•´å°ºç è¡¨ï¼ˆå¥³æ¬¾å›ºå®š 4..20ï¼Œç”·æ¬¾åœ¨ä¸¤å¥—ç”·ç ä½“ç³»é‡Œé€‰ï¼‰
    observed = set(normalized_stock.keys())
    full_order = _choose_size_family_for_gender(gender or "", observed)

    # 3. æŠŠä¸åœ¨è¯¥ä½“ç³»é‡Œçš„ç å‰”é™¤ï¼ˆé˜²æ­¢ "12" è·Ÿ "M" æ··åœ¨ä¸€èµ·ï¼‰
    for k in list(normalized_stock.keys()):
        if k not in full_order:
            normalized_stock.pop(k, None)

    # 4. æŒ‰ full_order è¡¥å…¨æ‰€æœ‰ç ï¼šæœ‰çš„å†™çœŸå®åº“å­˜>0 => æœ‰è´§ï¼Œå¦åˆ™æ— è´§
    EAN_PLACEHOLDER = "0000000000000"
    size_line_parts = []
    size_detail_parts = []

    for size_token in full_order:
        qty = normalized_stock.get(size_token, 0)
        status = "æœ‰è´§" if qty > 0 else "æ— è´§"

        size_line_parts.append(f"{size_token}:{status}")
        size_detail_parts.append(f"{size_token}:{3 if qty > 0 else 0}:{EAN_PLACEHOLDER}")

    product_size_str = ";".join(size_line_parts) if size_line_parts else "No Data"
    product_size_detail_str = ";".join(size_detail_parts) if size_detail_parts else "No Data"

    return product_size_str, product_size_detail_str






def _normalize_url(u: str) -> str:
    return u.strip() if u else ""

def get_dbapi_connection(conn_or_engine):
    if hasattr(conn_or_engine, "cursor"): return conn_or_engine
    if hasattr(conn_or_engine, "raw_connection"): return conn_or_engine.raw_connection()
    c = getattr(conn_or_engine, "connection", None)
    if c is not None:
        dbapi = getattr(c, "dbapi_connection", None)
        if dbapi is not None and hasattr(dbapi, "cursor"): return dbapi
        inner = getattr(c, "connection", None)
        if inner is not None and hasattr(inner, "cursor"): return inner
        if hasattr(c, "cursor"): return c
    return conn_or_engine

def _safe_sql_to_cache(raw_conn, sql: str, params=None) -> Dict[str, str]:
    cache = OrderedDict()
    try:
        with raw_conn.cursor() as cur:
            cur.execute(sql, params or {})
            for url, code in cur.fetchall():
                if url and code:
                    cache[_normalize_url(str(url))] = str(code).strip()
    except Exception:
        try: raw_conn.rollback()
        except Exception: pass
    return cache

def build_url_code_cache(raw_conn, products_table: str, offers_table: Optional[str], site_name: str):
    """å¯åŠ¨æ—¶æ„å»ºä¸€æ¬¡ URLâ†’ProductCode æ˜ å°„ç¼“å­˜ã€‚"""
    global URL_CODE_CACHE, _URL_CODE_CACHE_READY
    if _URL_CODE_CACHE_READY:
        return URL_CODE_CACHE

    cache = OrderedDict()

    if offers_table:
        candidates = [
            ("offer_url",   "product_code"),
            ("source_url",  "product_code"),
            ("product_url", "product_code"),
            ("offer_url",   "color_code"),
            ("source_url",  "color_code"),
            ("product_url", "color_code"),
        ]
        for url_col, code_col in candidates:
            sql = f"""
                SELECT {url_col}, {code_col}
                  FROM {offers_table}
                 WHERE site_name = %(site)s
                   AND {url_col} IS NOT NULL
                   AND {code_col} IS NOT NULL
            """
            cache.update(_safe_sql_to_cache(raw_conn, sql, {"site": site_name}))

    for url_col in ("source_url", "offer_url", "product_url"):
        sql = f"""
            SELECT {url_col}, product_code
              FROM {products_table}
             WHERE {url_col} IS NOT NULL
               AND product_code IS NOT NULL
        """
        cache.update(_safe_sql_to_cache(raw_conn, sql))

    URL_CODE_CACHE = dict(cache)
    _URL_CODE_CACHE_READY = True
    print(f"ğŸ§  URLâ†’Code ç¼“å­˜æ„å»ºå®Œæˆï¼š{len(URL_CODE_CACHE)} æ¡")
    return URL_CODE_CACHE

# ================== æ–‡ä»¶å†™å…¥/æ¨¡æ¿ ==================
def _atomic_write_bytes(data: bytes, dst: Path, retries: int = 6, backoff: float = 0.25) -> bool:
    dst.parent.mkdir(parents=True, exist_ok=True)
    for i in range(retries):
        tmp = None
        try:
            with tempfile.NamedTemporaryFile(delete=False, dir=str(dst.parent),
                                             prefix=".tmp_", suffix=f".{os.getpid()}.{threading.get_ident()}") as tf:
                tmp = Path(tf.name)
                tf.write(data); tf.flush(); os.fsync(tf.fileno())
            try:
                os.replace(tmp, dst)
            finally:
                if tmp and tmp.exists():
                    try: tmp.unlink(missing_ok=True)
                    except Exception: pass
            return True
        except Exception:
            if dst.exists(): return True
            time.sleep(backoff * (i + 1))
            try:
                if tmp and tmp.exists(): tmp.unlink(missing_ok=True)
            except Exception:
                pass
    return dst.exists()

def _kv_txt_bytes(info: Dict[str, Any]) -> bytes:
    # âœ¨ ä¿æŒä¸æ—§ç‰ˆå®Œå…¨ä¸€è‡´çš„ KV è¾“å‡ºå­—æ®µé¡ºåº
    fields = [
        "Product Code", "Product Name", "Product Description", "Product Gender",
        "Product Color", "Product Price", "Adjusted Price", "Product Material",
        "Style Category", "Feature", "Product Size", "Product Size Detail",
        "Source URL", "Site Name"
    ]
    lines = [f"{k}: {info.get(k, 'No Data')}" for k in fields]
    return ("\n".join(lines) + "\n").encode("utf-8", errors="ignore")

def _safe_name(s: str) -> str:
    return re.sub(r"[\\/:*?\"<>|'\s]+", "_", (s or "NoName"))

def _dump_debug_html(html: str, url: str, tag: str = "debug1") -> Path:
    short = f"{abs(hash(url)) & 0xFFFFFFFF:08x}"
    out = DEBUG_DIR / f"{tag}_{short}.html"
    _atomic_write_bytes(html.encode("utf-8", errors="ignore"), out)
    print(f"ğŸ§ª HTML dump â†’ {out}")
    return out

def _clean(s: Optional[str]) -> str:
    return re.sub(r"\s+", " ", (s or "").strip())

def _to_num(s: Optional[str]) -> Optional[float]:
    if not s: return None
    m = re.search(r"([0-9]+(?:\.[0-9]{1,2})?)", s.replace(",", ""))
    return float(m.group(1)) if m else None

# ================== ç­‰å¾…æ°´åˆ ==================
def _soft_scroll(driver, steps=6, pause=0.45):
    for _ in range(steps):
        try:
            driver.execute_script("window.scrollBy(0, Math.floor(document.body.scrollHeight * 0.28));")
        except Exception:
            pass
        time.sleep(pause)
    try:
        driver.execute_script("window.scrollTo(0, 0);")
    except Exception:
        pass
    time.sleep(0.3)

def wait_pdp_ready(driver, timeout: int = WAIT_HYDRATE_SECONDS) -> bool:
    key_css = ", ".join([
        # price/title
        "[data-testid*='price']","[data-component*='price']","[itemprop='price']","meta[itemprop='price']",
        "h1","[data-testid*='title']","[data-component*='title']",
        # sizes
        "button[aria-pressed][data-testid*='size']","li[role='option']","div[role='option']",
        "option[data-testid*='drop-down-option']","#sizeDdl option",
        # JSON-LD
        "script[type='application/ld+json']",
    ])
    end = time.time() + timeout
    while time.time() < end:
        try:
            if driver.find_elements(By.CSS_SELECTOR, key_css):
                return True
        except Exception:
            pass
        _soft_scroll(driver, steps=2, pause=0.45)
    return False

# ================== JSON-LD è§£æ ==================
def _pick_first(x):
    if isinstance(x, list) and x: return x[0]
    return x

def parse_jsonld(html: str) -> dict:
    soup = BeautifulSoup(html, "html.parser")
    blocks = []
    for tag in soup.find_all("script", {"type": "application/ld+json"}):
        txt = (tag.string or tag.text or "").strip()
        if not txt: continue
        try:
            data = json.loads(txt)
            blocks.append(data)
        except Exception:
            continue

    def _walk(obj):
        if isinstance(obj, dict):
            yield obj
            for v in obj.values():
                yield from _walk(v)
        elif isinstance(obj, list):
            for it in obj:
                yield from _walk(it)

    product = None
    offers = []
    for b in blocks:
        for node in _walk(b):
            if not isinstance(node, dict): continue
            t = node.get("@type") or node.get("@type".lower())
            if t == "Product":
                product = node
            elif t == "Offer":
                offers.append(node)

    out = {}
    if product:
        out["title"] = _pick_first(product.get("name"))
        out["sku"] = product.get("sku") or product.get("mpn") or product.get("productID")
        brand = product.get("brand")
        if isinstance(brand, dict): out["brand"] = brand.get("name")
        else: out["brand"] = brand
        imgs = product.get("image") or []
        if isinstance(imgs, str): imgs = [imgs]
        out["images"] = imgs
        desc = product.get("description") or ""
        desc = re.sub(r"<[^>]+>", " ", desc)
        out["description"] = re.sub(r"\s+", " ", ihtml.unescape(desc)).strip()

    price = currency = availability = None
    url = None
    for off in offers:
        p = off.get("price") or off.get("priceSpecification", {}).get("price")
        if p:
            price = p
            currency = off.get("priceCurrency") or off.get("priceSpecification", {}).get("priceCurrency")
            availability = off.get("availability")
            url = off.get("url")
            break
    if price: out["price"] = str(price)
    if currency: out["currency"] = currency
    if availability:
        out["availability"] = availability.split("/")[-1] if "/" in availability else availability
    if url: out["url"] = url
    return out

# ================== æ€§åˆ«/é¢œè‰²/å°ºç ï¼ˆåŠ å›ºç‰ˆï¼‰ ==================
GENDER_WORDS = (
    ("Womens", ("womens", "women", "ladies", "women's", "lady")),
    ("Mens",   ("mens", "men", "men's", "man")),
    ("Boys",   ("boys", "boy")),
    ("Girls",  ("girls", "girl")),
)
COLOR_WORDS = [
    "Black","Navy","Green","Olive","Brown","Blue","Red","Cream","Beige","Grey","Gray",
    "White","Pink","Burgundy","Khaki","Stone","Tan","Orange","Yellow","Purple"
]

def _infer_gender_from_text(*texts) -> str:
    blob = " ".join(t.lower() for t in texts if t).strip()
    for label, keys in GENDER_WORDS:
        if any(k in blob for k in keys):
            return label
    return "No Data"



def _extract_gender(html: str, soup: BeautifulSoup, title: str, desc: str, url: str) -> str:
    bc = soup.select("nav[aria-label*=breadcrumb] a, ol[aria-label*=breadcrumb] a")
    bc_txt = " ".join(a.get_text(" ", strip=True) for a in bc) if bc else ""
    g = _infer_gender_from_text(bc_txt, title, desc, url)
    if g != "No Data": return g
    m = re.search(r'"gender"\s*:\s*"(?P<g>Womens|Mens|Boys|Girls)"', html, re.I)
    if m: return m.group("g").capitalize()
    return _infer_gender_from_text(desc, title)

def _extract_color(html: str, soup: BeautifulSoup, title: str) -> str:
    el = soup.select_one("[data-testid*='colour'] [data-testid*='value'], [data-component*='colour'], [aria-label*='Colour'] [aria-live]")
    if el:
        c = el.get_text(strip=True)
        if c: return c
    m = re.search(r'"color"\s*:\s*"([A-Za-z /-]{3,20})"', html)
    if m: return m.group(1).strip()
    for w in COLOR_WORDS:
        if re.search(rf"\b{re.escape(w)}\b", title, re.I):
            return w
    return "No Data"

def _extract_sizes_legacy_dropdown(soup: BeautifulSoup) -> Tuple[str, str]:
    entries = []

    # 1) æŒ‰é’®
    for btn in soup.select("button[aria-pressed][data-testid*='size'], button[aria-pressed][aria-label*='Size']"):
        lab = (btn.get_text() or btn.get("aria-label") or "").strip()
        if not lab: continue
        disabled = (btn.get("disabled") is not None) or (btn.get("aria-disabled") in ("true", "True"))
        status = "æ— è´§" if disabled else "æœ‰è´§"
        entries.append((lab, status))

    # 2) åˆ—è¡¨/å¯é€‰é¡¹
    for node in soup.select("li[role='option'], div[role='option']"):
        lab = (node.get_text() or node.get("aria-label") or "").strip()
        if not lab: continue
        if lab.lower().startswith(("select size", "choose size")):
            continue
        disabled = node.get("aria-disabled") in ("true", "True") or "disabled" in (node.get("class") or [])
        status = "æ— è´§" if disabled else "æœ‰è´§"
        entries.append((lab, status))

    # 3) ä¸‹æ‹‰
    for opt in soup.select("select option[data-testid*='drop-down-option'], #sizeDdl option"):
        lab = (opt.get_text() or "").strip()
        if not lab or lab.lower().startswith(("select", "choose")):
            continue
        clean = re.sub(r"\s*-\s*Out\s*of\s*stock\s*$", "", lab, flags=re.I).strip(" -/")
        disabled = opt.has_attr("disabled") or (opt.get("aria-disabled") == "true") or "out of stock" in lab.lower()
        status = "æ— è´§" if disabled else "æœ‰è´§"
        entries.append((clean or lab, status))

    # 4) å…œåº•ï¼šå½¢ä¼¼å°ºç çš„æŒ‰é’®
    if not entries:
        for btn in soup.select("button, [role='option']"):
            lab = (btn.get_text() or getattr(btn, "get", lambda *_: None)("aria-label") or "").strip()
            if not lab: continue
            if re.search(r"\b\d{1,2}(\s*\([A-Z0-9]+\))?$", lab):
                disabled = hasattr(btn, "get") and (btn.get("disabled") is not None or btn.get("aria-disabled") == "true")
                status = "æ— è´§" if disabled else "æœ‰è´§"
                entries.append((lab, status))

    if not entries:
        return "No Data", "No Data"

    ordered = []
    seen = {}
    for label, status in entries:
        label = re.sub(r"\s+", " ", label).strip()
        if label not in seen or (seen[label] == "æ— è´§" and status == "æœ‰è´§"):
            seen[label] = status
            if label not in ordered: ordered.append(label)

    EAN = "0000000000000"
    product_size        = ";".join(f"{s}:{seen[s]}" for s in ordered) or "No Data"
    product_size_detail = ";".join(f"{s}:{3 if seen[s]=='æœ‰è´§' else 0}:{EAN}" for s in ordered) or "No Data"
    return product_size, product_size_detail

def _from_jsonld_product_new(soup: BeautifulSoup) -> Dict[str, Any]:
    """
    ä» JSON-LD æå–æœ€æ ¸å¿ƒçš„äº§å“å…ƒæ•°æ®ï¼ˆname/description/skuï¼‰ã€‚
    æ³¨æ„ï¼šæ­¤å‡½æ•°ä¸ parse_jsonld çš„å­—æ®µå‘½åå¯¹é½ï¼šname/description/skuã€‚
    """
    out = {"name": None, "description": None, "sku": None}

    # ç›´æ¥å¤ç”¨å·²å­˜åœ¨çš„ parse_jsonldï¼šæŠŠ soup è½¬æˆ html å†è§£æ
    try:
        html = str(soup)
        jd = parse_jsonld(html) or {}
    except Exception:
        jd = {}

    # parse_jsonld è¿”å›çš„æ˜¯ title/description/skuï¼Œè¿™é‡Œåšä¸€æ¬¡å­—æ®µå¯¹é½
    title = jd.get("title")
    if title:
        out["name"] = title
    if jd.get("description"):
        out["description"] = jd["description"]
    if jd.get("sku"):
        out["sku"] = jd["sku"]

    # å…œåº•ï¼šæ²¡æœ‰æ‹¿åˆ° name æ—¶ï¼Œç”¨ <h1> æˆ– <title>
    if not out["name"]:
        h1 = soup.select_one("h1,[data-testid*='title'],[data-component*='title']")
        out["name"] = h1.get_text(strip=True) if h1 else (soup.title.get_text(strip=True) if soup.title else None)

    return out




def _parse_price_string(txt: str) -> float | None:
    """
    è¾“å…¥ç±»ä¼¼ 'Â£189.00' / '189.00' / '18900' / '189'
    è¿”å› floatï¼Œæ¯”å¦‚ 189.0
    """
    if not txt:
        return None

    import re

    cleaned = txt.strip()
    # ä¼˜å…ˆï¼šç›´æ¥å¸¦Â£çš„ï¼Œæ¯”å¦‚ Â£189.00
    m_symbol = re.search(r"Â£\s*([0-9]+(?:\.[0-9]+)?)", cleaned)
    if m_symbol:
        return float(m_symbol.group(1))

    # å…¶æ¬¡ï¼šdata-testvalue="18900" è¿™ç§åˆ†é‡Œé¢æ˜¯åˆ†(pence)ï¼Œéœ€è¦/100
    m_pence = re.search(r"^([0-9]{3,})$", cleaned)
    if m_pence:
        try:
            pence_val = int(m_pence.group(1))
            return round(pence_val / 100.0, 2)
        except:
            pass

    # æœ€åå…œåº•ï¼šçº¯æ•°å­—å°æ•°
    m_plain = re.search(r"([0-9]+(?:\.[0-9]+)?)", cleaned)
    if m_plain:
        return float(m_plain.group(1))

    return None


def _extract_prices_new(soup: BeautifulSoup) -> tuple[str, str]:
    """
    è¿”å› (product_price_str, adjusted_price_str)
    - product_price_str => æˆ‘ä»¬TXTé‡Œçš„ Product Price (åŸä»·)
    - adjusted_price_str => æˆ‘ä»¬TXTé‡Œçš„ Adjusted Price (æŠ˜åä»· / æ²¡æ‰“æŠ˜åˆ™ 'No Data')
    """

    price_block = soup.select_one('p[data-testid="price"]')
    if not price_block:
        return ("No Data", "No Data")

    # 1. æŠ˜åä»· (ç°ä»·)
    #    åªæœ‰åœ¨æ‰“æŠ˜çš„æ—¶å€™æ‰ä¼šæœ‰ class åŒ…å« Price_isDiscounted__
    discounted_span = price_block.select_one("span[class*='Price_isDiscounted']")
    discounted_price = None
    if discounted_span:
        discounted_price = _parse_price_string(discounted_span.get_text(strip=True))

    # 2. åŸä»·
    #    åŸä»·é€šå¸¸åœ¨ data-testid="ticket-price"
    ticket_span = price_block.select_one('span[data-testid="ticket-price"]')
    ticket_price = None
    if ticket_span:
        ticket_price = _parse_price_string(ticket_span.get_text(strip=True))

    # 3. å¦‚æœæ²¡æœ‰ ticket-priceï¼Œå¯èƒ½æ˜¯æ²¡æ‰“æŠ˜ï¼Œ
    #    é‚£å°±ç›´æ¥çœ‹æ•´ä¸ª price_block è‡ªå·±çš„ data-testvalue æˆ–çº¯æ–‡æœ¬
    if ticket_price is None:
        # å°è¯• data-testvalue="17900"
        block_testvalue = price_block.get("data-testvalue")
        ticket_price = _parse_price_string(block_testvalue)

    if ticket_price is None:
        # å°è¯•æŠŠ <p> é‡Œé¢ç¬¬ä¸€ä¸ª span çš„æ–‡æœ¬å½“æˆåŸä»·
        first_span = price_block.find("span")
        if first_span:
            ticket_price = _parse_price_string(first_span.get_text(strip=True))

    # ç°åœ¨æˆ‘ä»¬æœ‰:
    #   discounted_price (å¯èƒ½ None)
    #   ticket_price (ä¸åº”è¯¥æ˜¯ None äº†ï¼Œé™¤éé¡µé¢çœŸçš„å‡ºé—®é¢˜)

    if discounted_price is not None and ticket_price is not None:
        # æœ‰æŠ˜æ‰£åœºæ™¯ï¼š
        # Product Price = åŸä»· (ticket_price)
        # Adjusted Price = æŠ˜åä»· (discounted_price)
        product_price_val = ticket_price
        adjusted_price_val = discounted_price
    else:
        # æ— æŠ˜æ‰£åœºæ™¯ï¼š
        # Product Price = å”¯ä¸€é‚£ä¸ªä»·
        # Adjusted Price = "No Data"
        product_price_val = ticket_price or discounted_price
        adjusted_price_val = None

    # æ ¼å¼åŒ–æˆå­—ç¬¦ä¸²ï¼›ä¿æŒä¸¤ä½å°æ•°æˆ– "No Data"
    product_price_str = (
        f"{product_price_val:.2f}" if product_price_val is not None else "No Data"
    )
    adjusted_price_str = (
        f"{adjusted_price_val:.2f}" if adjusted_price_val is not None else "No Data"
    )

    return product_price_str, adjusted_price_str




def _decide_gender_for_logic(sku: str, soup: BeautifulSoup, html: str, url: str) -> str:
    """
    è¿”å›æ ‡å‡†è‹±æ–‡æ€§åˆ«ï¼Œä¼˜å…ˆçº§ï¼š
    1. æ ¹æ® SKU çŒœ (æœ€ç¨³å®šï¼ŒLQUxxx = women, MQUxxx = men)
    2. å¦‚æœ SKU çŒœä¸åˆ°ï¼Œå†å°è¯•é¡µé¢ä¸Šçš„æ€§åˆ«æå– _extract_gender_new(...)
    3. å¦‚æœè¿˜çŒœä¸åˆ°ï¼Œè¿”å› "No Data"

    è¿”å›å€¼åªå¯èƒ½æ˜¯: "women", "men", "kids", "unisex", or "No Data"
    ï¼ˆæŒ‰ä½ ä»¬skuè§„åˆ™ä¸€èˆ¬å°±æ˜¯ men / women / No Dataï¼‰
    """

    # 1. SKU æ¨æ–­
    sku_guess = _infer_gender_from_code(sku or "")
    # å‡è®¾ _infer_gender_from_code è¿”å›ç±»ä¼¼ "women", "men", æˆ– "No Data"
    if sku_guess and sku_guess != "No Data":
        return sku_guess

    # 2. å…¶æ¬¡å°è¯•é¡µé¢
    page_guess = _extract_gender_new(soup, html, url)
    if page_guess and page_guess != "No Data":
        return page_guess

    # 3. å…œåº•
    return "No Data"




# ================== æ ¸å¿ƒè§£æ ==================
def parse_info_new(html: str, url: str, conn) -> Dict[str, Any]:
    """
    ç»Ÿä¸€å‡ºå£ï¼šè¿™é‡Œäº§å‡ºçš„ info å°±æ˜¯æœ€ç»ˆè¦å†™è¿› TXT çš„å†…å®¹ã€‚
    å…³é”®ç‚¹ï¼š
      1. æˆ‘ä»¬åœ¨è¿™é‡Œå°±ç¡®å®šæœ€ç»ˆ Product Codeï¼ˆç”¨ç¼“å­˜/DBåŒ¹é…ï¼‰ã€‚
      2. ç”¨æœ€ç»ˆ Product Code æ¨æ–­ gender_for_logicã€‚
      3. ç”¨ gender_for_logic ç”Ÿæˆå®Œæ•´å°ºç è¡¨ Product Size / Product Size Detailã€‚
      4. æŠŠ Gender ç›´æ¥è½¬æˆä¸­æ–‡ï¼ˆç”·æ¬¾/å¥³æ¬¾/...ï¼‰ã€‚

    å‚æ•°:
        html: å½“å‰å•†å“é¡µå®Œæ•´ HTML
        url:  å½“å‰å•†å“é¡µ URL
        conn: SQLAlchemy Connection (process_url_with_driver é‡Œä¼ è¿›æ¥çš„ conn)
    """
    soup = BeautifulSoup(html, "html.parser")

    # ---------- (A) åŸºç¡€é¡µé¢ä¿¡æ¯ï¼ˆä¸ä¾èµ–æ•°æ®åº“ï¼‰ ----------
    # ä» JSON-LD æŠŠåŸºç¡€å­—æ®µæŠ“å‡ºæ¥ï¼šname / description / sku ç­‰
    jd = _from_jsonld_product_new(soup) or {}
    title_guess = jd.get("name") or (soup.title.get_text(strip=True) if soup.title else "No Data")
    desc_guess  = jd.get("description") or "No Data"
    sku_guess   = jd.get("sku") or "No Data"

    # é¢œè‰²ï¼šæˆ‘ä»¬æœ‰ _extract_color_new (å…ˆçœ‹é¡µé¢ JSON é‡Œçš„colorï¼Œå†å…œåº• og:image:alt)
    color_guess = _extract_color_new(soup, html) or "No Data"

    # å°ºç åŸå§‹æŠ“å–ï¼ˆé¡µé¢ä¸Šæ‰€æœ‰å°ºç æŒ‰é’®ï¼Œå¸¦ stock>0 or 0ï¼‰
    raw_sizes = _extract_sizes_new(soup)  # dict like { "12(M)": {"stock":3}, "14(L)": {"stock":3}, ... }

    # ä»·æ ¼ï¼ˆ"Product Price" / "Adjusted Price" å£å¾„ï¼‰
    product_price_str, adjusted_price_str = _extract_prices_new(soup)

    # ---------- (B) åŸºäº URL / DB ç¡®å®šæœ€ç»ˆ Product Code ----------
    norm_url = _normalize_url(url)

    # 1. å…ˆè¯• URLâ†’Code ç¼“å­˜
    final_code = URL_CODE_CACHE.get(norm_url)

    # 2. å¦‚æœç¼“å­˜æ²¡æœ‰ï¼Œç”¨ DB æ¨¡ç³ŠåŒ¹é… (match_product + choose_best)
    if not final_code:
        # match_product éœ€è¦åŸå§‹è¿æ¥
        raw_conn = get_dbapi_connection(conn)

        results = match_product(
            raw_conn,
            scraped_title=title_guess or "",
            scraped_color=color_guess or "",
            table=PRODUCTS_TABLE,
            name_weight=NAME_WEIGHT,
            color_weight=COLOR_WEIGHT,
            type_weight=(1.0 - NAME_WEIGHT - COLOR_WEIGHT),
            topk=5,
            recall_limit=2000,
            min_name=0.92,
            min_color=0.85,
            require_color_exact=False,
            require_type=False,
        )
        chosen = choose_best(results, min_score=MIN_SCORE, min_lead=MIN_LEAD)
        if chosen:
            final_code = chosen

    # 3. å¦‚æœè¿˜æ˜¯æ‹¿ä¸åˆ°ï¼Œå°±é€€å› JSON-LD é‡Œçš„ sku_guess
    if not final_code:
        final_code = sku_guess if sku_guess and sku_guess != "No Data" else "No Data"

    # ---------- (C) ç”¨æœ€ç»ˆ code åˆ¤æ–­æ€§åˆ« ----------
    # æˆ‘ä»¬ç°åœ¨ç”¨çš„æ˜¯æœ€ç»ˆ codeï¼Œè€Œä¸æ˜¯ä¸€å¼€å§‹çš„ sku_guess
    gender_for_logic = _decide_gender_for_logic(final_code, soup, html, url)
    # _decide_gender_for_logic ä¼šï¼š
    #   1. ç”¨ _infer_gender_from_code(final_code) åˆ¤æ–­ men/women
    #   2. å¦‚æœè¿˜ä¸è¡Œï¼Œå†çœ‹é¡µé¢ JSON/DOM
    #   3. å®åœ¨ä¸è¡Œæ‰ "No Data"

    # ---------- (D) ç”¨æ€§åˆ«ç”Ÿæˆæœ€ç»ˆå°ºç ä¸² ----------
    # æ³¨æ„ï¼š_finalize_sizes_for_hof ä¼šï¼š
    #   - æ¸…æ´—å„ç±»ä¹±ä¸ƒå…«ç³Ÿçš„å°ºç  ("12(M)" -> "12", "XL" -> "XL", "32" -> "32")
    #   - æ ¹æ® gender_for_logic é€‰æ•´å¥—å°ºç å…¨é›†ï¼ˆå¥³æ¬¾=4..20ï¼›ç”·æ¬¾=2XS..3XL æˆ– 30..50ï¼‰
    #   - ç»™æ²¡å‡ºç°çš„å°ºç è¡¥ "æ— è´§" / 0:0000000000000
    product_size_str, product_size_detail_str = _finalize_sizes_for_hof(raw_sizes, gender_for_logic)

    # ---------- (E) æŠŠ gender_for_logic å˜æˆä¸­æ–‡å±•ç¤º ----------
    def _gender_to_display(g: str) -> str:
        if g == "women":
            return "å¥³æ¬¾"
        if g == "men":
            return "ç”·æ¬¾"
        if g == "kids":
            return "ç«¥æ¬¾"
        if g == "unisex":
            return "ä¸­æ€§æ¬¾"
        return "No Data"

    gender_display = _gender_to_display(gender_for_logic)

    # ---------- (F) ç»„è£…æœ€ç»ˆ info ----------
    info = {
        "Product Code":        final_code or "No Data",
        "Product Name":        title_guess or "No Data",
        "Product Description": desc_guess or "No Data",
        "Product Gender":      gender_display or "No Data",
        "Product Color":       color_guess or "No Data",

        "Product Price":       product_price_str,
        "Adjusted Price":      adjusted_price_str,

        # æš‚æ—¶æ²¡åšæè´¨ç»†åˆ†ï¼Œä¿æŒåŸè¡Œä¸º
        "Product Material":    "No Data",

        # ä½ åŸæœ¬ç¡¬ç¼–ç äº† "casual wear"ï¼Œä¿æŒä¸å˜ï¼Œé¿å…ä¸‹æ¸¸å´©
        "Style Category":      "casual wear",

        # ä½ åŸä»£ç é‡Œå†™çš„æ˜¯ "Feature": "No Data"
        "Feature":             "No Data",

        "Product Size":        product_size_str,
        "Product Size Detail": product_size_detail_str,

        "Source URL":          url,
        "Site Name":           SITE_NAME,
    }

    return info








# ================== Selenium åŸºç¡€ ==================
def get_driver(headless: bool = False):
    """
    ä½¿ç”¨ TaobaoProj çš„é€šç”¨è‡ªåŠ¨é©±åŠ¨ï¼šbuild_uc_driver
    - è‡ªåŠ¨æ£€æµ‹æœ¬æœº Chrome ä¸»ç‰ˆæœ¬
    - è‡ªåŠ¨é€‰æ‹©æ­£ç¡®é©±åŠ¨
    - è‡ªåŠ¨æ¸…é™¤ uc ç¼“å­˜
    - æ›´ç¨³å®šï¼Œé¿å… WinError 10060 / session not created
    """
    extra = [
        "--disable-blink-features=AutomationControlled",
        "--lang=en-GB",
        "accept-language=en-GB,en-US;q=0.9,en;q=0.8",
    ]
    return build_uc_driver(headless=headless, extra_options=extra, verbose=True)


def fetch_product_info_with_single_driver(driver, url: str) -> Dict[str, Any]:
    driver.get(url)
    ok = wait_pdp_ready(driver, timeout=WAIT_HYDRATE_SECONDS)
    if not ok:
        html = driver.page_source or ""
        _dump_debug_html(html, url, tag="timeout_debug")
        return parse_info_new(html, url)
    _soft_scroll(driver, steps=6, pause=0.4)
    html = driver.page_source or ""
    _dump_debug_html(html, url, tag="debug_new")
    return parse_info_new(html, url)

# ================== å¤„ç†å•ä¸ª URL ==================
def process_url_with_driver(driver, url: str, conn: Connection, delay: float = DEFAULT_DELAY) -> Path | None:
    """
    æ‰“å¼€å•ä¸ª URLï¼Œè§£æå‡º infoï¼ˆå·²ç»åŒ…å«æœ€ç»ˆ Product Code / Gender / å°ºç ç­‰ï¼‰ï¼Œ
    ç„¶åå†™å…¥ TXTã€‚

    ç›¸æ¯”æ—§ç‰ˆæœ¬ï¼š
    - ä¸å†è¿™é‡Œåš URLâ†’Code åŒ¹é…ã€gender å…œåº•ï¼Œè¿™äº›éƒ½æå‰æ”¾è¿›äº† parse_info_newã€‚
    - ä¸å†é‡å¤ _gender_to_cnï¼Œå› ä¸º parse_info_new é‡Œå·²ç»æŠŠæ€§åˆ«è½¬æˆ "ç”·æ¬¾"/"å¥³æ¬¾"ã€‚
    """

    print(f"\nğŸŒ æ­£åœ¨æŠ“å–: {url}")

    # 1. æ‰“å¼€é¡µé¢å¹¶ç­‰å¾…æ¸²æŸ“å®Œæˆ
    driver.get(url)
    ok = wait_pdp_ready(driver, timeout=WAIT_HYDRATE_SECONDS)
    if not ok:
        # é¡µé¢å¯èƒ½è¿˜æ²¡å®Œå…¨hydratedï¼Œæˆ‘ä»¬ä»ç„¶æŠ“å½“å‰HTMLåšå…œåº•
        html = driver.page_source or ""
        _dump_debug_html(html, url, tag="timeout_debug")
    else:
        # é¡µé¢ ready çš„æƒ…å†µä¸‹å¤šæ»šå‡ ä¸‹ï¼Œç¡®ä¿å°ºç /ä»·æ ¼ç­‰JSæ¸²æŸ“å‡ºæ¥
        _soft_scroll(driver, steps=6, pause=0.4)
        html = driver.page_source or ""
        _dump_debug_html(html, url, tag="debug_new")

    # 2. ç›´æ¥ç”¨æ–°çš„ parse_info_new è§£æå®Œæ•´ä¿¡æ¯ï¼ˆå«æœ€ç»ˆcode/æ€§åˆ«/å°ºç ï¼‰
    info = parse_info_new(html, url, conn)

    # 3. é€‰è¾“å‡ºæ–‡ä»¶å
    code_for_filename = info.get("Product Code") or "NoDataCode"
    code_for_filename = code_for_filename.strip() or "NoDataCode"
    safe_code_for_filename = _safe_name(code_for_filename)

    # å¦‚æœçœŸçš„æ²¡æ‹¿åˆ°æ ‡å‡†codeï¼ˆè¿˜æ˜¯ "No Data"ï¼‰ï¼Œæˆ‘ä»¬ fallback ç”¨hash+å•†å“åç”Ÿæˆæ–‡ä»¶å
    if safe_code_for_filename in ("NoDataCode", "No_Data", "NoData", "No", ""):
        short = f"{abs(hash(url)) & 0xFFFFFFFF:08x}"
        safe_name_part = _safe_name(info.get("Product Name") or "BARBOUR")
        out_path = TXT_DIR / f"{safe_name_part}_{short}.txt"
    else:
        out_path = TXT_DIR / f"{safe_code_for_filename}.txt"

    # 4. å†™ TXT (ä¿æŒä½ åŸæ¥çš„å­—æ®µé¡ºåºå’Œæ ¼å¼)
    payload = _kv_txt_bytes(info)
    ok_write = _atomic_write_bytes(payload, out_path)

    if ok_write:
        print(f"âœ… å†™å…¥: {out_path} (code={info.get('Product Code')})")
    else:
        print(f"â— æ”¾å¼ƒå†™å…¥: {out_path.name}")

    # 5. å°å»¶è¿Ÿï¼Œé˜²æ­¢è¢«é£æ§
    if delay > 0:
        time.sleep(delay)

    return out_path


# ================== ä¸»å…¥å£ ==================
def houseoffraser_fetch_info(max_workers: int = MAX_WORKERS_DEFAULT, delay: float = DEFAULT_DELAY, headless: bool = False):
    links_file = Path(LINKS_FILE)
    if not links_file.exists():
        print(f"âš  æ‰¾ä¸åˆ°é“¾æ¥æ–‡ä»¶ï¼š{links_file}")
        return
    raw_urls = [line.strip() for line in links_file.read_text(encoding="utf-8", errors="ignore").splitlines()
                if line.strip() and not line.strip().startswith("#")]

    # è§„èŒƒåŒ–å»é‡ï¼ˆä¿åºï¼‰
    seen = set(); urls = []
    for u in raw_urls:
        nu = _normalize_url(u)
        if nu in seen: continue
        seen.add(nu); urls.append(u)

    total = len(urls)
    print(f"ğŸ“„ å…± {total} ä¸ªå•†å“é¡µé¢å¾…è§£æ...ï¼ˆå¹¶å‘ {max_workers}ï¼‰")
    if total == 0: return

    engine_url = f"postgresql+psycopg2://{PG['user']}:{PG['password']}@{PG['host']}:{PG['port']}/{PG['dbname']}"
    engine = create_engine(engine_url)

    # æ„å»º URLâ†’Code ç¼“å­˜
    with engine.begin() as conn:
        raw = get_dbapi_connection(conn)
        build_url_code_cache(raw, PRODUCTS_TABLE, OFFERS_TABLE, SITE_NAME)

    # å•å®ä¾‹æµè§ˆå™¨ï¼šé¦–ä¸ªå•†å“é¡µå…ˆç»™ä½ 10ç§’ç‚¹ Cookie
    driver = get_driver(headless=headless)
    try:
        if urls:
            print("ğŸ•’ å°†æ‰“å¼€é¦–ä¸ªå•†å“é¡µã€‚è¯·åœ¨ 10 ç§’å†…æ‰‹åŠ¨ç‚¹å‡» Cookie çš„ 'Allow all' æŒ‰é’®...")
            driver.get(urls[0])
            time.sleep(10)
            print("âœ… å·²ç­‰å¾… 10 ç§’ï¼Œå¼€å§‹æ­£å¼æŠ“å–")

        ok, fail = 0, 0
        with engine.begin() as conn:
            for idx, u in enumerate(urls, start=1):
                print(f"[å¯åŠ¨] [{idx}/{total}] {u}")
                try:
                    path = process_url_with_driver(driver, u, conn=conn, delay=delay)
                    ok += 1 if path else 0
                    print(f"[å®Œæˆ] [{idx}/{total}] {u} -> {path}")
                except Exception as e:
                    fail += 1
                    print(f"[å¤±è´¥] [{idx}/{total}] âŒ {u}\n    {repr(e)}")

        print(f"\nğŸ“¦ ä»»åŠ¡ç»“æŸï¼šæˆåŠŸ {ok}ï¼Œå¤±è´¥ {fail}ï¼Œæ€»è®¡ {total}")

    finally:
        try: driver.quit()
        except Exception: pass


if __name__ == "__main__":
    houseoffraser_fetch_info()
