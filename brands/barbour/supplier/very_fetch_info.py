# -*- coding: utf-8 -*-
"""
Very | Barbour å•†å“æŠ“å–ï¼ˆä¸ houseoffraser_fetch_info ç»“æ„å¯¹é½ï¼‰
- è§£æï¼šæ ‡é¢˜ã€æè¿°ã€é¢œè‰²ã€åŸä»·ã€æŠ˜æ‰£ä»·ã€å°ºç åº“å­˜ï¼ˆæœ‰è´§/æ— è´§ï¼‰â†’ TXT
- ç›¸ä¼¼åº¦åŒ¹é… barbour_productsï¼ˆæ ‡é¢˜+é¢œè‰²ï¼‰ï¼Œå‘½ä¸­åˆ™ç”¨ç¼–ç å‘½å TXT
- æ”¯æŒå¹¶å‘ä¸åŸå­å†™å…¥ï¼›å­—æ®µ/æ ¼å¼ä¸ HOF ç‰ˆä¸€è‡´
"""

from __future__ import annotations

import os
import json
import time
import tempfile
import threading
from pathlib import Path
from typing import Optional, Tuple, List, Dict, Any
from concurrent.futures import ThreadPoolExecutor, as_completed

# ===== ç¬¬ä¸‰æ–¹ä¸è§£æ =====
from bs4 import BeautifulSoup
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# ç»Ÿä¸€ Selenium é©±åŠ¨
from common_taobao.core.selenium_utils import get_driver as selenium_get_driver, quit_driver

# ===== DB ä¸é¡¹ç›®é…ç½® =====
from sqlalchemy import create_engine
from sqlalchemy.engine import Connection
from config import BARBOUR, BRAND_CONFIG
from brands.barbour.core.site_utils import assert_site_or_raise as canon
from brands.barbour.core.sim_matcher import match_product, choose_best, explain_results

import re  # safe filename ç­‰ç”¨åˆ°

# ================== ç«™ç‚¹ä¸ç›®å½• ==================
SITE_NAME = canon("very")
LINKS_FILE: str = BARBOUR["LINKS_FILES"]["very"]
TXT_DIR: Path = BARBOUR["TXT_DIRS"]["very"]
TXT_DIR.mkdir(parents=True, exist_ok=True)

PRODUCTS_TABLE: str = BRAND_CONFIG.get("barbour", {}).get("PRODUCTS_TABLE", "barbour_products")
PG = BRAND_CONFIG["barbour"]["PGSQL_CONFIG"]

# ================== å¯è°ƒå‚æ•° ==================
WAIT_PRICE_SECONDS = 8           # ç­‰ä»·é¢ä»·æ¨¡å—çš„æœ€é•¿ç­‰å¾…ï¼ˆç§’ï¼‰
DEFAULT_DELAY = 2.0              # æ‰“å¼€é¡µé¢åçš„ç¼“å†²ç­‰å¾…ï¼ˆç§’ï¼‰
MAX_WORKERS_DEFAULT = 4          # å¹¶å‘æ•°
MIN_SCORE = 0.72                 # ç›¸ä¼¼åº¦é˜ˆå€¼
MIN_LEAD = 0.04                  # é¢†å…ˆå¹…åº¦é˜ˆå€¼ï¼ˆTop1 ä¸ Top2 å·®å€¼ï¼‰
NAME_WEIGHT = 0.75               # åç§°æƒé‡
COLOR_WEIGHT = 0.25              # é¢œè‰²æƒé‡

# ===== æ ‡å‡†å°ºç è¡¨ï¼ˆç”¨äºè¡¥é½æœªå‡ºç°å°ºç =0ï¼‰ =====
WOMEN_ORDER = ["4","6","8","10","12","14","16","18","20"]
MEN_ALPHA_ORDER = ["2XS","XS","S","M","L","XL","2XL","3XL"]
MEN_NUM_ORDER = [str(n) for n in range(30, 52, 2)]  # 30..50ï¼ˆä¸å«52ï¼‰

def _full_order_for_gender(gender: str) -> list[str]:
    """æ ¹æ®æ€§åˆ«è¿”å›å®Œæ•´å°ºç é¡ºåºï¼›æœªçŸ¥/ç«¥æ¬¾å…ˆæŒ‰ç”·æ¬¾å¤„ç†ã€‚"""
    g = (gender or "").lower()
    if "å¥³" in g or "women" in g or "ladies" in g:
        return WOMEN_ORDER
    return MEN_ALPHA_ORDER + MEN_NUM_ORDER


# ================== å¹¶å‘å»é‡ + åŸå­å†™ ==================
_WRITTEN: set[str] = set()
_WRITTEN_LOCK = threading.Lock()

def _atomic_write_bytes(data: bytes, dst: Path, retries: int = 6, backoff: float = 0.25) -> bool:
    dst.parent.mkdir(parents=True, exist_ok=True)
    for i in range(retries):
        tmp = None
        try:
            with tempfile.NamedTemporaryFile(
                delete=False, dir=str(dst.parent), prefix=".tmp_", suffix=f".{os.getpid()}.{threading.get_ident()}"
            ) as tf:
                tmp = Path(tf.name)
                tf.write(data)
                tf.flush()
                os.fsync(tf.fileno())
            try:
                os.replace(tmp, dst)
            finally:
                if tmp and tmp.exists():
                    try: tmp.unlink(missing_ok=True)
                    except Exception: pass
            return True
        except (PermissionError, FileExistsError, OSError):
            if dst.exists():
                return True
            time.sleep(backoff * (i + 1))
            try:
                if tmp and tmp.exists():
                    tmp.unlink(missing_ok=True)
            except Exception:
                pass
        except Exception:
            time.sleep(backoff * (i + 1))
    return dst.exists()

def _kv_txt_bytes(info: Dict[str, Any]) -> bytes:
    fields = [
        "Product Code","Product Name","Product Description","Product Gender",
        "Product Color","Product Price","Adjusted Price","Product Material",
        "Style Category","Feature","Product Size","Product Size Detail",
        "Source URL","Site Name"
    ]
    lines = [f"{k}: {info.get(k, 'No Data')}" for k in fields]
    return ("\n".join(lines) + "\n").encode("utf-8", errors="ignore")

def _safe_name(s: str) -> str:
    return re.sub(r"[\\/:*?\"<>|'\s]+", "_", s or "NoName")

def get_dbapi_connection(conn_or_engine):
    if hasattr(conn_or_engine, "cursor"):
        return conn_or_engine
    if hasattr(conn_or_engine, "raw_connection"):
        return conn_or_engine.raw_connection()
    c = getattr(conn_or_engine, "connection", None)
    if c is not None:
        dbapi = getattr(c, "dbapi_connection", None)
        if dbapi is not None and hasattr(dbapi, "cursor"):
            return dbapi
        inner = getattr(c, "connection", None)
        if inner is not None and hasattr(inner, "cursor"):
            return inner
        if hasattr(c, "cursor"):
            return c
    return conn_or_engine

# ================== å·¥å…·å‡½æ•° ==================

def _choose_full_order_for_gender(gender: str, present: set[str]) -> list[str]:
    """ç”·æ¬¾åœ¨ã€å­—æ¯ç³»(2XSâ€“3XL)ã€‘ä¸ã€æ•°å­—ç³»(30â€“50,ä¸å«52)ã€‘äºŒé€‰ä¸€ï¼›å¥³æ¬¾å›ºå®š 4â€“20ã€‚"""
    g = (gender or "").lower()
    if "å¥³" in g or "women" in g or "ladies" in g:
        return WOMEN_ORDER[:]  # å¥³æ¬¾å›ºå®š 4..20

    has_num   = any(k in MEN_NUM_ORDER   for k in present)
    has_alpha = any(k in MEN_ALPHA_ORDER for k in present)
    if has_num and not has_alpha:
        return MEN_NUM_ORDER[:]          # åªç”¨æ•°å­—ç³» 30..50
    if has_alpha and not has_num:
        return MEN_ALPHA_ORDER[:]        # åªç”¨å­—æ¯ç³» 2XS..3XL
    if has_num or has_alpha:
        num_count   = sum(1 for k in present if k in MEN_NUM_ORDER)
        alpha_count = sum(1 for k in present if k in MEN_ALPHA_ORDER)
        return MEN_NUM_ORDER[:] if num_count >= alpha_count else MEN_ALPHA_ORDER[:]
    # å®åœ¨åˆ¤ä¸å‡ºæ¥ï¼šé»˜è®¤ç”¨å­—æ¯ç³»ï¼ˆå¤–å¥—æ›´å¸¸è§ï¼‰
    return MEN_ALPHA_ORDER[:]


def _clean(s: Optional[str]) -> str:
    return re.sub(r"\s+", " ", (s or "").strip())

def _json_from_script(html: str, var_name: str) -> Optional[dict]:
    """
    ä» window.__product_body_initial_state__= {...}; æŠ½ JSON
    """
    pat = re.compile(rf"{re.escape(var_name)}\s*=\s*({{.*?}})\s*;", re.S)
    m = pat.search(html)
    if not m:
        return None
    blob = m.group(1)
    try:
        return json.loads(blob)
    except Exception:
        try:
            blob2 = re.sub(r",\s*([}\]])", r"\1", blob)
            return json.loads(blob2)
        except Exception:
            return None

def _extract_title(soup: BeautifulSoup, initial: Optional[dict]) -> str:
    if initial and initial.get("name"):
        return _clean(initial["name"])
    t = _clean(soup.title.get_text()) if soup.title else "No Data"
    t = re.sub(r"\s*\|\s*Very\s*$", "", t, flags=re.I)
    return t or "No Data"

def _extract_desc(soup: BeautifulSoup, productData: Optional[dict]) -> str:
    m = soup.find("meta", attrs={"property": "og:description"})
    if m and m.get("content"):
        return _clean(m["content"])
    if productData and productData.get("description"):
        return _clean(productData["description"])
    return "No Data"

def _extract_color(initial: Optional[dict]) -> str:
    if initial and isinstance(initial.get("skus"), list):
        for sku in initial["skus"]:
            opts = sku.get("options") or {}
            col = opts.get("colour")
            if col:
                return _clean(str(col))
    return "No Data"

def _extract_gender(title: str, soup: BeautifulSoup, productData: Optional[dict]) -> str:
    t = (title or "").lower()
    if "women" in t or "ladies" in t: return "å¥³æ¬¾"
    if "men" in t: return "ç”·æ¬¾"
    if productData:
        dept = (productData.get("subcategory") or "") + " " + (productData.get("category") or "") + " " + (productData.get("department") or "")
        d = dept.lower()
        if any(k in d for k in ["ladies","women","women's"]): return "å¥³æ¬¾"
        if "men" in d: return "ç”·æ¬¾"
    return "No Data"

def _to_num(s: Optional[str]) -> Optional[float]:
    if s is None:
        return None
    m = re.search(r"([0-9]+(?:\.[0-9]{1,2})?)", str(s).replace(",", ""))
    return float(m.group(1)) if m else None

def _extract_prices(initial: Optional[dict], soup: BeautifulSoup) -> Tuple[Optional[float], Optional[float]]:
    """
    è¿”å› (curr, orig) -> (æŠ˜åä»·, åŸä»·)
    Very çš„ initial_state.price.amount.decimal/previous
    """
    if initial:
        price = (initial.get("price") or {}).get("amount") or {}
        curr = _to_num(price.get("decimal") or price.get("current"))
        orig = _to_num(price.get("previous")) or curr
        if curr is not None:
            return curr, (orig if orig is not None else curr)
    m_curr = soup.find("meta", attrs={"property": "product:price:amount"})
    if m_curr and m_curr.get("content"):
        curr = _to_num(m_curr["content"])
        return curr, curr
    return (None, None)

def _extract_sizes_and_stock(initial: Optional[dict], soup: BeautifulSoup) -> List[Tuple[str, str]]:
    """
    è¿”å› [(size, æœ‰è´§/æ— è´§), ...]
    ä¼˜å…ˆç”¨ skus[].stock.codeï¼›å¤±è´¥å†çœ‹ DOM é‡Œçš„ checkbox disabled
    """
    pairs: List[Tuple[str, str]] = []

    # 1) JSON ä¼˜å…ˆ
    if initial and isinstance(initial.get("skus"), list):
        for sku in initial["skus"]:
            opts = sku.get("options") or {}
            size = _clean(str(opts.get("size") or ""))
            if not size:
                continue
            stock = (sku.get("stock") or {}).get("code") or ""
            status = "æ— è´§"
            if stock:
                status = "æœ‰è´§" if stock.upper() in {"DCSTOCK", "IN_STOCK", "AVAILABLE"} else "æ— è´§"
            pairs.append((size, status))

    # 2) DOM å›é€€
    if not pairs:
        for inp in soup.select('input[id^="size-"][type="checkbox"]'):
            size = _clean((inp.get("id") or "").replace("size-", ""))
            if not size:
                continue
            disabled = inp.has_attr("disabled") or inp.get("aria-disabled") == "true"
            status = "æ— è´§" if disabled else "æœ‰è´§"
            pairs.append((size, status))

    # å°ºç è§„èŒƒåŒ–
    _alpha_canon = {
        "XXXS":"2XS","2XS":"2XS","XXS":"XS","XS":"XS",
        "S":"S","SMALL":"S","M":"M","MEDIUM":"M","L":"L","LARGE":"L",
        "XL":"XL","X-LARGE":"XL","XXL":"2XL","2XL":"2XL","XXXL":"3XL","3XL":"3XL",
    }
    normed: List[Tuple[str, str]] = []
    for s, st in pairs:
        s2 = re.sub(r"\s*\(.*?\)\s*", "", s).strip()
        s2 = re.sub(r"^(UK|EU|US)\s+", "", s2, flags=re.I)
        s2u = s2.upper().replace("-", "").strip()
        if s2u in _alpha_canon:
            s2 = _alpha_canon[s2u]
        normed.append((s2, st))
    return normed


def _build_size_lines(pairs: List[Tuple[str, str]], gender: str) -> Tuple[str, str]:
    """
    å°†å‡ºç°çš„å°ºç æŒ‰â€œæœ‰è´§ä¼˜å…ˆâ€åˆå¹¶ï¼Œå¹¶è¡¥é½æœªå‡ºç°çš„å°ºç ä¸º æ— è´§/0ã€‚
    - Product Size:         34:æœ‰è´§;36:æ— è´§;...
    - Product Size Detail:  34:3:000...;36:0:000...;...
    âœ… ç”·æ¬¾ï¼šäºŒé€‰ä¸€ï¼ˆå­—æ¯ç³» æˆ– æ•°å­—ç³»ï¼‰ï¼Œç»ä¸æ··ç”¨
    âœ… å¥³æ¬¾ï¼šå›ºå®š 4â€“20
    """
    by_size: Dict[str, str] = {}

    for size, status in (pairs or []):
        prev = by_size.get(size)
        if prev is None or (prev == "æ— è´§" and status == "æœ‰è´§"):
            by_size[size] = status

    present_keys = set(by_size.keys())
    full_order = _choose_full_order_for_gender(gender, present_keys)

    for k in list(by_size.keys()):
        if k not in full_order:
            by_size.pop(k, None)

    for s in full_order:
        if s not in by_size:
            by_size[s] = "æ— è´§"

    EAN = "0000000000000"
    ordered = list(full_order)
    ps  = ";".join(f"{k}:{by_size[k]}" for k in ordered) or "No Data"
    psd = ";".join(f"{k}:{3 if by_size[k]=='æœ‰è´§' else 0}:{EAN}" for k in ordered) or "No Data"
    return ps, psd


def _guess_material(desc: str) -> str:
    if not desc or desc == "No Data":
        return "No Data"
    m = re.search(r"Material\s*Content\s*:\s*(.+?)(?:$|Washing|Wash|Care)", desc, flags=re.I)
    if m:
        return _clean(m.group(1))
    return "No Data"


def _split_title_color(title: str) -> tuple[str, str | None]:
    t = (title or "").strip()
    if not t:
        return "No Data", None
    parts = [p.strip() for p in re.split(r"\s*-\s*", t) if p.strip()]
    if len(parts) >= 2:
        raw_color = parts[-1]
        color = re.split(r"[\/&]", re.sub(r"[^\w\s/&-]", "", raw_color))[0].strip()
        color = color.title() if color else None
        clean_title = " - ".join(parts[:-1])
        return (clean_title or t, color or None)
    return t, None


def parse_info(html: str, url: str) -> Dict[str, Any]:
    soup = BeautifulSoup(html, "html.parser")

    initial = _json_from_script(html, "window.__product_body_initial_state__")
    productData = None  # æš‚ä¸å¼ºä¾èµ– mergeIntoDataLayer

    title = _extract_title(soup, initial)
    desc  = _extract_desc(soup, productData)
    color = _extract_color(initial)

    t2, color_from_title = _split_title_color(title)
    if not color or color.lower() == "no data":
        color = color_from_title or "No Data"
    title = t2

    gender = _extract_gender(title, soup, productData)
    curr, orig = _extract_prices(initial, soup)
    if curr is None and orig is not None: curr = orig
    if orig is None and curr is not None: orig = curr

    size_pairs = _extract_sizes_and_stock(initial, soup)
    product_size, product_size_detail = _build_size_lines(size_pairs, gender)

    material = _guess_material(desc)

    info = {
        "Product Code": "No Data",
        "Product Name": title or "No Data",
        "Product Description": desc or "No Data",
        "Product Gender": gender,
        "Product Color": color or "No Data",
        "Product Price": f"{orig:.2f}" if orig else "No Data",
        "Adjusted Price": f"{curr:.2f}" if curr else "No Data",
        "Product Material": material,
        "Style Category": "casual wear",
        "Feature": "No Data",
        "Product Size": product_size,
        "Product Size Detail": product_size_detail,
        "Source URL": url,
        "Site Name": SITE_NAME,
    }
    return info


# ================== Selenium æŠ“å–å•é¡µ ==================

def process_url(url: str, conn: Connection, delay: float = DEFAULT_DELAY, driver=None) -> Path:
    """
    å•ä¸ªå•†å“é¡µé¢æŠ“å– + è§£æ + å†™ TXTã€‚
    æ³¨æ„ï¼šdriver å¿…é¡»ç”±å¤–éƒ¨åˆ›å»ºå¹¶ä¼ å…¥ï¼ˆç»Ÿä¸€ç”±çº¿ç¨‹ worker ç®¡ç†å…³é—­ï¼‰ã€‚
    """
    if driver is None:
        raise ValueError("process_url ç°åœ¨è¦æ±‚æ˜¾å¼ä¼ å…¥ driverï¼Œè¯·ä» selenium_utils.get_driver åˆ›å»ºã€‚")

    print(f"\nğŸŒ æ­£åœ¨æŠ“å–: {url}")
    driver.get(url)
    if WAIT_PRICE_SECONDS > 0:
        try:
            WebDriverWait(driver, WAIT_PRICE_SECONDS).until(
                EC.presence_of_element_located((By.TAG_NAME, "title"))
            )
        except Exception:
            pass
    if delay > 0:
        time.sleep(delay)
    html = driver.page_source

    info = parse_info(html, url)

    # ============= ç›¸ä¼¼åº¦åŒ¹é…ï¼ˆä¸ HOF ä¸€è‡´ï¼‰ =============
    raw_conn = get_dbapi_connection(conn)
    title = info.get("Product Name") or ""
    color = info.get("Product Color") or ""

    print("title:::::::::::::" + title)
    print("color:::::::::::::" + color)

    results = match_product(
        raw_conn,
        scraped_title=title,
        scraped_color=color,
        table=PRODUCTS_TABLE,
        name_weight=0.72,
        color_weight=0.18,
        type_weight=0.10,
        topk=20,
        recall_limit=5000,
        min_name=None,
        min_color=None,
        require_color_exact=False,
        require_type=False,
    )
    code = choose_best(results, min_score=MIN_SCORE, min_lead=MIN_LEAD)
    if not code and results:
        st = results[0].get("type_scraped")
        if st:
            for r in results:
                if r.get("type_db") == st:
                    code = r["product_code"]
                    print(f"ğŸ¯ tie-break by type â†’ {code}")
                    break

    print("ğŸ” match debug")
    print(f"  raw_title: {title}")
    print(f"  raw_color: {color}")
    txt, why = explain_results(results, min_score=MIN_SCORE, min_lead=MIN_LEAD)
    print(txt)
    if code:
        print(f"  â‡’ âœ… choose_best = {code}")
    else:
        print(f"  â‡’ âŒ no match ({why})")
        if results:
            print("ğŸ§ª top:", " | ".join(f"{r['product_code']}[{r['score']:.3f}]" for r in results[:3]))

    # å‘½åï¼šä¼˜å…ˆç”¨ç¼–ç 
    if code:
        info["Product Code"] = code
        out_name = f"{code}.txt"
    else:
        short = f"{abs(hash(url)) & 0xFFFF:04x}"
        out_name = f"{_safe_name(title)}_{short}.txt"

    out_path = TXT_DIR / out_name

    # å¹¶å‘å»é‡
    with _WRITTEN_LOCK:
        if out_name in _WRITTEN:
            print(f"â†©ï¸  è·³è¿‡é‡å¤å†™å…¥ï¼š{out_name}")
            return out_path
        _WRITTEN.add(out_name)

    payload = _kv_txt_bytes(info)
    ok = _atomic_write_bytes(payload, out_path)
    if ok:
        print(f"âœ… å†™å…¥: {out_path.name} (code={info.get('Product Code')})")
    else:
        print(f"â— æ”¾å¼ƒå†™å…¥: {out_path.name}")
    return out_path


# ================== å¤šçº¿ç¨‹å°è£…ï¼ˆç»Ÿä¸€æ¡†æ¶ï¼‰ ==================

def _process_single_url(idx: int, total: int, url: str, engine, delay: float, headless: bool):
    """
    å• URL çš„çº¿ç¨‹ä»»åŠ¡ï¼š
      - ä¸ºè¯¥ä»»åŠ¡åˆ›å»ºä¸€ä¸ªä¸“ç”¨ driverï¼ˆselenium_utilsï¼‰
      - æ‰“å¼€ DB äº‹åŠ¡ï¼Œè°ƒç”¨ process_url
      - å…³é—­ driver
    è¿”å›: (idx, url, path_str | None, error | None)
    """
    driver_name = f"very_{idx}"
    driver = None
    try:
        driver = selenium_get_driver(
            name=driver_name,
            headless=headless,
            window_size="1200,2000",
        )
        print(f"[å¯åŠ¨] [{idx}/{total}] {url}")
        with engine.begin() as conn:
            path = process_url(url, conn=conn, delay=delay, driver=driver)
        return idx, url, str(path), None
    except Exception as e:
        print(f"[å¤±è´¥] [{idx}/{total}] âŒ {url}\n    {repr(e)}")
        return idx, url, None, e
    finally:
        if driver is not None:
            try:
                quit_driver(driver_name)
            except Exception:
                pass


def very_fetch_info(max_workers: int = MAX_WORKERS_DEFAULT, delay: float = DEFAULT_DELAY, headless: bool = False):
    links_file = Path(LINKS_FILE)
    if not links_file.exists():
        print(f"âš  æ‰¾ä¸åˆ°é“¾æ¥æ–‡ä»¶ï¼š{links_file}")
        return

    urls = [
        line.strip()
        for line in links_file.read_text(encoding="utf-8", errors="ignore").splitlines()
        if line.strip() and not line.strip().startswith("#")
    ]
    total = len(urls)
    print(f"ğŸ“„ å…± {total} ä¸ªå•†å“é¡µé¢å¾…è§£æ...ï¼ˆå¹¶å‘ {max_workers}ï¼‰")
    if total == 0:
        return

    engine_url = (
        f"postgresql+psycopg2://{PG['user']}:{PG['password']}"
        f"@{PG['host']}:{PG['port']}/{PG['dbname']}"
    )
    engine = create_engine(engine_url)

    ok, fail = 0, 0
    indexed = list(enumerate(urls, start=1))

    with ThreadPoolExecutor(max_workers=max_workers, thread_name_prefix="very") as ex:
        futures = [
            ex.submit(_process_single_url, idx, total, url, engine, delay, headless)
            for idx, url in indexed
        ]
        for fut in as_completed(futures):
            idx, url, path, err = fut.result()
            if err is None:
                ok += 1
                print(f"[å®Œæˆ] [{idx}/{total}] âœ… {url} -> {path}")
            else:
                fail += 1
                print(f"[å¤±è´¥] [{idx}/{total}] âŒ {url}\n    {repr(err)}")

    print(f"\nğŸ“¦ ä»»åŠ¡ç»“æŸï¼šæˆåŠŸ {ok}ï¼Œå¤±è´¥ {fail}ï¼Œæ€»è®¡ {total}")


if __name__ == "__main__":
    very_fetch_info()
