import os
import re
import json
import time
import threading
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from concurrent.futures import ThreadPoolExecutor, as_completed

# ===========================
# é…ç½®å‚æ•°
# ===========================
CHROMEDRIVER_PATH = "D:/Software/chromedriver-win64/chromedriver.exe"
PRODUCT_URLS_FILE = "D:/TB/Products/camper/publication/product_urls.txt"
TEXT_SAVE_PATH = "D:/TB/Products/camper/publication/TXT/"
IMAGE_SAVE_PATH = "D:/TB/Products/camper/publication/images/"
MAX_WORKERS = 6
IMAGE_SUFFIXES = ['_C.jpg', '_F.jpg', '_L.jpg', '_T.jpg', '_P.jpg']
IMAGE_BASE_URL = "https://cloud.camper.com/is/image/YnJldW5pbmdlcjAx/"

# ===========================
# å‡†å¤‡ç›®å½•
# ===========================
os.makedirs(TEXT_SAVE_PATH, exist_ok=True)
os.makedirs(IMAGE_SAVE_PATH, exist_ok=True)

# ===========================
# Selenium è®¾ç½®
# ===========================
thread_local = threading.local()

def get_driver():
    if not hasattr(thread_local, "driver"):
        options = Options()
        options.add_argument("--headless")
        options.add_argument("--disable-gpu")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--window-size=1920x1080")
        options.add_argument("user-agent=Mozilla/5.0")
        service = Service(CHROMEDRIVER_PATH)
        thread_local.driver = webdriver.Chrome(service=service, options=options)
    return thread_local.driver

# ===========================
# ä¸‹è½½å›¾ç‰‡å‡½æ•°
# ===========================
def download_images(product_code):
    for suffix in IMAGE_SUFFIXES:
        image_url = IMAGE_BASE_URL + product_code + suffix
        image_name = f"{product_code}{suffix}"
        save_path = os.path.join(IMAGE_SAVE_PATH, image_name)
        try:
            response = requests.get(image_url, timeout=10)
            response.raise_for_status()
            with open(save_path, 'wb') as img_file:
                img_file.write(response.content)
            print(f"âœ… å›¾ç‰‡å·²ä¸‹è½½: {image_name}")
        except Exception as e:
            print(f"âŒ å›¾ç‰‡ä¸‹è½½å¤±è´¥: {image_url}ï¼ŒåŸå› : {e}")

# ===========================
# é¡µé¢å¤„ç†ä¸»å‡½æ•°
# ===========================
def process_product_url(url):
    os.makedirs(TEXT_SAVE_PATH, exist_ok=True)
    os.makedirs(IMAGE_SAVE_PATH, exist_ok=True)
    try:
        driver = get_driver()
        print(f"\nğŸ” æ­£åœ¨å¤„ç†é¡µé¢: {url}")
        driver.get(url)
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, "body")))
        time.sleep(4)

        soup = BeautifulSoup(driver.page_source, "html.parser")

        # é¡µé¢æ•°æ®è§£æ
        script_tag = soup.find("script", {"id": "__NEXT_DATA__", "type": "application/json"})
        if not script_tag:
            print(f"âš ï¸ é¡µé¢ä¸­æœªæ‰¾åˆ° JSON æ•°æ®: {url}")
            return

        data = json.loads(script_tag.string)
        sheet = data["props"]["pageProps"]["productSheet"]

        product_code = sheet.get("code", "Unknown_Code")
        title = soup.find("title").text.strip()
        title = re.sub(r"\s*[-â€“â€”]\s*Spring/Summer collection\s*[-â€“â€”]\s*Camper Germany", "", title)
        brand = sheet.get("brand", "")
        price = f"{sheet.get('prices', {}).get('current', '')}{sheet.get('prices', {}).get('currency', '')}"
        description = sheet.get("description", "")
        features = sheet.get("features", [])
        care = sheet.get("careDetailsArray", [])
        materials = sheet.get("careTextArray", [])
        sizes = sheet.get("sizes", [])

        size_lines = []
        for s in sizes:
            label = s.get("label", "N/A")
            value = s.get("value", "N/A")
            ean = s.get("ean", "N/A")
            stock = s.get("quantity", 0)
            available = s.get("available", False)
            size_lines.append(f"å°ºç : {value}ï¼ˆ{label}ï¼‰ | EAN: {ean} | å¯ç”¨: {available} | åº“å­˜: {stock}")

        # ä¿å­˜ TXT æ–‡ä»¶
        file_name = f"{product_code}.txt"
        with open(os.path.join(TEXT_SAVE_PATH, file_name), "w", encoding="utf-8") as f:
            f.write(f"Product CODE: {product_code}\n\n")
            f.write(f"Product Title: {title}\n\n")
            f.write(f"Product style: {brand}\n\n")
            f.write(f"Product price: {price}\n\n")
            f.write("Description:\n" + description + "\n\n")
            f.write("Features:\n" + json.dumps(features, ensure_ascii=False) + "\n\n")
            f.write("Product Care:\n" + json.dumps(care, ensure_ascii=False) + "\n\n")
            f.write("Product materials:\n" + json.dumps(materials, ensure_ascii=False) + "\n\n")
            f.write("Size & EAN Info:\n" + "\n".join(size_lines))

        print(f"ğŸ“„ å•†å“ä¿¡æ¯å·²å†™å…¥: {file_name}")

        # ä¸‹è½½å›¾ç‰‡
        download_images(product_code)

    except Exception as e:
        print(f"âŒ é¡µé¢å¤„ç†å¤±è´¥: {url}ï¼Œé”™è¯¯: {e}")

# ===========================
# ä¸»ç¨‹åºå…¥å£
# ===========================
def main():
    with open(PRODUCT_URLS_FILE, "r", encoding="utf-8") as f:
        urls = [line.strip() for line in f if line.strip()]

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = [executor.submit(process_product_url, url) for url in urls]
        for future in as_completed(futures):
            future.result()

    print("ğŸ¯ æ‰€æœ‰å•†å“å¤„ç†å®Œæˆï¼")

if __name__ == "__main__":
    main()
