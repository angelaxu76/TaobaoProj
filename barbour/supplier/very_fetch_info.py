# -*- coding: utf-8 -*-
"""
Very | Barbour å•†å“æŠ“å–ï¼ˆä¸ houseoffraser_fetch_info ç»“æ„å¯¹é½ï¼‰
- è§£æï¼šæ ‡é¢˜ã€æè¿°ã€é¢œè‰²ã€åŸä»·ã€æŠ˜æ‰£ä»·ã€å°ºç åº“å­˜ï¼ˆæœ‰è´§/æ— è´§ï¼‰â†’ TXT
- ç›¸ä¼¼åº¦åŒ¹é… barbour_productsï¼ˆæ ‡é¢˜+é¢œè‰²ï¼‰ï¼Œå‘½ä¸­åˆ™ç”¨ç¼–ç å‘½å TXT
- æ”¯æŒå¹¶å‘ä¸åŸå­å†™å…¥ï¼›å­—æ®µ/æ ¼å¼ä¸ HOF ç‰ˆä¸€è‡´
"""

from __future__ import annotations

import re
import os
import json
import time
import tempfile
import threading
from pathlib import Path
from typing import Optional, Tuple, List, Dict, Any
from concurrent.futures import ThreadPoolExecutor, as_completed

# ===== ç¬¬ä¸‰æ–¹ä¸è§£æ =====
import undetected_chromedriver as uc
from bs4 import BeautifulSoup
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# ===== DB ä¸é¡¹ç›®é…ç½® =====
from sqlalchemy import create_engine
from sqlalchemy.engine import Connection
from config import BARBOUR, BRAND_CONFIG
from barbour.core.site_utils import assert_site_or_raise as canon
from barbour.core.sim_matcher import match_product, choose_best, explain_results

# ================== ç«™ç‚¹ä¸ç›®å½• ==================
SITE_NAME = canon("very")
LINKS_FILE: str = BARBOUR["LINKS_FILES"]["very"]
TXT_DIR: Path = BARBOUR["TXT_DIRS"]["very"]
TXT_DIR.mkdir(parents=True, exist_ok=True)

PRODUCTS_TABLE: str = BRAND_CONFIG.get("barbour", {}).get("PRODUCTS_TABLE", "barbour_products")
PG = BRAND_CONFIG["barbour"]["PGSQL_CONFIG"]

# ================== å¯è°ƒå‚æ•° ==================
WAIT_PRICE_SECONDS = 8           # ç­‰ä»·é¢ä»·æ¨¡å—çš„æœ€é•¿ç­‰å¾…ï¼ˆç§’ï¼‰
DEFAULT_DELAY = 2.0              # æ‰“å¼€é¡µé¢åçš„ç¼“å†²ç­‰å¾…ï¼ˆç§’ï¼‰
MAX_WORKERS_DEFAULT = 4          # å¹¶å‘æ•°
MIN_SCORE = 0.72                 # ç›¸ä¼¼åº¦é˜ˆå€¼
MIN_LEAD = 0.04                  # é¢†å…ˆå¹…åº¦é˜ˆå€¼ï¼ˆTop1 ä¸ Top2 å·®å€¼ï¼‰
NAME_WEIGHT = 0.75               # åç§°æƒé‡
COLOR_WEIGHT = 0.25              # é¢œè‰²æƒé‡

# ================== å¹¶å‘å»é‡ + åŸå­å†™ ==================
_WRITTEN: set[str] = set()
_WRITTEN_LOCK = threading.Lock()

def _atomic_write_bytes(data: bytes, dst: Path, retries: int = 6, backoff: float = 0.25) -> bool:
    dst.parent.mkdir(parents=True, exist_ok=True)
    for i in range(retries):
        tmp = None
        try:
            with tempfile.NamedTemporaryFile(
                delete=False, dir=str(dst.parent), prefix=".tmp_", suffix=f".{os.getpid()}.{threading.get_ident()}"
            ) as tf:
                tmp = Path(tf.name)
                tf.write(data)
                tf.flush()
                os.fsync(tf.fileno())
            try:
                os.replace(tmp, dst)
            finally:
                if tmp and tmp.exists():
                    try: tmp.unlink(missing_ok=True)
                    except Exception: pass
            return True
        except (PermissionError, FileExistsError, OSError):
            if dst.exists():
                return True
            time.sleep(backoff * (i + 1))
            try:
                if tmp and tmp.exists():
                    tmp.unlink(missing_ok=True)
            except Exception:
                pass
        except Exception:
            time.sleep(backoff * (i + 1))
    return dst.exists()

def _kv_txt_bytes(info: Dict[str, Any]) -> bytes:
    fields = [
        "Product Code","Product Name","Product Description","Product Gender",
        "Product Color","Product Price","Adjusted Price","Product Material",
        "Style Category","Feature","Product Size","Product Size Detail",
        "Source URL","Site Name"
    ]
    lines = [f"{k}: {info.get(k, 'No Data')}" for k in fields]
    return ("\n".join(lines) + "\n").encode("utf-8", errors="ignore")

def _safe_name(s: str) -> str:
    return re.sub(r"[\\/:*?\"<>|'\s]+", "_", s or "NoName")

def get_dbapi_connection(conn_or_engine):
    if hasattr(conn_or_engine, "cursor"):
        return conn_or_engine
    if hasattr(conn_or_engine, "raw_connection"):
        return conn_or_engine.raw_connection()
    c = getattr(conn_or_engine, "connection", None)
    if c is not None:
        dbapi = getattr(c, "dbapi_connection", None)
        if dbapi is not None and hasattr(dbapi, "cursor"):
            return dbapi
        inner = getattr(c, "connection", None)
        if inner is not None and hasattr(inner, "cursor"):
            return inner
        if hasattr(c, "cursor"):
            return c
    return conn_or_engine

# ================== å·¥å…·å‡½æ•° ==================
def _clean(s: Optional[str]) -> str:
    return re.sub(r"\s+", " ", (s or "").strip())

def _json_from_script(html: str, var_name: str) -> Optional[dict]:
    """
    ä» window.__product_body_initial_state__= {...}; æŠ½ JSON
    """
    # ç²—æš´ä½†æœ‰æ•ˆï¼šå®šä½å˜é‡ååï¼ŒæŠ“å–èŠ±æ‹¬å·å¹³è¡¡å—
    pat = re.compile(rf"{re.escape(var_name)}\s*=\s*({{.*?}})\s*;", re.S)
    m = pat.search(html)
    if not m:
        return None
    blob = m.group(1)
    try:
        return json.loads(blob)
    except Exception:
        # å»æ‰è¡Œå°¾æ³¨é‡Š/å°¾é€—å·ç­‰å†è¯•
        try:
            blob2 = re.sub(r",\s*([}\]])", r"\1", blob)
            return json.loads(blob2)
        except Exception:
            return None

def _extract_title(soup: BeautifulSoup, initial: Optional[dict]) -> str:
    if initial and initial.get("name"):
        return _clean(initial["name"])
    t = _clean(soup.title.get_text()) if soup.title else "No Data"
    t = re.sub(r"\s*\|\s*Very\s*$", "", t, flags=re.I)
    return t or "No Data"

def _extract_desc(soup: BeautifulSoup, productData: Optional[dict]) -> str:
    # ä¼˜å…ˆ og:description
    m = soup.find("meta", attrs={"property": "og:description"})
    if m and m.get("content"):
        return _clean(m["content"])
    # é€€åˆ° dataLayer çš„ productData.description
    if productData and productData.get("description"):
        return _clean(productData["description"])
    return "No Data"

def _extract_color(initial: Optional[dict]) -> str:
    # ä» skus[].options.colour é‡Œå–ï¼›è‹¥å¤šè‰²ï¼Œå½“å‰è‰²ä¸€èˆ¬ä¸é¡µé¢ URL ä¸€è‡´ï¼Œè¿™é‡Œä¸æ·±ç©¶
    if initial and isinstance(initial.get("skus"), list):
        for sku in initial["skus"]:
            opts = sku.get("options") or {}
            col = opts.get("colour")
            if col:
                return _clean(str(col))
    return "No Data"

def _extract_gender(title: str, soup: BeautifulSoup, productData: Optional[dict]) -> str:
    t = (title or "").lower()
    if "women" in t or "ladies" in t: return "å¥³æ¬¾"
    if "men" in t: return "ç”·æ¬¾"
    # å†ä» dataLayer çš„åˆ†ç±»æ¨æ–­
    if productData:
        dept = (productData.get("subcategory") or "") + " " + (productData.get("category") or "") + " " + (productData.get("department") or "")
        d = dept.lower()
        if any(k in d for k in ["ladies","women","women's"]): return "å¥³æ¬¾"
        if "men" in d: return "ç”·æ¬¾"
    return "No Data"

def _to_num(s: Optional[str]) -> Optional[float]:
    if s is None:
        return None
    m = re.search(r"([0-9]+(?:\.[0-9]{1,2})?)", str(s).replace(",", ""))
    return float(m.group(1)) if m else None

def _extract_prices(initial: Optional[dict], soup: BeautifulSoup) -> Tuple[Optional[float], Optional[float]]:
    """
    è¿”å› (curr, orig) -> (æŠ˜åä»·, åŸä»·)
    Very çš„ initial_state.price.amount.decimal/previous
    """
    if initial:
        price = (initial.get("price") or {}).get("amount") or {}
        curr = _to_num(price.get("decimal") or price.get("current"))
        orig = _to_num(price.get("previous")) or curr
        if curr is not None:
            return curr, (orig if orig is not None else curr)
    # å…œåº• meta
    m_curr = soup.find("meta", attrs={"property": "product:price:amount"})
    if m_curr and m_curr.get("content"):
        curr = _to_num(m_curr["content"])
        return curr, curr
    return (None, None)

def _extract_sizes_and_stock(initial: Optional[dict], soup: BeautifulSoup) -> List[Tuple[str, str]]:
    """
    è¿”å› [(size, æœ‰è´§/æ— è´§), ...]
    ä¼˜å…ˆç”¨ skus[].stock.codeï¼›å¤±è´¥å†çœ‹ DOM é‡Œçš„ checkbox disabled
    """
    pairs: List[Tuple[str, str]] = []

    # 1) JSON ä¼˜å…ˆ
    if initial and isinstance(initial.get("skus"), list):
        for sku in initial["skus"]:
            opts = sku.get("options") or {}
            size = _clean(str(opts.get("size") or ""))
            if not size:
                continue
            stock = (sku.get("stock") or {}).get("code") or ""
            status = "æ— è´§"
            if stock:
                status = "æœ‰è´§" if stock.upper() in {"DCSTOCK", "IN_STOCK", "AVAILABLE"} else "æ— è´§"
            pairs.append((size, status))

    # 2) DOM å›é€€ï¼ˆid="size-XX" input[type=checkbox] disabled -> æ— è´§ï¼‰
    if not pairs:
        for inp in soup.select('input[id^="size-"][type="checkbox"]'):
            size = _clean((inp.get("id") or "").replace("size-", ""))
            if not size:
                continue
            disabled = inp.has_attr("disabled") or inp.get("aria-disabled") == "true"
            status = "æ— è´§" if disabled else "æœ‰è´§"
            pairs.append((size, status))

    # è§„èŒƒåŒ– size æ–‡æœ¬
    normed: List[Tuple[str, str]] = []
    for s, st in pairs:
        s2 = re.sub(r"\s*\(.*?\)\s*", "", s).strip()
        s2 = re.sub(r"^(UK|EU|US)\s+", "", s2, flags=re.I)
        normed.append((s2, st))
    return normed

def _build_size_lines(pairs: List[Tuple[str, str]]) -> Tuple[str, str]:
    by_size: Dict[str, str] = {}
    for size, status in pairs:
        prev = by_size.get(size)
        if prev is None or (prev == "æ— è´§" and status == "æœ‰è´§"):
            by_size[size] = status
    def _key(k: str):
        m = re.fullmatch(r"\d{1,3}", k)
        return (0, int(k)) if m else (1, k)
    ordered = sorted(by_size.keys(), key=_key)
    ps = ";".join(f"{k}:{by_size[k]}" for k in ordered) or "No Data"
    EAN = "0000000000000"
    psd = ";".join(f"{k}:{3 if by_size[k]=='æœ‰è´§' else 0}:{EAN}" for k in ordered) or "No Data"
    return ps, psd

def _guess_material(desc: str) -> str:
    if not desc or desc == "No Data":
        return "No Data"
    m = re.search(r"Material\s*Content\s*:\s*(.+?)(?:$|Washing|Wash|Care)", desc, flags=re.I)
    if m:
        return _clean(m.group(1))
    return "No Data"

def parse_info(html: str, url: str) -> Dict[str, Any]:
    soup = BeautifulSoup(html, "html.parser")

    initial = _json_from_script(html, "window.__product_body_initial_state__")
    # Very çš„ productDataï¼ˆmergeIntoDataLayerï¼‰å¸¸è§äºä¸€ä¸ªå†…è”è„šæœ¬é‡Œï¼Œå®½æ¾åŒ¹é…ä¸€ä¸‹
    productData = None
    m_pd = re.search(r"window\.mergeIntoDataLayer\s*\(\s*productData\s*\)", html)
    if m_pd:
        # é¡µé¢ä¸­å¸¸æœ‰ window.productData= {...}ï¼›ä¸ä¸€å®šç¨³å®šï¼Œè¿™é‡Œå°±ä¸å¼ºä¾èµ–äº†
        pass

    title = _extract_title(soup, initial)
    desc  = _extract_desc(soup, productData)
    color = _extract_color(initial)
    gender = _extract_gender(title, soup, productData)
    curr, orig = _extract_prices(initial, soup)
    if curr is None and orig is not None: curr = orig
    if orig is None and curr is not None: orig = curr

    size_pairs = _extract_sizes_and_stock(initial, soup)
    product_size, product_size_detail = _build_size_lines(size_pairs)

    material = _guess_material(desc)

    info = {
        "Product Code": "No Data",
        "Product Name": title or "No Data",
        "Product Description": desc or "No Data",
        "Product Gender": gender,
        "Product Color": color or "No Data",
        "Product Price": f"{orig:.2f}" if orig else "No Data",
        "Adjusted Price": f"{curr:.2f}" if curr else "No Data",
        "Product Material": material,
        "Style Category": "casual wear",
        "Feature": "No Data",
        "Product Size": product_size,
        "Product Size Detail": product_size_detail,
        "Source URL": url,
        "Site Name": SITE_NAME,
    }
    return info

# ================== Selenium & æŠ“å–æµç¨‹ ==================
def get_driver(headless: bool = False):
    options = uc.ChromeOptions()
    if headless:
        options.add_argument("--headless=new")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("--start-maximized")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    return uc.Chrome(options=options)

def process_url(url: str, conn: Connection, delay: float = DEFAULT_DELAY, headless: bool = False) -> Path:
    print(f"\nğŸŒ æ­£åœ¨æŠ“å–: {url}")
    driver = get_driver(headless=headless)
    try:
        driver.get(url)
        if WAIT_PRICE_SECONDS > 0:
            try:
                # ç­‰å¾…ä»»ä½•ä¸€ä¸ªä»·æ ¼/åç§°å®šä½åˆ°å³å¯
                WebDriverWait(driver, WAIT_PRICE_SECONDS).until(
                    EC.presence_of_element_located((By.TAG_NAME, "title"))
                )
            except Exception:
                pass
        if delay > 0:
            time.sleep(delay)
        html = driver.page_source
    finally:
        try: driver.quit()
        except Exception: pass

    info = parse_info(html, url)

    # ============= ç›¸ä¼¼åº¦åŒ¹é…ï¼ˆä¸ HOF ä¸€è‡´ï¼‰ =============
    raw_conn = get_dbapi_connection(conn)
    title = info.get("Product Name") or ""
    color = info.get("Product Color") or ""

    results = match_product(
        raw_conn,
        scraped_title=title,
        scraped_color=color,
        table=PRODUCTS_TABLE,
        name_weight=0.72,
        color_weight=0.18,
        type_weight=0.10,
        topk=5,
        recall_limit=2000,
        min_name=0.92,
        min_color=0.85,
        require_color_exact=False,
        require_type=False,
    )
    code = choose_best(results, min_score=MIN_SCORE, min_lead=MIN_LEAD)
    if not code and results:
        st = results[0].get("type_scraped")
        if st:
            for r in results:
                if r.get("type_db") == st:
                    code = r["product_code"]
                    print(f"ğŸ¯ tie-break by type â†’ {code}")
                    break

    print("ğŸ” match debug")
    print(f"  raw_title: {title}")
    print(f"  raw_color: {color}")
    txt, why = explain_results(results, min_score=MIN_SCORE, min_lead=MIN_LEAD)
    print(txt)
    if code:
        print(f"  â‡’ âœ… choose_best = {code}")
    else:
        print(f"  â‡’ âŒ no match ({why})")
        if results:
            print("ğŸ§ª top:", " | ".join(f"{r['product_code']}[{r['score']:.3f}]" for r in results[:3]))

    # å‘½åï¼šä¼˜å…ˆç”¨ç¼–ç 
    if code:
        info["Product Code"] = code
        out_name = f"{code}.txt"
    else:
        short = f"{abs(hash(url)) & 0xFFFF:04x}"
        out_name = f"{_safe_name(title)}_{short}.txt"

    out_path = TXT_DIR / out_name

    # å¹¶å‘å»é‡
    with _WRITTEN_LOCK:
        if out_name in _WRITTEN:
            print(f"â†©ï¸  è·³è¿‡é‡å¤å†™å…¥ï¼š{out_name}")
            return out_path
        _WRITTEN.add(out_name)

    # åŸå­å†™å…¥
    payload = _kv_txt_bytes(info)
    ok = _atomic_write_bytes(payload, out_path)
    if ok:
        print(f"âœ… å†™å…¥: {out_path.name} (code={info.get('Product Code')})")
    else:
        print(f"â— æ”¾å¼ƒå†™å…¥: {out_path.name}")
    return out_path

def very_fetch_info(max_workers: int = MAX_WORKERS_DEFAULT, delay: float = DEFAULT_DELAY, headless: bool = False):
    links_file = Path(LINKS_FILE)
    if not links_file.exists():
        print(f"âš  æ‰¾ä¸åˆ°é“¾æ¥æ–‡ä»¶ï¼š{links_file}")
        return

    urls = [line.strip() for line in links_file.read_text(encoding="utf-8", errors="ignore").splitlines()
            if line.strip() and not line.strip().startswith("#")]
    total = len(urls)
    print(f"ğŸ“„ å…± {total} ä¸ªå•†å“é¡µé¢å¾…è§£æ...ï¼ˆå¹¶å‘ {max_workers}ï¼‰")
    if total == 0:
        return

    engine_url = f"postgresql+psycopg2://{PG['user']}:{PG['password']}@{PG['host']}:{PG['port']}/{PG['dbname']}"
    engine = create_engine(engine_url)

    indexed = list(enumerate(urls, start=1))

    def _worker(idx_url):
        idx, u = idx_url
        print(f"[å¯åŠ¨] [{idx}/{total}] {u}")
        try:
            with engine.begin() as conn:
                path = process_url(u, conn=conn, delay=delay, headless=headless)
            return (idx, u, str(path), None)
        except Exception as e:
            return (idx, u, None, e)

    ok, fail = 0, 0
    with ThreadPoolExecutor(max_workers=max_workers, thread_name_prefix="very") as ex:
        futures = [ex.submit(_worker, iu) for iu in indexed]
        for fut in as_completed(futures):
            idx, u, path, err = fut.result()
            if err is None:
                ok += 1
                print(f"[å®Œæˆ] [{idx}/{total}] âœ… {u} -> {path}")
            else:
                fail += 1
                print(f"[å¤±è´¥] [{idx}/{total}] âŒ {u}\n    {repr(err)}")

    print(f"\nğŸ“¦ ä»»åŠ¡ç»“æŸï¼šæˆåŠŸ {ok}ï¼Œå¤±è´¥ {fail}ï¼Œæ€»è®¡ {total}")

if __name__ == "__main__":
    very_fetch_info()
