# -*- coding: utf-8 -*-
"""
Very | Barbour å•†å“æŠ“å–ï¼ˆä¸ houseoffraser_fetch_info ç»“æ„å¯¹é½ï¼‰
- è§£æï¼šæ ‡é¢˜ã€æè¿°ã€é¢œè‰²ã€åŸä»·ã€æŠ˜æ‰£ä»·ã€å°ºç åº“å­˜ï¼ˆæœ‰è´§/æ— è´§ï¼‰â†’ TXT
- ç›¸ä¼¼åº¦åŒ¹é… barbour_productsï¼ˆæ ‡é¢˜+é¢œè‰²ï¼‰ï¼Œå‘½ä¸­åˆ™ç”¨ç¼–ç å‘½å TXT
- æ”¯æŒå¹¶å‘ä¸åŸå­å†™å…¥ï¼›å­—æ®µ/æ ¼å¼ä¸ HOF ç‰ˆä¸€è‡´
"""

from __future__ import annotations

import re
import os
import json
import time
import tempfile
import threading
from pathlib import Path
from typing import Optional, Tuple, List, Dict, Any
from concurrent.futures import ThreadPoolExecutor, as_completed

# ===== ç¬¬ä¸‰æ–¹ä¸è§£æ =====
import undetected_chromedriver as uc
from bs4 import BeautifulSoup
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# ===== DB ä¸é¡¹ç›®é…ç½® =====
from sqlalchemy import create_engine
from sqlalchemy.engine import Connection
from config import BARBOUR, BRAND_CONFIG
from barbour.core.site_utils import assert_site_or_raise as canon
from barbour.core.sim_matcher import match_product, choose_best, explain_results

# ================== ç«™ç‚¹ä¸ç›®å½• ==================
SITE_NAME = canon("very")
LINKS_FILE: str = BARBOUR["LINKS_FILES"]["very"]
TXT_DIR: Path = BARBOUR["TXT_DIRS"]["very"]
TXT_DIR.mkdir(parents=True, exist_ok=True)

PRODUCTS_TABLE: str = BRAND_CONFIG.get("barbour", {}).get("PRODUCTS_TABLE", "barbour_products")
PG = BRAND_CONFIG["barbour"]["PGSQL_CONFIG"]

# ================== å¯è°ƒå‚æ•° ==================
WAIT_PRICE_SECONDS = 8           # ç­‰ä»·é¢ä»·æ¨¡å—çš„æœ€é•¿ç­‰å¾…ï¼ˆç§’ï¼‰
DEFAULT_DELAY = 2.0              # æ‰“å¼€é¡µé¢åçš„ç¼“å†²ç­‰å¾…ï¼ˆç§’ï¼‰
MAX_WORKERS_DEFAULT = 4          # å¹¶å‘æ•°
MIN_SCORE = 0.72                 # ç›¸ä¼¼åº¦é˜ˆå€¼
MIN_LEAD = 0.04                  # é¢†å…ˆå¹…åº¦é˜ˆå€¼ï¼ˆTop1 ä¸ Top2 å·®å€¼ï¼‰
NAME_WEIGHT = 0.75               # åç§°æƒé‡
COLOR_WEIGHT = 0.25              # é¢œè‰²æƒé‡


# ===== æ ‡å‡†å°ºç è¡¨ï¼ˆç”¨äºè¡¥é½æœªå‡ºç°å°ºç =0ï¼‰ =====
WOMEN_ORDER = ["4","6","8","10","12","14","16","18","20"]
MEN_ALPHA_ORDER = ["2XS","XS","S","M","L","XL","2XL","3XL"]
MEN_NUM_ORDER = [str(n) for n in range(30, 52, 2)]  # 30..50ï¼ˆä¸å«52ï¼‰

def _full_order_for_gender(gender: str) -> list[str]:
    """æ ¹æ®æ€§åˆ«è¿”å›å®Œæ•´å°ºç é¡ºåºï¼›æœªçŸ¥/ç«¥æ¬¾å…ˆæŒ‰ç”·æ¬¾å¤„ç†ã€‚"""
    g = (gender or "").lower()
    if "å¥³" in g or "women" in g or "ladies" in g:
        return WOMEN_ORDER
    return MEN_ALPHA_ORDER + MEN_NUM_ORDER


# ================== å¹¶å‘å»é‡ + åŸå­å†™ ==================
_WRITTEN: set[str] = set()
_WRITTEN_LOCK = threading.Lock()

def _atomic_write_bytes(data: bytes, dst: Path, retries: int = 6, backoff: float = 0.25) -> bool:
    dst.parent.mkdir(parents=True, exist_ok=True)
    for i in range(retries):
        tmp = None
        try:
            with tempfile.NamedTemporaryFile(
                delete=False, dir=str(dst.parent), prefix=".tmp_", suffix=f".{os.getpid()}.{threading.get_ident()}"
            ) as tf:
                tmp = Path(tf.name)
                tf.write(data)
                tf.flush()
                os.fsync(tf.fileno())
            try:
                os.replace(tmp, dst)
            finally:
                if tmp and tmp.exists():
                    try: tmp.unlink(missing_ok=True)
                    except Exception: pass
            return True
        except (PermissionError, FileExistsError, OSError):
            if dst.exists():
                return True
            time.sleep(backoff * (i + 1))
            try:
                if tmp and tmp.exists():
                    tmp.unlink(missing_ok=True)
            except Exception:
                pass
        except Exception:
            time.sleep(backoff * (i + 1))
    return dst.exists()

def _kv_txt_bytes(info: Dict[str, Any]) -> bytes:
    fields = [
        "Product Code","Product Name","Product Description","Product Gender",
        "Product Color","Product Price","Adjusted Price","Product Material",
        "Style Category","Feature","Product Size","Product Size Detail",
        "Source URL","Site Name"
    ]
    lines = [f"{k}: {info.get(k, 'No Data')}" for k in fields]
    return ("\n".join(lines) + "\n").encode("utf-8", errors="ignore")

def _safe_name(s: str) -> str:
    return re.sub(r"[\\/:*?\"<>|'\s]+", "_", s or "NoName")

def get_dbapi_connection(conn_or_engine):
    if hasattr(conn_or_engine, "cursor"):
        return conn_or_engine
    if hasattr(conn_or_engine, "raw_connection"):
        return conn_or_engine.raw_connection()
    c = getattr(conn_or_engine, "connection", None)
    if c is not None:
        dbapi = getattr(c, "dbapi_connection", None)
        if dbapi is not None and hasattr(dbapi, "cursor"):
            return dbapi
        inner = getattr(c, "connection", None)
        if inner is not None and hasattr(inner, "cursor"):
            return inner
        if hasattr(c, "cursor"):
            return c
    return conn_or_engine

# ================== å·¥å…·å‡½æ•° ==================

def _choose_full_order_for_gender(gender: str, present: set[str]) -> list[str]:
    """ç”·æ¬¾åœ¨ã€å­—æ¯ç³»(2XSâ€“3XL)ã€‘ä¸ã€æ•°å­—ç³»(30â€“50,ä¸å«52)ã€‘äºŒé€‰ä¸€ï¼›å¥³æ¬¾å›ºå®š 4â€“20ã€‚"""
    g = (gender or "").lower()
    if "å¥³" in g or "women" in g or "ladies" in g:
        return WOMEN_ORDER[:]  # å¥³æ¬¾å›ºå®š 4..20

    has_num   = any(k in MEN_NUM_ORDER   for k in present)
    has_alpha = any(k in MEN_ALPHA_ORDER for k in present)
    if has_num and not has_alpha:
        return MEN_NUM_ORDER[:]          # åªç”¨æ•°å­—ç³» 30..50
    if has_alpha and not has_num:
        return MEN_ALPHA_ORDER[:]        # åªç”¨å­—æ¯ç³» 2XS..3XL
    if has_num or has_alpha:
        # åŒæ—¶å‡ºç°ï¼ˆå¼‚å¸¸ï¼‰â†’ é€‰å‡ºç°æ›´å¤šçš„é‚£ä¸€ç³»
        num_count   = sum(1 for k in present if k in MEN_NUM_ORDER)
        alpha_count = sum(1 for k in present if k in MEN_ALPHA_ORDER)
        return MEN_NUM_ORDER[:] if num_count >= alpha_count else MEN_ALPHA_ORDER[:]
    # å®åœ¨åˆ¤ä¸å‡ºæ¥ï¼šé»˜è®¤ç”¨å­—æ¯ç³»ï¼ˆå¤–å¥—æ›´å¸¸è§ï¼‰
    return MEN_ALPHA_ORDER[:]


def _clean(s: Optional[str]) -> str:
    return re.sub(r"\s+", " ", (s or "").strip())

def _json_from_script(html: str, var_name: str) -> Optional[dict]:
    """
    ä» window.__product_body_initial_state__= {...}; æŠ½ JSON
    """
    # ç²—æš´ä½†æœ‰æ•ˆï¼šå®šä½å˜é‡ååï¼ŒæŠ“å–èŠ±æ‹¬å·å¹³è¡¡å—
    pat = re.compile(rf"{re.escape(var_name)}\s*=\s*({{.*?}})\s*;", re.S)
    m = pat.search(html)
    if not m:
        return None
    blob = m.group(1)
    try:
        return json.loads(blob)
    except Exception:
        # å»æ‰è¡Œå°¾æ³¨é‡Š/å°¾é€—å·ç­‰å†è¯•
        try:
            blob2 = re.sub(r",\s*([}\]])", r"\1", blob)
            return json.loads(blob2)
        except Exception:
            return None

def _extract_title(soup: BeautifulSoup, initial: Optional[dict]) -> str:
    if initial and initial.get("name"):
        return _clean(initial["name"])
    t = _clean(soup.title.get_text()) if soup.title else "No Data"
    t = re.sub(r"\s*\|\s*Very\s*$", "", t, flags=re.I)
    return t or "No Data"

def _extract_desc(soup: BeautifulSoup, productData: Optional[dict]) -> str:
    # ä¼˜å…ˆ og:description
    m = soup.find("meta", attrs={"property": "og:description"})
    if m and m.get("content"):
        return _clean(m["content"])
    # é€€åˆ° dataLayer çš„ productData.description
    if productData and productData.get("description"):
        return _clean(productData["description"])
    return "No Data"

def _extract_color(initial: Optional[dict]) -> str:
    # ä» skus[].options.colour é‡Œå–ï¼›è‹¥å¤šè‰²ï¼Œå½“å‰è‰²ä¸€èˆ¬ä¸é¡µé¢ URL ä¸€è‡´ï¼Œè¿™é‡Œä¸æ·±ç©¶
    if initial and isinstance(initial.get("skus"), list):
        for sku in initial["skus"]:
            opts = sku.get("options") or {}
            col = opts.get("colour")
            if col:
                return _clean(str(col))
    return "No Data"

def _extract_gender(title: str, soup: BeautifulSoup, productData: Optional[dict]) -> str:
    t = (title or "").lower()
    if "women" in t or "ladies" in t: return "å¥³æ¬¾"
    if "men" in t: return "ç”·æ¬¾"
    # å†ä» dataLayer çš„åˆ†ç±»æ¨æ–­
    if productData:
        dept = (productData.get("subcategory") or "") + " " + (productData.get("category") or "") + " " + (productData.get("department") or "")
        d = dept.lower()
        if any(k in d for k in ["ladies","women","women's"]): return "å¥³æ¬¾"
        if "men" in d: return "ç”·æ¬¾"
    return "No Data"

def _to_num(s: Optional[str]) -> Optional[float]:
    if s is None:
        return None
    m = re.search(r"([0-9]+(?:\.[0-9]{1,2})?)", str(s).replace(",", ""))
    return float(m.group(1)) if m else None

def _extract_prices(initial: Optional[dict], soup: BeautifulSoup) -> Tuple[Optional[float], Optional[float]]:
    """
    è¿”å› (curr, orig) -> (æŠ˜åä»·, åŸä»·)
    Very çš„ initial_state.price.amount.decimal/previous
    """
    if initial:
        price = (initial.get("price") or {}).get("amount") or {}
        curr = _to_num(price.get("decimal") or price.get("current"))
        orig = _to_num(price.get("previous")) or curr
        if curr is not None:
            return curr, (orig if orig is not None else curr)
    # å…œåº• meta
    m_curr = soup.find("meta", attrs={"property": "product:price:amount"})
    if m_curr and m_curr.get("content"):
        curr = _to_num(m_curr["content"])
        return curr, curr
    return (None, None)

def _extract_sizes_and_stock(initial: Optional[dict], soup: BeautifulSoup) -> List[Tuple[str, str]]:
    """
    è¿”å› [(size, æœ‰è´§/æ— è´§), ...]
    ä¼˜å…ˆç”¨ skus[].stock.codeï¼›å¤±è´¥å†çœ‹ DOM é‡Œçš„ checkbox disabled
    """
    pairs: List[Tuple[str, str]] = []

    # 1) JSON ä¼˜å…ˆ
    if initial and isinstance(initial.get("skus"), list):
        for sku in initial["skus"]:
            opts = sku.get("options") or {}
            size = _clean(str(opts.get("size") or ""))
            if not size:
                continue
            stock = (sku.get("stock") or {}).get("code") or ""
            status = "æ— è´§"
            if stock:
                status = "æœ‰è´§" if stock.upper() in {"DCSTOCK", "IN_STOCK", "AVAILABLE"} else "æ— è´§"
            pairs.append((size, status))

    # 2) DOM å›é€€ï¼ˆid="size-XX" input[type=checkbox] disabled -> æ— è´§ï¼‰
    if not pairs:
        for inp in soup.select('input[id^="size-"][type="checkbox"]'):
            size = _clean((inp.get("id") or "").replace("size-", ""))
            if not size:
                continue
            disabled = inp.has_attr("disabled") or inp.get("aria-disabled") == "true"
            status = "æ— è´§" if disabled else "æœ‰è´§"
            pairs.append((size, status))

    # è§„èŒƒåŒ– size æ–‡æœ¬ï¼ˆå«å­—æ¯å°ºç æ˜ å°„ï¼‰
    _alpha_canon = {
        "XXXS":"2XS","2XS":"2XS","XXS":"XS","XS":"XS",
        "S":"S","SMALL":"S","M":"M","MEDIUM":"M","L":"L","LARGE":"L",
        "XL":"XL","X-LARGE":"XL","XXL":"2XL","2XL":"2XL","XXXL":"3XL","3XL":"3XL",
    }
    normed: List[Tuple[str, str]] = []
    for s, st in pairs:
        s2 = re.sub(r"\s*\(.*?\)\s*", "", s).strip()
        s2 = re.sub(r"^(UK|EU|US)\s+", "", s2, flags=re.I)
        s2u = s2.upper().replace("-", "").strip()
        # å­—æ¯ä¼˜å…ˆè§„èŒƒï¼›å¦åˆ™ä¿æŒæ•°å­—ï¼ˆå¦‚ 30â€“50ï¼‰
        if s2u in _alpha_canon:
            s2 = _alpha_canon[s2u]
        normed.append((s2, st))
    return normed


def _build_size_lines(pairs: List[Tuple[str, str]], gender: str) -> Tuple[str, str]:
    """
    å°†å‡ºç°çš„å°ºç æŒ‰â€œæœ‰è´§ä¼˜å…ˆâ€åˆå¹¶ï¼Œå¹¶è¡¥é½æœªå‡ºç°çš„å°ºç ä¸º æ— è´§/0ã€‚
    - Product Size:         34:æœ‰è´§;36:æ— è´§;...
    - Product Size Detail:  34:3:000...;36:0:000...;...
    âœ… ç”·æ¬¾ï¼šäºŒé€‰ä¸€ï¼ˆå­—æ¯ç³» æˆ– æ•°å­—ç³»ï¼‰ï¼Œç»ä¸æ··ç”¨
    âœ… å¥³æ¬¾ï¼šå›ºå®š 4â€“20
    """
    by_size: Dict[str, str] = {}

    # 1) å…ˆåˆå¹¶â€œå‡ºç°çš„å°ºç â€ï¼ˆåŒå°ºç å¤šæ¬¡ â†’ æœ‰è´§ä¼˜å…ˆï¼‰
    for size, status in (pairs or []):
        prev = by_size.get(size)
        if prev is None or (prev == "æ— è´§" and status == "æœ‰è´§"):
            by_size[size] = status

    # 2) ä¾æ®â€œå·²å‡ºç°çš„å°ºç â€é€‰æ‹©ç”·æ¬¾å°ºç ç³»ï¼ˆæˆ–å¥³æ¬¾ 4â€“20ï¼‰
    present_keys = set(by_size.keys())
    full_order = _choose_full_order_for_gender(gender, present_keys)

    # 3) æ¸…ç†æ··å…¥çš„å¦ä¸€ç³»ï¼ˆé˜²æ­¢åŒæ—¶è¾“å‡ºä¸¤å¥—ç³»ï¼‰
    for k in list(by_size.keys()):
        if k not in full_order:
            by_size.pop(k, None)

    # 4) ä»…åœ¨é€‰å®šé‚£ä¸€ç³»å†…è¡¥é½æœªå‡ºç°çš„å°ºç ä¸º æ— è´§/0
    for s in full_order:
        if s not in by_size:
            by_size[s] = "æ— è´§"

    # 5) å›ºå®šé¡ºåºè¾“å‡ºï¼ˆæœ‰è´§=3ï¼Œæ— è´§=0ï¼‰
    EAN = "0000000000000"
    ordered = list(full_order)
    ps  = ";".join(f"{k}:{by_size[k]}" for k in ordered) or "No Data"
    psd = ";".join(f"{k}:{3 if by_size[k]=='æœ‰è´§' else 0}:{EAN}" for k in ordered) or "No Data"
    return ps, psd



def _guess_material(desc: str) -> str:
    if not desc or desc == "No Data":
        return "No Data"
    m = re.search(r"Material\s*Content\s*:\s*(.+?)(?:$|Washing|Wash|Care)", desc, flags=re.I)
    if m:
        return _clean(m.group(1))
    return "No Data"

def parse_info(html: str, url: str) -> Dict[str, Any]:
    soup = BeautifulSoup(html, "html.parser")

    initial = _json_from_script(html, "window.__product_body_initial_state__")
    # Very çš„ productDataï¼ˆmergeIntoDataLayerï¼‰å¸¸è§äºä¸€ä¸ªå†…è”è„šæœ¬é‡Œï¼Œå®½æ¾åŒ¹é…ä¸€ä¸‹
    productData = None
    m_pd = re.search(r"window\.mergeIntoDataLayer\s*\(\s*productData\s*\)", html)
    if m_pd:
        # é¡µé¢ä¸­å¸¸æœ‰ window.productData= {...}ï¼›ä¸ä¸€å®šç¨³å®šï¼Œè¿™é‡Œå°±ä¸å¼ºä¾èµ–äº†
        pass

    title = _extract_title(soup, initial)
    desc  = _extract_desc(soup, productData)
    color = _extract_color(initial)

    # â€”â€” ç”¨ title å…œåº•é¢œè‰² & å»æ‰æ ‡é¢˜ä¸­çš„é¢œè‰²å°¾å·´
    t2, color_from_title = _split_title_color(title)
    if not color or color.lower() == "no data":
        color = color_from_title or "No Data"
    title = t2


    gender = _extract_gender(title, soup, productData)
    curr, orig = _extract_prices(initial, soup)
    if curr is None and orig is not None: curr = orig
    if orig is None and curr is not None: orig = curr

    size_pairs = _extract_sizes_and_stock(initial, soup)
    product_size, product_size_detail = _build_size_lines(size_pairs, gender)


    material = _guess_material(desc)

    info = {
        "Product Code": "No Data",
        "Product Name": title or "No Data",
        "Product Description": desc or "No Data",
        "Product Gender": gender,
        "Product Color": color or "No Data",
        "Product Price": f"{orig:.2f}" if orig else "No Data",
        "Adjusted Price": f"{curr:.2f}" if curr else "No Data",
        "Product Material": material,
        "Style Category": "casual wear",
        "Feature": "No Data",
        "Product Size": product_size,
        "Product Size Detail": product_size_detail,
        "Source URL": url,
        "Site Name": SITE_NAME,
    }
    return info

# ================== Selenium & æŠ“å–æµç¨‹ ==================
import re, subprocess, shutil, sys
try:
    import winreg  # Windows æ‰æœ‰
except Exception:
    winreg = None

def _get_chrome_major_version() -> int | None:
    """Windows ä¸Šè·å–å·²å®‰è£… Chrome ä¸»ç‰ˆæœ¬ï¼›å¤±è´¥è¿”å› Noneã€‚"""
    # 1) æ³¨å†Œè¡¨ï¼šHKCU/HKLM\SOFTWARE\Google\Chrome\BLBeacon\version
    if winreg is not None and sys.platform.startswith("win"):
        reg_paths = [
            (winreg.HKEY_CURRENT_USER,  r"SOFTWARE\Google\Chrome\BLBeacon"),
            (winreg.HKEY_LOCAL_MACHINE, r"SOFTWARE\Google\Chrome\BLBeacon"),
            # å…¼å®¹ 32/64 ä½é‡å®šå‘
            (winreg.HKEY_LOCAL_MACHINE, r"SOFTWARE\WOW6432Node\Google\Chrome\BLBeacon"),
        ]
        for hive, path in reg_paths:
            try:
                with winreg.OpenKey(hive, path) as k:
                    ver, _ = winreg.QueryValueEx(k, "version")
                    m = re.search(r"^(\d+)\.", ver)
                    if m:
                        return int(m.group(1))
            except OSError:
                pass

    # 2) å‘½ä»¤è¡Œï¼šchrome.exe --version
    candidates = [
        "chrome",  # PATH é‡Œæœ‰çš„è¯
        r"C:\Program Files\Google\Chrome\Application\chrome.exe",
        r"C:\Program Files (x86)\Google\Chrome\Application\chrome.exe",
    ]
    for exe in candidates:
        path = shutil.which(exe) or exe
        try:
            out = subprocess.check_output([path, "--version"], stderr=subprocess.STDOUT, text=True, timeout=3)
            # å½¢å¦‚ "Google Chrome 129.0.6668.59"
            m = re.search(r"(\d+)\.\d+\.\d+\.\d+", out)
            if m:
                return int(m.group(1))
        except Exception:
            continue
    return None

def get_driver(headless: bool = False, retries: int = 2):
    """
    æ›´ç¨³çš„ UC å¯åŠ¨ï¼š
    - æ¯æ¬¡å°è¯•éƒ½åˆ›å»ºå…¨æ–°çš„ ChromeOptionsï¼ˆé¿å…å¤ç”¨æŠ¥é”™ï¼‰
    - è‹¥é»˜è®¤å¯åŠ¨å¤±è´¥ï¼Œè¯»å–æœ¬æœº Chrome ä¸»ç‰ˆæœ¬ï¼Œå¸¦ version_main é‡è¯•
    - use_subprocess=True æå‡å…¼å®¹æ€§
    """
    import undetected_chromedriver as uc

    def make_options():
        opts = uc.ChromeOptions()
        if headless:
            opts.add_argument("--headless=new")
        opts.add_argument("--disable-blink-features=AutomationControlled")
        opts.add_argument("--start-maximized")
        opts.add_argument("--disable-gpu")
        opts.add_argument("--no-sandbox")
        return opts

    last_err = None
    for attempt in range(1, retries + 1):
        # å°è¯• 1ï¼šé»˜è®¤å¯åŠ¨ï¼ˆæ–°çš„ options å®ä¾‹ï¼‰
        try:
            drv = uc.Chrome(options=make_options(), headless=headless, use_subprocess=True)
            print(f"[uc] started (attempt {attempt})")
            return drv
        except Exception as e:
            last_err = e
            print(f"[uc] default start failed (attempt {attempt}): {e}")

        # å°è¯• 2ï¼šæºå¸¦ version_mainï¼ˆå†æ¬¡ä½¿ç”¨â€œæ–°çš„â€ options å®ä¾‹ï¼‰
        try:
            vm = _get_chrome_major_version()
            if vm:
                print(f"[uc] retry with version_main={vm} (attempt {attempt})")
                drv = uc.Chrome(
                    options=make_options(),
                    headless=headless,
                    use_subprocess=True,
                    version_main=vm
                )
                print(f"[uc] started with version_main={vm}")
                return drv
            else:
                print("[uc] cannot detect local Chrome version; skip version_main retry")
        except Exception as e2:
            last_err = e2
            print(f"[uc] version_main retry failed: {e2}")

    # è‹¥ä»å¤±è´¥ï¼ŒæŠ›å‡ºæœ€åä¸€æ¬¡é”™è¯¯
    raise last_err


def _split_title_color(title: str) -> tuple[str, str | None]:
    t = (title or "").strip()
    if not t:
        return "No Data", None
    # ä»¥ â€œ - â€ æ‹†åˆ†ï¼›æ‹¿æœ€åä¸€æ®µåšé¢œè‰²ï¼Œå…¶ä½™æ‹¼å›ä½œä¸ºå‡€åŒ–åçš„æ ‡é¢˜
    parts = [p.strip() for p in re.split(r"\s*-\s*", t) if p.strip()]
    if len(parts) >= 2:
        raw_color = parts[-1]
        # å¤šè¯é¢œè‰²å–ç¬¬ä¸€ä¸ªä¸»è¦è¯ï¼›å»æ‰è¿æ¥ç¬¦/æ ‡ç‚¹
        color = re.split(r"[\/&]", re.sub(r"[^\w\s/&-]", "", raw_color))[0].strip()
        color = color.title() if color else None
        clean_title = " - ".join(parts[:-1])  # å»æ‰é¢œè‰²å°¾å·´
        return (clean_title or t, color or None)
    return t, None

def process_url(url: str, conn: Connection, delay: float = DEFAULT_DELAY, headless: bool = False, driver=None) -> Path:
    print(f"\nğŸŒ æ­£åœ¨æŠ“å–: {url}")
    if driver is None:
        driver = get_driver(headless=headless)  # å…œåº•
        owns = True
    else:
        owns = False
    try:
        driver.get(url)
        if WAIT_PRICE_SECONDS > 0:
            try:
                # ç­‰å¾…ä»»ä½•ä¸€ä¸ªä»·æ ¼/åç§°å®šä½åˆ°å³å¯
                WebDriverWait(driver, WAIT_PRICE_SECONDS).until(
                    EC.presence_of_element_located((By.TAG_NAME, "title"))
                )
            except Exception:
                pass
        if delay > 0:
            time.sleep(delay)
        html = driver.page_source
    finally:
        if owns:   # åªæœ‰è‡ªå·±æ–°å»ºçš„æ‰è´Ÿè´£å…³é—­
            try: driver.quit()
            except Exception: pass

    info = parse_info(html, url)

    # ============= ç›¸ä¼¼åº¦åŒ¹é…ï¼ˆä¸ HOF ä¸€è‡´ï¼‰ =============
    raw_conn = get_dbapi_connection(conn)
    title = info.get("Product Name") or ""
    color = info.get("Product Color") or ""

    print("title:::::::::::::" + title)
    print("color:::::::::::::" + color)

    results = match_product(
        raw_conn,
        scraped_title=title,
        scraped_color=color,
        table=PRODUCTS_TABLE,
        name_weight=0.72,
        color_weight=0.18,
        type_weight=0.10,
        topk=20,              # è°ƒå¤§ï¼ŒæŸ¥çœ‹æ›´å¤šå€™é€‰
        recall_limit=5000,    # é€‚å½“è°ƒå¤§å¬å›
        min_name=None,        # å…³é—­åç§°ç¡¬é˜ˆå€¼ï¼ˆè°ƒè¯•ï¼‰
        min_color=None,       # å…³é—­é¢œè‰²ç¡¬é˜ˆå€¼ï¼ˆè°ƒè¯•ï¼‰
        require_color_exact=False,
        require_type=False,
    )
    code = choose_best(results, min_score=MIN_SCORE, min_lead=MIN_LEAD)
    if not code and results:
        st = results[0].get("type_scraped")
        if st:
            for r in results:
                if r.get("type_db") == st:
                    code = r["product_code"]
                    print(f"ğŸ¯ tie-break by type â†’ {code}")
                    break

    print("ğŸ” match debug")
    print(f"  raw_title: {title}")
    print(f"  raw_color: {color}")
    txt, why = explain_results(results, min_score=MIN_SCORE, min_lead=MIN_LEAD)
    print(txt)
    if code:
        print(f"  â‡’ âœ… choose_best = {code}")
    else:
        print(f"  â‡’ âŒ no match ({why})")
        if results:
            print("ğŸ§ª top:", " | ".join(f"{r['product_code']}[{r['score']:.3f}]" for r in results[:3]))

    # å‘½åï¼šä¼˜å…ˆç”¨ç¼–ç 
    if code:
        info["Product Code"] = code
        out_name = f"{code}.txt"
    else:
        short = f"{abs(hash(url)) & 0xFFFF:04x}"
        out_name = f"{_safe_name(title)}_{short}.txt"

    out_path = TXT_DIR / out_name

    # å¹¶å‘å»é‡
    with _WRITTEN_LOCK:
        if out_name in _WRITTEN:
            print(f"â†©ï¸  è·³è¿‡é‡å¤å†™å…¥ï¼š{out_name}")
            return out_path
        _WRITTEN.add(out_name)

    # åŸå­å†™å…¥
    payload = _kv_txt_bytes(info)
    ok = _atomic_write_bytes(payload, out_path)
    if ok:
        print(f"âœ… å†™å…¥: {out_path.name} (code={info.get('Product Code')})")
    else:
        print(f"â— æ”¾å¼ƒå†™å…¥: {out_path.name}")
    return out_path

def very_fetch_info(max_workers: int = MAX_WORKERS_DEFAULT, delay: float = DEFAULT_DELAY, headless: bool = False):
    links_file = Path(LINKS_FILE)
    if not links_file.exists():
        print(f"âš  æ‰¾ä¸åˆ°é“¾æ¥æ–‡ä»¶ï¼š{links_file}")
        return

    urls = [line.strip() for line in links_file.read_text(encoding="utf-8", errors="ignore").splitlines()
            if line.strip() and not line.strip().startswith("#")]
    total = len(urls)
    print(f"ğŸ“„ å…± {total} ä¸ªå•†å“é¡µé¢å¾…è§£æ...ï¼ˆå¹¶å‘ {max_workers}ï¼‰")
    if total == 0:
        return

    engine_url = f"postgresql+psycopg2://{PG['user']}:{PG['password']}@{PG['host']}:{PG['port']}/{PG['dbname']}"
    engine = create_engine(engine_url)

    indexed = list(enumerate(urls, start=1))

    def _worker(idx_url_chunk):
        # æ¯ä¸ªçº¿ç¨‹ä¸€ä¸ª driver
        driver = get_driver(headless=headless)
        try:
            res = []
            for idx, u in idx_url_chunk:
                print(f"[å¯åŠ¨] [{idx}/{total}] {u}")
                try:
                    with engine.begin() as conn:
                        path = process_url(u, conn=conn, delay=delay, headless=headless, driver=driver)
                    res.append((idx, u, str(path), None))
                except Exception as e:
                    res.append((idx, u, None, e))
            return res
        finally:
            try: driver.quit()
            except Exception: pass

    # æŠŠ urls æ‹†æˆ N ä»½ï¼Œæ¯ä¸ª worker å¤„ç†ä¸€ä»½
    CHUNK = max(1, (len(indexed) + max_workers - 1) // max_workers)
    chunks = [indexed[i:i+CHUNK] for i in range(0, len(indexed), CHUNK)]

    ok, fail = 0, 0
    with ThreadPoolExecutor(max_workers=max_workers, thread_name_prefix="very") as ex:
        futures = [ex.submit(_worker, chunk) for chunk in chunks]
        for fut in as_completed(futures):
            for idx, u, path, err in fut.result():
                if err is None:
                    ok += 1
                    print(f"[å®Œæˆ] [{idx}/{total}] âœ… {u} -> {path}")
                else:
                    fail += 1
                    print(f"[å¤±è´¥] [{idx}/{total}] âŒ {u}\n    {repr(err)}")

    print(f"\nğŸ“¦ ä»»åŠ¡ç»“æŸï¼šæˆåŠŸ {ok}ï¼Œå¤±è´¥ {fail}ï¼Œæ€»è®¡ {total}")

if __name__ == "__main__":
    very_fetch_info()
