# outdoorandcountry_fetch_info.py
# -*- coding: utf-8 -*-

import time
import json
import re
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed

import undetected_chromedriver as uc
from bs4 import BeautifulSoup
from urllib.parse import urlparse, parse_qs, unquote

from config import BARBOUR
from barbour.supplier.outdoorandcountry_parse_offer_info import parse_offer_info

# â¬‡ï¸ ç»Ÿä¸€åˆ°â€œé²¸èŠ½/é€šç”¨â€TXT å†™å…¥å™¨ï¼ˆä¸ camper / clarks_jingya ç›¸åŒï¼‰
# å¦‚æœä½ çš„å†™å…¥å™¨è·¯å¾„æ˜¯ txt_writer.pyï¼Œå°±æ”¹æˆ: from txt_writer import format_txt
from common_taobao.txt_writer import format_txt


def accept_cookies(driver, timeout=8):
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    try:
        WebDriverWait(driver, timeout).until(
            EC.element_to_be_clickable((By.ID, "onetrust-accept-btn-handler"))
        ).click()
        time.sleep(1)
    except Exception:
        pass


def _normalize_color_from_url(url: str) -> str:
    """
    è§£æ ?c= é¢œè‰²å‚æ•°ï¼Œå¹¶è§„èŒƒåŒ–ï¼š
    - URL è§£ç ï¼ˆ%2F -> /, %20 -> ç©ºæ ¼ï¼‰
    - å‹ç¼©å¤šä½™ç©ºç™½
    - æŠŠæ–œæ ä¸¤ä¾§åŠ ç©ºæ ¼ï¼Œç»Ÿä¸€ä¸º ' / '
    - æ¯ä¸ªè¯é¦–å­—æ¯å¤§å†™
    """
    try:
        qs = parse_qs(urlparse(url).query)
        c = qs.get("c", [None])[0]
        if not c:
            return ""
        c = unquote(c)
        c = c.replace("\\", "/")
        c = re.sub(r"\s*/\s*", " / ", c)
        c = re.sub(r"\s+", " ", c).strip()
        c = " ".join(w.capitalize() for w in c.split(" "))
        return c
    except Exception:
        return ""


def sanitize_filename(name: str) -> str:
    """å°†æ–‡ä»¶åä¸­éæ³•å­—ç¬¦æ›¿æ¢æˆä¸‹åˆ’çº¿ï¼Œé¿å…åˆ›å»ºå­ç›®å½•"""
    return re.sub(r"[\\/:*?\"<>|'\\s]+", "_", (name or "").strip())


def _extract_description(html: str) -> str:
    soup = BeautifulSoup(html, "html.parser")
    tag = soup.find("meta", attrs={"property": "og:description"})
    if tag and tag.get("content"):
        desc = tag["content"].replace("<br>", "").replace("<br/>", "").replace("<br />", "")
        return desc.strip()
    tab = soup.select_one(".product_tabs .tab_content[data-id='0'] div")
    return tab.get_text(" ", strip=True) if tab else "No Data"


def _extract_features(html: str) -> str:
    soup = BeautifulSoup(html, "html.parser")
    h3 = soup.find("h3", attrs={"title": "Features"})
    if h3:
        ul = h3.find_next("ul")
        if ul:
            items = [li.get_text(" ", strip=True) for li in ul.find_all("li")]
            if items:
                return " | ".join(items)
    return "No Data"


def _infer_gender_from_name(name: str) -> str:
    n = (name or "").lower()
    if any(x in n for x in ["men", "men's", "mens"]):
        return "ç”·æ¬¾"
    if any(x in n for x in ["women", "women's", "womens", "ladies", "lady"]):
        return "å¥³æ¬¾"
    if any(x in n for x in ["kid", "kids", "child", "children", "boys", "girls", "boy's", "girl's"]):
        return "ç«¥æ¬¾"
    return "æœªçŸ¥"


def _extract_color_code_from_jsonld(html: str) -> str:
    """
    Outdoor & Country çš„ JSON-LD é‡Œï¼Œå•ä¸ª offer.mpn å½¢å¦‚: MWX0017OL9934
                                    ^^^^ é¢œè‰²ç ï¼ˆç¤ºä¾‹ OL99ï¼‰ï¼Œåé¢ä¸¤ä½ä¸ºå°ºç ç¼–ç ã€‚
    è¿™é‡Œå–æœ«å°¾ 4 ä½é¢œè‰²å—ï¼ˆOL99ï¼‰ã€‚è‹¥è¦â€œå…¨ç â€ï¼ˆå¦‚ MWX0017OL99ï¼‰ï¼Œ
    è¯·åœ¨ parse_offer_info é‡Œæ‹¼æ¥ style code ä¸æ­¤é¢œè‰²å—ã€‚
    """
    soup = BeautifulSoup(html, "html.parser")
    for script in soup.find_all("script", type="application/ld+json"):
        try:
            data = script.string and script.string.strip()
            if not data:
                continue
            j = json.loads(data)
            if isinstance(j, dict) and j.get("@type") == "Product" and isinstance(j.get("offers"), list):
                for off in j["offers"]:
                    mpn = (off or {}).get("mpn")
                    if isinstance(mpn, str):
                        m = re.search(r'([A-Z]{2}\d{2})(\d{2})$', mpn)
                        if m:
                            return m.group(1)  # e.g. "OL99"
        except Exception:
            continue
    return ""


def process_url(url: str, output_dir: Path):
    options = uc.ChromeOptions()
    options.add_argument("--start-maximized")
    # å¦‚éœ€æ— å¤´ï¼šoptions.add_argument("--headless=new")
    driver = uc.Chrome(options=options)

    try:
        print(f"\nğŸŒ æ­£åœ¨æŠ“å–: {url}")
        driver.get(url)
        accept_cookies(driver)
        time.sleep(3)
        html = driver.page_source

        # é¢œè‰²åä» URL å…œåº•ï¼ˆ?c= å‚æ•°ï¼‰
        url_color = _normalize_color_from_url(url)

        # 1) å…ˆç”¨ä½ ç°æœ‰è§£æå™¨æ‹¿ç»“æ„åŒ–ä¿¡æ¯ï¼ˆOffersã€æ ‡é¢˜ã€é¢œè‰²ã€å¯èƒ½çš„ç ç­‰ï¼‰
        info = parse_offer_info(html, url)
        if not isinstance(info, dict):
            info = {}

        # 2) ç»Ÿä¸€è¡¥å…¨å­—æ®µï¼ˆä¸é²¸èŠ½æ¨¡æ¿å¯¹é½ï¼‰
        info.setdefault("Product Name", "No Data")
        # Outdoor åŸæ¥å¤šç”¨ "Product Color"ï¼›å†™å…¥å™¨ä¹Ÿæ”¯æŒ "Product Colour"ï¼Œè¿™é‡Œä¸¤ä¸ªé”®éƒ½å†™ï¼Œæœ€å¤§å…¼å®¹
        colour = info.get("Product Color") or url_color or "No Data"
        info["Product Color"] = colour
        info["Product Colour"] = colour
        info.setdefault("Site Name", "Outdoor and Country")
        info.setdefault("Source URL", url)
        info.setdefault("Offers", [])

        # æè¿° / å–ç‚¹ / æ€§åˆ«
        info["Product Description"] = _extract_description(html)
        info["Feature"] = _extract_features(html)
        info["Product Gender"] = _infer_gender_from_name(info.get("Product Name", ""))

        # é¢œè‰²ç¼–ç ï¼ˆç”¨äºæ–‡ä»¶å & TXT å­—æ®µï¼‰
        color_code = info.get("Product Color Code") or _extract_color_code_from_jsonld(html)
        if color_code:
            info["Product Color Code"] = color_code

        # è‹¥ parse_offer_info å·²è§£æå‡ºâ€œå®Œæ•´å•†å“ç ï¼ˆå¦‚ MWX0339NY92ï¼‰â€ï¼Œå¯å†™å…¥ Product Code
        # å†™å…¥å™¨åŒæ—¶å…¼å®¹ "Product Code" / "Product Color Code"
        if info.get("Product Code"):
            code_for_file = info["Product Code"]
        elif color_code:
            code_for_file = color_code
        else:
            # å›é€€ï¼šå_è‰²
            safe_name = sanitize_filename(info.get("Product Name", "NoName"))
            safe_color = sanitize_filename(info.get("Product Color", "NoColor"))
            code_for_file = f"{safe_name}_{safe_color}"

        # 3) ç»Ÿä¸€å†™ TXTï¼ˆä¸ camper/clarks_jingya å®Œå…¨ä¸€è‡´ï¼‰
        txt_path = output_dir / f"{sanitize_filename(code_for_file)}.txt"
        format_txt(info, txt_path)  # âœ… ç»Ÿä¸€å†™å…¥å™¨
        print(f"âœ… å†™å…¥: {txt_path.name}")

    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {url}\n    {e}")
    finally:
        driver.quit()


def fetch_outdoor_product_offers_concurrent(max_workers=3):
    links_file = BARBOUR["LINKS_FILES"]["outdoorandcountry"]
    output_dir = BARBOUR["TXT_DIRS"]["outdoorandcountry"]
    output_dir.mkdir(parents=True, exist_ok=True)

    urls = []
    with open(links_file, "r", encoding="utf-8") as f:
        for line in f:
            url = line.strip()
            if url:
                urls.append(url)

    print(f"ğŸ”„ å¯åŠ¨å¤šçº¿ç¨‹æŠ“å–ï¼Œæ€»é“¾æ¥æ•°: {len(urls)}ï¼Œå¹¶å‘çº¿ç¨‹æ•°: {max_workers}")

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(process_url, url, output_dir) for url in urls]
        for _ in as_completed(futures):
            pass  # å¯åŠ è¿›åº¦æˆ–é”™è¯¯æ”¶é›†


if __name__ == "__main__":
    fetch_outdoor_product_offers_concurrent(max_workers=3)
