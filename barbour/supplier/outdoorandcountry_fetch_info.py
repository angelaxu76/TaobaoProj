import time
import json
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
import undetected_chromedriver as uc
from config import BARBOUR
from barbour.supplier.outdoorandcountry_parse_offer_info import parse_offer_info
from barbour.barbouir_write_offer_txt import write_supplier_offer_txt

def accept_cookies(driver, timeout=8):
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    try:
        WebDriverWait(driver, timeout).until(
            EC.element_to_be_clickable((By.ID, "onetrust-accept-btn-handler"))
        ).click()
        time.sleep(1)
    except:
        pass

import re


from urllib.parse import urlparse, parse_qs, unquote

def _normalize_color_from_url(url: str) -> str:
    """
    è§£æ ?c= é¢œè‰²å‚æ•°ï¼Œå¹¶è§„èŒƒåŒ–ï¼š
    - URL è§£ç ï¼ˆ%2F -> /, %20 -> ç©ºæ ¼ï¼‰
    - å‹ç¼©å¤šä½™ç©ºç™½
    - æŠŠæ–œæ ä¸¤ä¾§åŠ ç©ºæ ¼ï¼Œç»Ÿä¸€ä¸º ' / '
    - é¦–å­—æ¯å¤§å†™æ¯ä¸ªè¯ï¼Œä¾¿äºåŒ¹é…ç«™ç‚¹æ˜¾ç¤ºé¢œè‰²
    """
    try:
        qs = parse_qs(urlparse(url).query)
        c = qs.get("c", [None])[0]
        if not c:
            return ""
        c = unquote(c)              # %2F -> /
        c = c.replace("\\", "/")
        c = re.sub(r"\s*/\s*", " / ", c)   # ä¸¤ä¾§ç•™ç©ºæ ¼
        c = re.sub(r"\s+", " ", c).strip()
        c = " ".join(w.capitalize() for w in c.split(" "))
        return c
    except Exception:
        return ""


def sanitize_filename(name: str) -> str:
    """å°†æ–‡ä»¶åä¸­éæ³•å­—ç¬¦æ›¿æ¢æˆä¸‹åˆ’çº¿ï¼Œç¡®ä¿ä¸ä¼šåˆ›å»ºå­ç›®å½•"""
    return re.sub(r"[\\/:*?\"<>|'\s]+", "_", name.strip())

import re
from bs4 import BeautifulSoup

def _extract_description(html: str) -> str:
    soup = BeautifulSoup(html, "html.parser")
    tag = soup.find("meta", attrs={"property": "og:description"})
    if tag and tag.get("content"):
        # å»æ‰ <br> çš„å®ä½“
        desc = tag["content"].replace("<br>", "").replace("<br/>", "").replace("<br />", "")
        return desc.strip()
    # å…œåº•ï¼šæœ‰äº›é¡µç­¾é‡Œä¹Ÿæœ‰ Description æ–‡æœ¬
    tab = soup.select_one(".product_tabs .tab_content[data-id='0'] div")
    return tab.get_text(" ", strip=True) if tab else "No Data"

def _extract_features(html: str) -> str:
    soup = BeautifulSoup(html, "html.parser")
    h3 = soup.find("h3", attrs={"title": "Features"})
    if h3:
        ul = h3.find_next("ul")
        if ul:
            items = [li.get_text(" ", strip=True) for li in ul.find_all("li")]
            if items:
                return " | ".join(items)
    return "No Data"

def _infer_gender_from_name(name: str) -> str:
    n = (name or "").lower()
    if any(x in n for x in ["men", "men's", "mens"]):
        return "ç”·æ¬¾"
    if any(x in n for x in ["women", "women's", "womens", "ladies", "lady"]):
        return "å¥³æ¬¾"
    if any(x in n for x in ["kid", "kids", "child", "children", "boys", "girls", "boy's", "girl's"]):
        return "ç«¥æ¬¾"
    return "æœªçŸ¥"

def _extract_color_code_from_jsonld(html: str) -> str:
    """
    æœŸæœ› mpn å½¢å¦‚: MWX0017OL9934
                         ^^^^ ä¸ºé¢œè‰²ç ï¼Œæœ«å°¾ä¸¤ä½ä¸ºå°ºç 
    æ­£åˆ™: æ•è·æœ€å 4 ä½å­—æ¯æ•°å­—å— ([A-Z]{2}\d{2}), ä¸”å…¶åç´§è·Ÿ 2 ä½å°ºç æ•°å­—ã€‚
    """
    soup = BeautifulSoup(html, "html.parser")
    for script in soup.find_all("script", type="application/ld+json"):
        try:
            data = script.string and script.string.strip()
            if not data:
                continue
            j = json.loads(data)
            if isinstance(j, dict) and j.get("@type") == "Product" and isinstance(j.get("offers"), list):
                for off in j["offers"]:
                    mpn = (off or {}).get("mpn")
                    if isinstance(mpn, str):
                        m = re.search(r'([A-Z]{2}\d{2})(\d{2})$', mpn)
                        if m:
                            return m.group(1)  # e.g. OL99
        except Exception:
            continue
    return ""

def process_url(url, output_dir):
    import json
    import undetected_chromedriver as uc

    options = uc.ChromeOptions()
    options.add_argument("--start-maximized")
    # å¦‚éœ€æ— å¤´ï¼šoptions.add_argument("--headless=new")
    driver = uc.Chrome(options=options)

    try:
        print(f"\nğŸŒ æ­£åœ¨æŠ“å–: {url}")
        driver.get(url)
        accept_cookies(driver)
        time.sleep(3)
        html = driver.page_source


        url_color = _normalize_color_from_url(url)

        # å…ˆè·‘ä½ ç°æœ‰çš„è§£æï¼ˆåŒ…å« Offersã€Product Nameã€Color ç­‰ï¼‰
        info = parse_offer_info(html, url)

        if not isinstance(info, dict):
            info = {}

        info.setdefault("Product Name", "No Data")
        info.setdefault("Product Color", url_color or "No Data")
        info.setdefault("Site Name", "Outdoor and Country")
        info.setdefault("Product URL", url)
        info.setdefault("Offers", [])
        # --- æ–°å¢è¡¥å…¨å­—æ®µ ---
        # æè¿°
        info["Product Description"] = _extract_description(html)
        # Feature
        info["Feature"] = _extract_features(html)
        # æ€§åˆ«
        info["Product Gender"] = _infer_gender_from_name(info.get("Product Name", ""))

        # é¢œè‰²ç¼–ç ï¼ˆç”¨äºæ–‡ä»¶å & å†™å…¥æ–‡æœ¬ï¼šProduct Color Codeï¼‰
        # --- æ–°å¢/ç¡®ä¿æ‹¿åˆ°é¢œè‰²ç¼–ç  ---
        color_code = info.get("Product Color Code") or _extract_color_code_from_jsonld(html)
        if color_code:
            info["Product Color Code"] = color_code  # ç¡®ä¿å†™å…¥åˆ°TXTé‡Œ

        # --- ç”¨ Product Color Code å‘½åæ–‡ä»¶ï¼›æ²¡æœ‰å†å›é€€åˆ° åç§°_é¢œè‰² ---
        if color_code:
            filename = f"{sanitize_filename(color_code)}.txt"
        else:
            safe_name = sanitize_filename(info.get('Product Name', 'NoName'))
            safe_color = sanitize_filename(info.get('Product Color', 'NoColor'))
            filename = f"{safe_name}_{safe_color}.txt"

        filepath = output_dir / filename
        write_supplier_offer_txt(info, filepath)
        print(f"âœ… å†™å…¥: {filepath.name}")

    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {url}\n    {e}")
    finally:
        driver.quit()



def fetch_outdoor_product_offers_concurrent(max_workers=3):
    links_file = BARBOUR["LINKS_FILES"]["outdoorandcountry"]
    output_dir = BARBOUR["TXT_DIRS"]["outdoorandcountry"]
    output_dir.mkdir(parents=True, exist_ok=True)

    urls = []
    with open(links_file, "r", encoding="utf-8") as f:
        for line in f:
            url = line.strip()
            if url:
                urls.append(url)

    print(f"ğŸ”„ å¯åŠ¨å¤šçº¿ç¨‹æŠ“å–ï¼Œæ€»é“¾æ¥æ•°: {len(urls)}ï¼Œå¹¶å‘çº¿ç¨‹æ•°: {max_workers}")

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(process_url, url, output_dir) for url in urls]

        for future in as_completed(futures):
            pass  # å¯æ·»åŠ è¿›åº¦æ˜¾ç¤ºæˆ–å¼‚å¸¸æ•è·

if __name__ == "__main__":
    fetch_outdoor_product_offers_concurrent(max_workers=3)

