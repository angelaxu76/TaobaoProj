# -*- coding: utf-8 -*-
"""
Terraces Menswear å•†å“ä¿¡æ¯æŠ“å–ï¼ˆä¸ very/houseoffraser ä¿æŒä¸€è‡´çš„å­—æ®µä¸ç›®å½•è§„èŒƒï¼‰
- è¯»å–ï¼šconfig.BARBOUR["LINKS_FILES"]["terraces"]
- è¾“å‡ºï¼šconfig.BARBOUR["TXT_DIRS"]["terraces"] / <CleanTitle>_<hash4>.txt
- å­—æ®µï¼ˆé¡ºåºå¯¹é½ HOF/Veryï¼‰ï¼š
  Product Code, Product Name, Product Description, Product Gender, Product Color,
  Product Price, Adjusted Price, Product Material, Style Category, Feature,
  Product Size, Product Size Detail, Source URL, Site Name
"""
import re
import json
import hashlib
from pathlib import Path
import requests
from bs4 import BeautifulSoup
from barbour.core.site_utils import assert_site_or_raise as canon
from barbour.core.sim_matcher import match_product, choose_best, explain_results
from sqlalchemy import create_engine
from config import BARBOUR, BRAND_CONFIG


# ==== æµè§ˆå™¨å…œåº•ï¼ˆä¸ very åŒé£æ ¼ï¼‰ ====
import shutil, subprocess, sys
import undetected_chromedriver as uc

# ===== æ ‡å‡†å°ºç è¡¨ï¼ˆç”¨äºè¡¥é½æœªå‡ºç°å°ºç =0ï¼‰ =====
WOMEN_ORDER = ["4","6","8","10","12","14","16","18","20"]
MEN_ALPHA_ORDER = ["2XS","XS","S","M","L","XL","2XL","3XL"]
MEN_NUM_ORDER = [str(n) for n in range(30, 52, 2)]  # 30..50ï¼ˆä¸å« 52ï¼‰

def _full_order_for_gender(gender: str) -> list[str]:
    """æ ¹æ®æ€§åˆ«è¿”å›å®Œæ•´å°ºç é¡ºåºï¼›Terraces ç«™æ•´ä½“ä¸ºç”·æ¬¾ï¼ŒæœªçŸ¥ä¹ŸæŒ‰ç”·æ¬¾å¤„ç†ã€‚"""
    g = (gender or "").lower()
    if "å¥³" in g or "women" in g or "ladies" in g:
        return WOMEN_ORDER
    return MEN_ALPHA_ORDER + MEN_NUM_ORDER


def _get_chrome_major_version() -> int | None:
    try:
        import winreg
    except Exception:
        winreg = None
    if winreg is not None and sys.platform.startswith("win"):
        reg_paths = [
            (winreg.HKEY_CURRENT_USER,  r"SOFTWARE\Google\Chrome\BLBeacon"),
            (winreg.HKEY_LOCAL_MACHINE, r"SOFTWARE\Google\Chrome\BLBeacon"),
            (winreg.HKEY_LOCAL_MACHINE, r"SOFTWARE\WOW6432Node\Google\Chrome\BLBeacon"),
        ]
        for hive, path in reg_paths:
            try:
                with winreg.OpenKey(hive, path) as k:
                    ver, _ = winreg.QueryValueEx(k, "version")
                    m = re.search(r"^(\d+)\.", ver)
                    if m:
                        return int(m.group(1))
            except OSError:
                pass
    for exe in ["chrome", r"C:\Program Files\Google\Chrome\Application\chrome.exe",
                r"C:\Program Files (x86)\Google\Chrome\Application\chrome.exe"]:
        path = shutil.which(exe) or exe
        try:
            out = subprocess.check_output([path, "--version"], stderr=subprocess.STDOUT, text=True, timeout=3)
            m = re.search(r"(\d+)\.\d+\.\d+\.\d+", out)
            if m:
                return int(m.group(1))
        except Exception:
            continue
    return None

def _get_uc_driver(headless: bool = True):
    def make_options():
        opts = uc.ChromeOptions()
        if headless: opts.add_argument("--headless=new")
        opts.add_argument("--disable-blink-features=AutomationControlled")
        opts.add_argument("--start-maximized")
        opts.add_argument("--disable-gpu")
        opts.add_argument("--no-sandbox")
        return opts
    last_err = None
    try:
        return uc.Chrome(options=make_options(), headless=headless, use_subprocess=True)
    except Exception as e:
        last_err = e
    try:
        vm = _get_chrome_major_version()
        if vm:
            return uc.Chrome(options=make_options(), headless=headless, use_subprocess=True, version_main=vm)
    except Exception as e2:
        last_err = e2
    raise last_err


PRODUCTS_TABLE: str = BRAND_CONFIG.get("barbour", {}).get("PRODUCTS_TABLE", "barbour_products")
PG = BRAND_CONFIG["barbour"]["PGSQL_CONFIG"]  # âœ… æ³¨æ„è¿™é‡Œ


# é¡¹ç›®å†…çš„é€šç”¨ TXT å†™å…¥ï¼ˆè‹¥å­˜åœ¨åˆ™ä¼˜å…ˆä½¿ç”¨ï¼Œå­—æ®µ/é¡ºåºå°†ä¸å…¨ç«™ä¸€è‡´ï¼‰
try:
    from common_taobao.txt_writer import format_txt as write_txt
except Exception:
    write_txt = None  # ç”¨æœ¬æ–‡ä»¶çš„ fallback å†™ç›˜

from config import BARBOUR  # å¤ç”¨å…¨å±€é…ç½®ï¼ˆå« LINKS_FILES / TXT_DIRSï¼‰

SUPPLIER_KEY  = "terraces"
SITE_NAME     = canon(SUPPLIER_KEY)   # âœ… æŒ‰ config æ ‡å‡†åŒ–
UA            = ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                 "AppleWebKit/537.36 (KHTML, like Gecko) "
                 "Chrome/119.0.0.0 Safari/537.36")
HEADERS       = {
    "User-Agent": UA,
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Language": "en-GB,en;q=0.9",
}

import random
import time
from urllib.parse import urljoin

UA_POOL = [
    # æŒ‘å‡ æ¡å¸¸è§æ¡Œé¢ UA è½®æ¢
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 13_4) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.5 Safari/605.1.15",
]
BASE_HOME = "https://www.terracesmenswear.co.uk/"
LISTING_REFERER = "https://www.terracesmenswear.co.uk/mens-outlet"

engine_url = (
    f"postgresql+psycopg2://{PG['user']}:{PG['password']}"
    f"@{PG['host']}:{PG['port']}/{PG['dbname']}"
)
_ENGINE = create_engine(engine_url)

def get_raw_connection():
    return _ENGINE.raw_connection()


# ==================== å·¥å…·å‡½æ•° ====================
def _safe_filename(s: str) -> str:
    s = re.sub(r"[^\w\s\-\_\.]+", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s.replace(" ", "_")[:150] or "No_Data"

def _short_hash(text: str) -> str:
    return hashlib.md5((text or "").encode("utf-8")).hexdigest()[:4]

def _text(el) -> str:
    return re.sub(r"\s+", " ", el.get_text(" ", strip=True)).strip() if el else ""

def _split_title_color(title: str) -> tuple[str, str | None]:
    """
    æ ‡é¢˜ä¸€èˆ¬æ˜¯ â€œName - Colorâ€ ç»“æ„ã€‚
    è¿”å› (å‡€åŒ–åçš„æ ‡é¢˜, é¢œè‰²)ï¼Œè‹¥æ— é¢œè‰²è¿”å› (åŸæ ‡é¢˜, None)
    """
    t = (title or "").strip()
    if not t:
        return "No Data", None
    parts = [p.strip() for p in re.split(r"\s*-\s*", t) if p.strip()]
    if len(parts) >= 2:
        raw_color = parts[-1]
        # å¤šè¯é¢œè‰²ä»…å–ç¬¬ä¸€ä¸ªä¸»è¯ï¼ˆBeige/Stone -> Beigeï¼‰
        color = re.split(r"[\/&]", re.sub(r"[^\w\s/&-]", "", raw_color))[0].strip()
        color = color.title() if color else None
        clean_title = " - ".join(parts[:-1])  # å»æ‰é¢œè‰²å°¾å·´
        return (clean_title or t, color or None)
    return t, None

def _parse_json_ld(soup: BeautifulSoup) -> dict:
    """
    è§£æé¡µé¢ä¸­çš„ JSON-LDï¼Œè¿”å›æœ€ç›¸å…³çš„ dictï¼›å¤±è´¥è¿”å› {}ã€‚
    """
    for tag in soup.find_all("script", {"type": "application/ld+json"}):
        try:
            obj = json.loads(tag.string or tag.get_text() or "{}")
            cand = obj[0] if isinstance(obj, list) and obj else obj
            if isinstance(cand, dict) and ("name" in cand or "offers" in cand or "color" in cand):
                return cand
        except Exception:
            continue
    return {}

# ==================== å°ºç /åº“å­˜æŠ½å–ï¼ˆDOM/JSON å¤šçº§ç­–ç•¥ï¼‰ ====================
_SIZE_PAT = re.compile(
    r"\b(One Size|OS|XXS|XS|S|M|L|XL|XXL|3XL|4XL|5|6|7|8|9|10|11|12|13|28|30|32|34|36|38|40|42)\b",
    re.I
)

def _extract_sizes(soup: BeautifulSoup, gender: str) -> tuple[list[str], str]:
    """
    è¿”å› (sizes_seen, size_detail)
    - sizes_seen: ç½‘é¡µä¸Šå‡ºç°åˆ°çš„å°ºç åˆ—è¡¨ï¼ˆä¿æŒåŸæ ·ï¼Œä¾¿äºå…¼å®¹ç°æœ‰ä¸‹æ¸¸ï¼‰
    - size_detail: æŒ‰å®Œæ•´å°ºç è¡¨è¾“å‡ºï¼Œå‡ºç°çš„å°ºç ç”¨ 3/0ï¼Œæœªå‡ºç°çš„å°ºç è¡¥ 0
    """
    sizes: list[str] = []
    avail: dict[str, int] = {}  # 1=æœ‰è´§, 0=æ— è´§

    # â€”â€” 1) product JSONï¼ˆä¼˜å…ˆï¼‰
    for tag in soup.find_all("script", {"type": "application/json"}):
        raw = (tag.string or tag.get_text() or "").strip()
        if not raw or "variants" not in raw:
            continue
        try:
            data = json.loads(raw)
        except Exception:
            continue
        variants = data.get("variants")
        if isinstance(variants, list) and variants:
            size_idx = None
            options = data.get("options")
            if isinstance(options, list):
                for i, name in enumerate(options, 1):
                    if str(name).strip().lower() in ("size", "sizes"):
                        size_idx = i
                        break
            for v in variants:
                is_avail = 1 if (v.get("available") or v.get("is_available") or v.get("in_stock")) else 0
                sz = None
                for key in filter(None, [f"option{size_idx}" if size_idx else None, "option1", "option2", "option3", "title"]):
                    val = v.get(key)
                    if val:
                        m = _SIZE_PAT.search(str(val))
                        if m:
                            sz = m.group(0).strip()
                            break
                if sz:
                    if sz not in sizes:
                        sizes.append(sz)
                    # æœ‰è´§ä¼˜å…ˆï¼šè‹¥ä¹‹å‰æ ‡è®°æ— è´§ï¼Œè¿™é‡Œæœ‰è´§è¦†ç›–å®ƒ
                    if avail.get(sz, -1) != 1:
                        avail[sz] = 1 if is_avail else 0
            if sizes:
                break

    # â€”â€” 2) DOMï¼ˆå›é€€ï¼‰
    if not sizes:
        for lab in soup.select("label.size-wrap"):
            btn = lab.find("button", class_="size-box")
            if not btn:
                continue
            sz = (btn.get_text(" ", strip=True) or "").strip()
            if not sz or not _SIZE_PAT.search(sz):
                continue
            disabled = False
            inp = lab.find("input")
            if inp and (inp.has_attr("disabled") or str(inp.get("aria-disabled", "")).lower() == "true"):
                disabled = True
            cls = " ".join(lab.get("class", [])).lower()
            if "disabled" in cls or "sold" in cls:
                disabled = True
            if sz not in sizes:
                sizes.append(sz)
            if avail.get(sz, -1) != 1:
                avail[sz] = 0 if disabled else 1

    # â€”â€” 3) JSON-LDï¼ˆå…œåº•ï¼‰
    if not sizes:
        jl = _parse_json_ld(soup)
        off = jl.get("offers")
        if isinstance(off, list):
            for o in off:
                k = (o.get("name") or o.get("sku") or "")
                m = _SIZE_PAT.search(str(k))
                if m:
                    sz = m.group(0)
                    if sz not in sizes:
                        sizes.append(sz)
                    if avail.get(sz, -1) != 1:
                        avail[sz] = 1 if "InStock" in str(o.get("availability", "")) else 0

    # ===== ç»Ÿä¸€è¡¥é½ï¼šæŒ‰å®Œæ•´å°ºç è¡¨è¾“å‡º Product Size Detail =====
    EAN = "0000000000000"
    full_order = _full_order_for_gender(gender)

    # å³ä½¿å®Œå…¨æŠ“ä¸åˆ°å°ºç ï¼Œä¹Ÿè¾“å‡ºå®Œæ•´ 0 æ …æ ¼ï¼Œé¿å…ä¸‹æ¸¸ç¼ºè¡Œ
    if not sizes:
        detail = ";".join(f"{s}:0:{EAN}" for s in full_order)
        return [], detail

    # å·²æŠ“åˆ°éƒ¨åˆ†å°ºç ï¼šå‡ºç°çš„æŒ‰ avail å†™ 3/0ï¼Œæœªå‡ºç°çš„è¡¥ 0
    detail = ";".join(f"{s}:{3 if avail.get(s, 0)==1 else 0}:{EAN}" for s in full_order)
    return sizes, detail


# ==================== é¡µé¢è§£æ ====================
def _price_to_num(s: str) -> str:
    s = (s or "").replace(",", "").strip()
    m = re.search(r"(\d+(?:\.\d+)?)", s)
    return m.group(1) if m else "No Data"

def _sleep_jitter(base: float = 1.0):
    time.sleep(base * random.uniform(0.6, 1.4))

def _refresh_session(sess: requests.Session):
    """é¢„çƒ­é¦–é¡µï¼Œåˆ·æ–° cookie / anti-bot token"""
    try:
        sess.get(BASE_HOME, timeout=20)
    except Exception:
        pass

def fetch_product_html(sess: requests.Session, url: str, timeout: int = 25) -> str | None:
    """
    å…ˆ requestsï¼ˆå¸¦ Refererï¼‰ï¼Œå¤±è´¥/è¢«æŒ¡å†å›é€€ UC æµè§ˆå™¨æ‹¿ page_sourceã€‚
    é™„å¸¦æ›´å¤šæ—¥å¿—ï¼Œä¾¿äºå®šä½ï¼šæ‰“å° HTTP çŠ¶æ€ç åŠé‡è¯•è·¯å¾„ã€‚
    """
    base_headers = {
        "User-Agent": sess.headers.get("User-Agent", random.choice(UA_POOL)),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
        "Accept-Language": "en-GB,en;q=0.9",
        "Cache-Control": "no-cache",
        "Pragma": "no-cache",
        "Referer": LISTING_REFERER,
    }

    # 1) requests ç›´è¿
    try:
        r = sess.get(url, headers=base_headers, timeout=timeout)
        if r.status_code == 200 and r.text and ("<html" in r.text.lower()):
            return r.text
        print(f"  â†ª requests got status {r.status_code} for {url}")
    except Exception as e:
        print(f"  â†ª requests error: {e!r}")

    # 2) requests å†è¯•ä¸€æ¬¡ï¼ˆè¡¥å°¾æ–œæ ï¼‰
    try:
        u2 = url if url.endswith("/") else (url + "/")
        r2 = sess.get(u2, headers=base_headers, timeout=timeout)
        if r2.status_code == 200 and r2.text and ("<html" in r2.text.lower()):
            return r2.text
        print(f"  â†ª requests(/{''}) got status {r2.status_code} for {u2}")
    except Exception as e2:
        print(f"  â†ª requests(/{''}) error: {e2!r}")

    # 3) â€”â€” æµè§ˆå™¨å…œåº•ï¼ˆæœ€ç¨³ï¼Œä½†è¾ƒæ…¢ï¼‰â€”â€”
    try:
        drv = _get_uc_driver(headless=True)
        try:
            drv.get(url)
            time.sleep(random.uniform(1.2, 2.2))  # è½»å¾®æŠ–åŠ¨
            html = drv.page_source
            if html and ("<html" in html.lower()):
                print("  â†ª UC fallback succeeded")
                return html
        finally:
            try: drv.quit()
            except Exception: pass
    except Exception as e3:
        print(f"  â†ª UC fallback error: {e3!r}")

    return None



def _parse_page(html: str, url: str) -> dict:
    soup = BeautifulSoup(html, "lxml")

    # åç§° & é¢œè‰²ï¼ˆæ ‡é¢˜ï¼‰
    h1 = soup.select_one("h1.primary-title a") or soup.select_one("h1.primary-title")
    raw_title = _text(h1)

    # a > span é‡Œå¸¸æ”¾é¢œè‰²ï¼›ä»æ ‡é¢˜å°¾éƒ¨å»æ‰ï¼Œé¿å… "Navy Navy"
    color_from_span = ""
    if h1:
        span = h1.find("span")
        if span:
            color_from_span = _text(span)
            if color_from_span:
                raw_title = re.sub(rf"\s*\b{re.escape(color_from_span)}\b\s*$", "", raw_title, flags=re.I).strip()

    # æ ‡é¢˜ â€œName - Colorâ€ åˆ‡åˆ†å…œåº•
    name_from_split, color_from_title = _split_title_color(raw_title)

    # æœ€ç»ˆåç§°
    name = name_from_split or raw_title or "No Data"
    if name == "No Data":
        og = soup.find("meta", {"property": "og:title"})
        if og and og.get("content"):
            name = og["content"].strip()

    # æœ€ç»ˆé¢œè‰²ï¼šspan > æ ‡é¢˜å°¾å·´ > JSON-LD
    color = color_from_span or color_from_title
    jsonld = _parse_json_ld(soup)
    if not color and isinstance(jsonld, dict):
        color = (jsonld.get("color") or "").strip()

    # å†æ¸…ç†ä¸€æ¬¡ï¼šæŠŠé¢œè‰²è¯ä»åç§°å°¾éƒ¨å»æ‰ï¼ˆé˜²é‡å¤ï¼‰
    if color:
        name = re.sub(rf"(?:\s+\b{re.escape(color)}\b)+\s*$", "", name, flags=re.I).strip()

    # ä»·æ ¼ï¼šç°ä»·ï¼ˆAdjustedï¼‰/ åŸä»·ï¼ˆProductï¼‰
    price_wrap = soup.select_one(".product__short-description .product__price") or soup.select_one(".product__price")
    adjusted_price = product_price = "No Data"
    if price_wrap:
        curr = price_wrap.select_one(".price:not(.price--compare)")
        comp = price_wrap.select_one(".price--compare")
        if curr:   adjusted_price = _price_to_num(_text(curr))
        if comp:   product_price  = _price_to_num(_text(comp))
    if (adjusted_price == "No Data" or product_price == "No Data") and isinstance(jsonld, dict):
        offers = jsonld.get("offers")
        if isinstance(offers, dict) and offers.get("price"):
            adjusted_price = _price_to_num(str(offers.get("price")))
            if product_price == "No Data":
                product_price = adjusted_price

    # å°ºç  & åº“å­˜
    gender = "Men"  # ç«™ç‚¹å…¨ä¸ºç”·æ¬¾
    sizes, size_detail = _extract_sizes(soup, gender)



    # Feature & Descriptionï¼ˆDetails æ¨¡å—ï¼›å†å…œåº• JSON-LD / meta descriptionï¼‰
    features: list[str] = []
    for head in soup.select(".section.product__details h3"):
        if "details" in _text(head).lower():
            ul = head.find_next("ul")
            if ul:
                features = [_text(li) for li in ul.find_all("li")]
                break
    if not features:
        ul = soup.select_one(".section.product__details ul")
        if ul:
            features = [_text(li) for li in ul.find_all("li")]
    description = features[0] if features else ""
    if not description and isinstance(jsonld, dict) and jsonld.get("description"):
        description = (jsonld["description"] or "").strip()
    if not description:
        meta = soup.find("meta", {"name": "description"})
        if meta and meta.get("content"):
            description = meta["content"].strip()

    # å›ºå®šå­—æ®µ
    gender         = "Men"       # ç«™ç‚¹å…¨ä¸ºç”·æ¬¾
    product_code   = "No Data"   # ç«™ç‚¹æœªæä¾›æ¬¾å·/æ¡ç 
    product_mat    = "No Data"   # æœªæä¾›æè´¨
    style_category = "No Data"   # æœªæä¾›ç±»ç›®
    feature_join   = "; ".join(features) if features else "No Data"

        # ========== å•†å“ç¼–ç åŒ¹é… ==========
    product_code = "No Data"
    try:
        raw_conn = get_raw_connection() 
        results = match_product(
            raw_conn,
            scraped_title=name,
            scraped_color=color,
            table=PRODUCTS_TABLE,   # å»ºè®®ç”¨ç»Ÿä¸€é…ç½®è¡¨å
            name_weight=0.72,
            color_weight=0.18,
            type_weight=0.10,
            topk=5,
            recall_limit=2000,
            min_name=0.92,
            min_color=0.85,
            require_color_exact=False,
            require_type=False,
        )
        product_code = choose_best(results)
        print("ğŸ” match debug")
        print(f"  raw_title: {name}")
        print(f"  raw_color: {color}")
        
    except Exception as e:
        print(f"âŒ åŒ¹é…å¤±è´¥: {e}")

    # è¿”å›ä¸ HOF/Very å¯¹é½çš„é”®å
    return {
        "Product Code":        product_code,
        "Product Name":        name or "No Data",
        "Product Description": description or "No Data",
        "Product Gender":      gender,
        "Product Color":       color or "No Data",
        "Product Price":       product_price or "No Data",
        "Adjusted Price":      adjusted_price or "No Data",
        "Product Material":    product_mat,
        "Style Category":      style_category,
        "Feature":             feature_join,
        "Product Size":        ";".join(sizes) if sizes else "No Data",
        "Product Size Detail": size_detail or "No Data",
        "Source URL":          url,
        "Site Name":           SITE_NAME,
    }

def _resolve_output_path(info: dict, out_dir: Path) -> Path:
    """
    æœ‰ç¼–ç  â†’ {code}.txt
    æ— ç¼–ç  â†’ _UNMATCHED/<CleanTitle[_Color]>_<hash4>.txt
    """
    code = (info.get("Product Code") or "").strip()
    if code and code.lower() != "no data":
        return out_dir / f"{code}.txt"

    # â€”â€” æ²¡åŒ¹é…ä¸Šï¼šæ”¾å…¥ _UNMATCHED å­ç›®å½•ï¼Œé¿å…ä¸å·²åŒ¹é…æ··åœ¨ä¸€èµ·
    subdir = out_dir / "_UNMATCHED"
    subdir.mkdir(parents=True, exist_ok=True)

    title = info.get("Product Name") or "No_Data"
    color = info.get("Product Color") or ""
    base  = _safe_filename(f"{title}" + (f"_{color}" if color and color != "No Data" else ""))
    # ç”¨ Source URL + æ ‡é¢˜ ç”ŸæˆçŸ­å“ˆå¸Œï¼Œé™ä½é‡åé£é™©
    suffix = _short_hash((info.get("Source URL") or "") + title)
    return subdir / f"{base}_{suffix}.txt"


# ==================== å†™ç›˜ ====================
def _write_txt(info: dict, out_dir: Path) -> Path:
    title = info.get("Product Name") or "No_Data"
    color = info.get("Product Color") or ""
    base  = _safe_filename(f"{title}" + (f"_{color}" if color and color != "No Data" else ""))
    code = info.get("Product Code") or "NoData"
    path = _resolve_output_path(info, out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    if write_txt is not None:
        # ä½¿ç”¨é¡¹ç›®å†…æ ¼å¼åŒ–å†™å…¥ï¼ˆä¸å…¶ä»–ç«™ç‚¹å®Œå…¨ä¸€è‡´ï¼‰
        write_txt(info, path)
    else:
        # è½»é‡ fallbackï¼šä½¿ç”¨ä¸ HOF/Very ä¸€è‡´çš„å­—æ®µé¡ºåºå†™å…¥
        order = [
            "Product Code","Product Name","Product Description","Product Gender",
            "Product Color","Product Price","Adjusted Price","Product Material",
            "Style Category","Feature","Product Size","Product Size Detail",
            "Source URL","Site Name"
        ]
        with open(path, "w", encoding="utf-8") as f:
            for k in order:
                f.write(f"{k}: {info.get(k, 'No Data')}\n")

    print(f"âœ… å†™å…¥: {path.name} (code={info.get('Product Code')})")
    return path

# ==================== ä¸»æµç¨‹ ====================
def terraces_fetch_info(max_count: int | None = None, timeout: int = 30) -> None:
    links_file = Path(BARBOUR["LINKS_FILES"][SUPPLIER_KEY])
    out_dir    = Path(BARBOUR["TXT_DIRS"][SUPPLIER_KEY])

    if not links_file.exists():
        print(f"âŒ æœªæ‰¾åˆ°é“¾æ¥æ–‡ä»¶: {links_file}")
        return

    urls = [ln.strip() for ln in links_file.read_text(encoding="utf-8").splitlines() if ln.strip()]
    total = len(urls)
    if max_count is not None:
        urls = urls[:max_count]
    print(f"ğŸ“„ å…± {len(urls)} / {total} ä¸ªå•†å“é¡µé¢å¾…è§£æ...")

    sess = requests.Session()
    # åˆå§‹ UA + é¢„çƒ­é¦–é¡µæ‹¿ cookie
    sess.headers.update({"User-Agent": random.choice(UA_POOL)})
    _refresh_session(sess)

    ok = fail = 0
    failed_links_path = out_dir.parent / "terraces_failed.txt"
    failed = []

    for i, url in enumerate(urls, 1):
        try:
            # æ¯æŠ“ 40 ä¸ªçŸ­æš‚åœé¡¿ï¼Œé™ä½è§¦å‘ç‡
            if i % 40 == 0:
                print("â³ èŠ‚æµä¸­ï¼ˆçŸ­æš‚ä¼‘æ¯ 8sï¼‰...")
                time.sleep(8)

            print(f"\nğŸŒ æ­£åœ¨æŠ“å–: {url}  [{i}/{len(urls)}]")
            html = fetch_product_html(sess, url, timeout=timeout)
            if not html:
                raise requests.HTTPError("fetch_product_html returned None")

            info = _parse_page(html, url)
            _write_txt(info, out_dir)
            ok += 1

        except Exception as e:
            fail += 1
            failed.append(url)
            print(f"[å¤±è´¥] [{i}/{len(urls)}] âŒ {url}\n    {repr(e)}")

    if failed:
        failed_links_path.parent.mkdir(parents=True, exist_ok=True)
        failed_links_path.write_text("\n".join(failed), encoding="utf-8")
        print(f"\nâš ï¸ å·²å°†å¤±è´¥é“¾æ¥å†™å…¥: {failed_links_path}")

    print(f"\nå®Œæˆï¼šæˆåŠŸ {ok}ï¼Œå¤±è´¥ {fail}ï¼Œè¾“å‡ºç›®å½•ï¼š{out_dir}")


if __name__ == "__main__":
    terraces_fetch_info()
