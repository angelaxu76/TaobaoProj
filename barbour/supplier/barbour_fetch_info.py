import os
import re
import json
import requests
from bs4 import BeautifulSoup
from pathlib import Path
from config import BARBOUR
from barbour.barbouir_write_offer_txt import write_barbour_product_txt

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36"
}

# ------ å·¥å…·å‡½æ•° ------

def _safe_json_loads(text: str):
    """å°è¯•å°†å­—ç¬¦ä¸²è§£æä¸º JSONï¼›å¤±è´¥åˆ™è¿”å› Noneã€‚"""
    try:
        return json.loads(text)
    except Exception:
        return None

def _extract_gender_from_html(html: str) -> str:
    """
    ä» gtmAnalytics çš„ items[0].item_category æå–æ€§åˆ«ï¼š
      womens -> å¥³æ¬¾, mens -> ç”·æ¬¾, kids -> ç«¥æ¬¾
    """
    # 1) å…ˆç²—æš´æ­£åˆ™å…œåº•ï¼ˆæœ€é²æ£’ã€å¯¹ script å†…éƒ¨ç»“æ„å˜åŠ¨ä¸æ•æ„Ÿï¼‰
    m = re.search(r'"item_category"\s*:\s*"([^"]+)"', html, re.IGNORECASE)
    gender_raw = m.group(1).strip().lower() if m else ""

    # 2) è‹¥æ­£åˆ™æ²¡å‘½ä¸­ï¼Œå†å°è¯•è§£æå¯èƒ½çš„ JSON ç‰‡æ®µï¼ˆæ›´è¯­ä¹‰åŒ–ï¼‰
    if not gender_raw:
        soup = BeautifulSoup(html, "html.parser")
        for script in soup.find_all("script"):
            if not script.string:
                continue
            s = script.string.strip()
            if "gtmAnalytics" in s and "items" in s and "item_category" in s:
                # æå– "gtmAnalytics": {...} é‡Œçš„ JSON ä½“ï¼ˆå°½é‡å°å¿ƒåŒ¹é…ï¼‰
                # ç®€åŒ–åšæ³•ï¼šç›´æ¥æ­£åˆ™æ‹¿ item_category å€¼
                m2 = re.search(r'"item_category"\s*:\s*"([^"]+)"', s, re.IGNORECASE)
                if m2:
                    gender_raw = m2.group(1).strip().lower()
                    break

    mapping = {
        "womens": "å¥³æ¬¾",
        "women": "å¥³æ¬¾",
        "ladies": "å¥³æ¬¾",
        "mens": "ç”·æ¬¾",
        "men": "ç”·æ¬¾",
        "kids": "ç«¥æ¬¾",
        "child": "ç«¥æ¬¾",
        "children": "ç«¥æ¬¾",
        "unisex": "é€šç”¨"
    }
    return mapping.get(gender_raw, "æœªçŸ¥")

def _extract_material_from_features(features_text: str) -> str:
    """
    ä» features æ–‡æœ¬ä¸­æå–â€œä¸»ä½“æè´¨â€ã€‚
    è§„åˆ™ï¼š
      1) å…ˆæ‰¾ Outer / Shell / Fabric / Material ç­‰å‰ç¼€åé¢çš„é…æ–¹ï¼›
      2) å¦åˆ™åœ¨æ•´ä¸²é‡Œæ‰¾ç¬¬ä¸€ä¸ª â€œxx% æè´¨â€ï¼›
      3) è¿‡æ»¤æ‰ Lining / Trim / Collar / Sleeve / Cuff ç­‰éä¸»ä½“å­—æ®µï¼›
      4) æœ€ç»ˆè¿”å›è¯¸å¦‚ â€œ100% Cottonâ€ æˆ–â€œ65% Polyester / 35% Cottonâ€ã€‚
    """
    if not features_text or features_text == "No Data":
        return "No Data"

    # ç»Ÿä¸€åˆ†éš”ï¼Œé¿å… HTML ä¸­çš„ <br>ã€æ¢è¡Œç­‰
    text = features_text.replace("\n", " ").replace("\r", " ")
    parts = re.split(r"\s*\|\s*|,\s+|;\s+|/+\s*", text)  # ä»¥ | æˆ–é€—å·/åˆ†å·/æ–œçº¿æ‹†åˆ†ï¼Œåç»­ä¼šé‡ç»„
    parts = [p.strip() for p in parts if p and p.strip()]

    # å±è”½éä¸»ä½“å­—æ®µå…³é”®è¯
    exclude_prefixes = ("lining", "trim", "collar", "sleeve", "cuff", "hood", "pocket")
    # ä¸»ä½“å­—æ®µå‰ç¼€å…³é”®è¯
    primary_prefixes = ("outer", "shell", "face", "fabric", "material", "main", "outer fabric", "outer shell")

    # 1) å…ˆå°è¯•ï¼šä»æ˜¾å¼â€œä¸»ä½“å…³é”®è¯â€ä¸­æå–ç™¾åˆ†æ¯”æè´¨
    for p in parts:
        low = p.lower()
        if any(low.startswith(pref) for pref in primary_prefixes):
            # ä¾‹ï¼š "Outer: 100% Polyamide" / "Fabric: 65% Polyester 35% Cotton"
            # å…ˆå‰¥ç¦»å‰ç¼€åŠå†’å·
            candidate = re.sub(r"^[A-Za-z ]+\s*:\s*", "", p).strip()
            # åŒ¹é…ä¸€ä¸ªæˆ–å¤šä¸ªâ€œç™¾åˆ†æ¯”+æè´¨â€
            mats = re.findall(r"\b\d{1,3}%\s+[A-Za-z][A-Za-z \-]*", candidate)
            if mats:
                # æœ‰æ—¶å¯èƒ½æ˜¯ "65% Polyester 35% Cotton" æ— åˆ†éš”ï¼Œå°è¯•æ‹†ä¸¤æ®µ
                # è¿½åŠ ç¬¬äºŒæ®µ
                joined = " / ".join(mats)
                return joined

    # 2) å†å°è¯•ï¼šå…¨å±€æ‰¾ç¬¬ä¸€å¤„ â€œxx% æè´¨â€ï¼Œä½†è¦è¿‡æ»¤éä¸»ä½“å­—æ®µ
    for p in parts:
        low = p.lower()
        if any(low.startswith(pref) for pref in exclude_prefixes):
            continue
        mats = re.findall(r"\b\d{1,3}%\s+[A-Za-z][A-Za-z \-]*", p)
        if mats:
            return " / ".join(mats)

    # 3) å…œåº•ï¼šå…¨é‡æ‰«æï¼ˆä¸æ’é™¤å‰ç¼€ï¼‰ï¼Œå–ç¬¬ä¸€ç»„
    mats_all = re.findall(r"\b\d{1,3}%\s+[A-Za-z][A-Za-z \-]*", text)
    if mats_all:
        return " / ".join(mats_all[:2])  # æœ€å¤šè¿”å›å‰ä¸¤æ®µï¼Œé¿å…è¿‡é•¿

    # 4) å†å…œåº•ï¼šå¦‚æœå‡ºç° "100% Cotton (Waxed)" è¿™ç±»ï¼Œå»æ‰æ‹¬å·åªå–æ ¸å¿ƒæè´¨è¯
    m = re.search(r"\b\d{1,3}%\s+([A-Za-z][A-Za-z \-]*)\b", text)
    if m:
        return f"{text[m.start():m.end()]}"  # å·²ç»æ˜¯â€œxx% æè´¨â€çš„æ ·å¼

    return "No Data"

# ------ è§£ææ ¸å¿ƒ ------

def extract_product_info_from_html(html: str, url: str):
    soup = BeautifulSoup(html, "html.parser")

    # åç§°
    name = "No Data"
    for script in soup.find_all("script", type="application/ld+json"):
        try:
            data = _safe_json_loads(script.string or "")
            if isinstance(data, dict) and data.get("@type") == "Product" and "name" in data:
                name = data["name"].strip()
                break
        except:
            continue

    # SKU
    sku = "No Data"
    for script in soup.find_all("script", type="application/ld+json"):
        try:
            data = _safe_json_loads(script.string or "")
            if isinstance(data, dict) and data.get("@type") == "Product" and "sku" in data:
                sku = str(data["sku"]).strip()
                break
        except:
            continue

    # æè¿°
    desc_tag = soup.find("div", {"id": "collapsible-description-1"})
    description = desc_tag.get_text(separator=" ", strip=True) if desc_tag else "No Data"
    # æœ‰äº›é¡µé¢ä¼šæŠŠ â€œSKU: XXXâ€ æ··åœ¨æè¿°ä¸­ï¼Œåšä¸ªæ¸…ç†
    if sku and sku != "No Data":
        description = description.replace(f"SKU: {sku}", "").strip()

    # Featureï¼ˆå« ä¿å…»/æè´¨ï¼‰
    features_tag = soup.find("div", class_="care-information")
    features = features_tag.get_text(separator=" | ", strip=True) if features_tag else "No Data"

    # ä»·æ ¼
    price_tag = soup.select_one("span.sales span.value")
    price = price_tag["content"] if price_tag and price_tag.has_attr("content") else "0"

    # é¢œè‰²
    color_tag = soup.select_one("span.selected-color")
    color = color_tag.get_text(strip=True).replace("(", "").replace(")", "") if color_tag else "No Data"

    # å°ºç 
    size_buttons = soup.select("div.size-wrapper button.size-button")
    size_map = {btn.get_text(strip=True): "æœ‰è´§" for btn in size_buttons} if size_buttons else {}

    # æ€§åˆ«ï¼ˆæ–°å¢ï¼‰
    product_gender = _extract_gender_from_html(html)

    # ä¸»ä½“æè´¨ï¼ˆæ–°å¢ï¼‰
    product_material = _extract_material_from_features(features)

    info = {
        "Product Code": sku,
        "Product Name": name,
        "Product Description": description if description else "No Data",
        "Product Gender": product_gender,                # âœ… æ–°å¢
        "Product Color": color,
        "Product Price": price,
        "Adjusted Price": price,
        "Product Material": product_material,            # âœ… æ–°å¢
        "Feature": features,
        "SizeMap": size_map,
        "Source URL": url,
        "Site Name": "Barbour"
    }
    return info

# ------ ä¸»æµç¨‹ ------

def fetch_and_write_txt():
    links_file = BARBOUR["LINKS_FILE"]
    txt_output_dir = BARBOUR["TXT_DIR"]
    os.makedirs(txt_output_dir, exist_ok=True)

    with open(links_file, "r", encoding="utf-8") as f:
        urls = [line.strip() for line in f if line.strip()]

    print(f"ğŸ“„ å…± {len(urls)} ä¸ªå•†å“é¡µé¢å¾…è§£æ...")

    for idx, url in enumerate(urls, 1):
        try:
            resp = requests.get(url, headers=HEADERS, timeout=20)
            resp.raise_for_status()
            info = extract_product_info_from_html(resp.text, url)

            # æ–‡ä»¶åç”¨ SKUï¼›è‹¥ SKU ç¼ºå¤±åˆ™ç”¨å®‰å…¨åŒ–çš„åç§°å…œåº•
            code_for_file = info.get("Product Code") or re.sub(r"[^A-Za-z0-9\-]+", "_", info.get("Product Name", "NoCode"))
            txt_path = Path(txt_output_dir) / f"{code_for_file}.txt"

            write_barbour_product_txt(info, txt_path, brand="barbour")
            print(f"âœ… [{idx}/{len(urls)}] å†™å…¥æˆåŠŸï¼š{txt_path.name}")
        except Exception as e:
            print(f"âŒ [{idx}/{len(urls)}] å¤±è´¥ï¼š{url}ï¼Œé”™è¯¯ï¼š{e}")

if __name__ == "__main__":
    fetch_and_write_txt()
