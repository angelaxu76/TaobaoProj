# barbour_fetch_info.py
# -*- coding: utf-8 -*-

import os
import re
import json
import requests
from bs4 import BeautifulSoup
from pathlib import Path

from config import BARBOUR
from common_taobao.txt_writer import format_txt              # âœ… å¤ç”¨ä½ å·²æœ‰çš„é²¸èŠ½å†™å…¥å™¨
# å¦‚æœä½ çš„ txt_writer åœ¨ common_taobao ç›®å½•ï¼š
# from common_taobao.txt_writer import format_txt

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    )
}

# -------- å·¥å…·å‡½æ•° --------

def _safe_json_loads(text: str):
    try:
        return json.loads(text)
    except Exception:
        return None

def _extract_gender_from_html(html: str) -> str:
    """
    ä»é¡µé¢é‡Œçš„ gtmAnalyticsï¼ˆæˆ–ç±»ä¼¼æ•°æ®å±‚ï¼‰æå–æ€§åˆ«å…³é”®è¯ï¼Œè½åˆ°ï¼šç”·æ¬¾/å¥³æ¬¾/ç«¥æ¬¾/æœªçŸ¥
    """
    m = re.search(r'"item_category"\s*:\s*"([^"]+)"', html, re.IGNORECASE)
    gender_raw = m.group(1).strip().lower() if m else ""

    mapping = {
        "womens": "å¥³æ¬¾", "women": "å¥³æ¬¾", "ladies": "å¥³æ¬¾",
        "mens": "ç”·æ¬¾", "men": "ç”·æ¬¾",
        "kids": "ç«¥æ¬¾", "children": "ç«¥æ¬¾", "child": "ç«¥æ¬¾",
        "unisex": "é€šç”¨",
    }
    return mapping.get(gender_raw, "æœªçŸ¥")

def _extract_material_from_features(features_text: str) -> str:
    """
    ä» features æ–‡æœ¬ä¸­å°½é‡æŠ½å–ä¸»ä½“æè´¨ï¼ˆxx% xxxï¼‰ï¼Œæ²¡æœ‰å°±è¿”å› No Dataã€‚
    """
    if not features_text:
        return "No Data"
    text = features_text.replace("\n", " ").replace("\r", " ")
    # å¸¸è§çš„ â€œ65% Polyester 35% Cottonâ€ / â€œ100% Cottonâ€
    mats = re.findall(r"\b\d{1,3}%\s+[A-Za-z][A-Za-z \-]*", text)
    if mats:
        return " / ".join(mats[:2])
    return "No Data"

# -------- è§£ææ ¸å¿ƒ --------

def extract_product_info_from_html(html: str, url: str) -> dict:
    soup = BeautifulSoup(html, "html.parser")

    # åç§° / SKU ä» ld+json é‡Œæ‹¿ï¼ˆBarbour å®˜ç½‘å¸¸è§ï¼‰
    name = "No Data"
    sku = "No Data"
    for script in soup.find_all("script", type="application/ld+json"):
        data = _safe_json_loads(script.string or "")
        if isinstance(data, dict) and data.get("@type") == "Product":
            if data.get("name"):
                name = str(data["name"]).strip()
            if data.get("sku"):
                sku = str(data["sku"]).strip()
            break

    # æè¿°
    desc_tag = soup.find("div", {"id": "collapsible-description-1"})
    description = desc_tag.get_text(separator=" ", strip=True) if desc_tag else "No Data"
    if sku and sku != "No Data":
        # æœ‰äº›é¡µé¢ä¼šæŠŠ â€œSKU: XXXâ€ æ··åœ¨æè¿°é‡Œï¼Œæ¸…ä¸€ä¸‹
        description = description.replace(f"SKU: {sku}", "").strip() or "No Data"

    # Featuresï¼ˆå«ä¿å…»/æè´¨ç­‰ï¼‰
    features_tag = soup.find("div", class_="care-information")
    features = features_tag.get_text(separator=" | ", strip=True) if features_tag else "No Data"

    # ä»·æ ¼ï¼ˆé¡µé¢é€šå¸¸æœ‰ meta content æˆ–å¯è§ä»·æ ¼ï¼‰
    price_tag = soup.select_one("span.sales span.value")
    price = price_tag["content"] if price_tag and price_tag.has_attr("content") else "0"

    # é¢œè‰²
    color_tag = soup.select_one("span.selected-color")
    color = color_tag.get_text(strip=True).replace("(", "").replace(")", "") if color_tag else "No Data"

    # å°ºç ï¼ˆæŒ‰é’®æ–‡æ¡ˆï¼‰
    size_buttons = soup.select("div.size-wrapper button.size-button")
    size_map = {}
    for btn in size_buttons or []:
        size_text = btn.get_text(strip=True)
        if not size_text:
            continue
        # ç®€åŒ–ï¼šèƒ½ç‚¹å³è®¤ä¸ºâ€œæœ‰è´§â€ï¼ˆå¦‚æœè¦æ›´ä¸¥è°¨ï¼Œå¯æ£€æŸ¥ disabled/classï¼‰
        disabled = ("disabled" in (btn.get("class") or [])) or btn.has_attr("disabled")
        size_map[size_text] = "æ— è´§" if disabled else "æœ‰è´§"

    # æ€§åˆ« / ä¸»ä½“æè´¨
    product_gender = _extract_gender_from_html(html)
    product_material = _extract_material_from_features(features)

    info = {
        # âœ… ç»Ÿä¸€æˆâ€œé²¸èŠ½æ¨¡æ¿â€é”®å
        "Product Code": sku,
        "Product Name": name,
        "Product Description": description,
        "Product Gender": product_gender,
        "Product Color": color,
        "Product Price": price,
        "Adjusted Price": price,          # å®˜ç½‘ä»·=æŠ˜åä»·ï¼ˆè‹¥åç»­æŠ“åˆ°ä¿ƒé”€ï¼Œå†å¡«ä¸åŒï¼‰
        "Product Material": product_material,
        "Feature": features,
        "SizeMap": size_map,              # ç®€å•ä¾›è´§ï¼›å¦‚ä½ å°†æ¥æŠ“åˆ° EAN/åº“å­˜ï¼Œå¯æ”¹å†™ SizeDetail
        "Source URL": url,
        "Site Name": "Barbour"            # å¯é€‰
        # ä¸ä¼  Style Category â†’ ç”± txt_writer å†…éƒ¨ infer_style_category(desc) è‡ªåŠ¨å…œåº•
    }
    return info

# -------- ä¸»æµç¨‹ --------

def fetch_and_write_txt():
    links_file = BARBOUR["LINKS_FILE"]
    txt_output_dir = Path(BARBOUR["TXT_DIR"])
    txt_output_dir.mkdir(parents=True, exist_ok=True)

    with open(links_file, "r", encoding="utf-8") as f:
        urls = [line.strip() for line in f if line.strip()]

    print(f"ğŸ“„ å…± {len(urls)} ä¸ªå•†å“é¡µé¢å¾…è§£æ...")

    for idx, url in enumerate(urls, 1):
        try:
            resp = requests.get(url, headers=HEADERS, timeout=25)
            resp.raise_for_status()
            info = extract_product_info_from_html(resp.text, url)

            # æ–‡ä»¶åä¼˜å…ˆç”¨ SKUï¼›æ²¡æœ‰å°±ç”¨å®‰å…¨åŒ–çš„æ ‡é¢˜å…œåº•
            code_for_file = info.get("Product Code") or re.sub(r"[^A-Za-z0-9\-]+", "_", info.get("Product Name", "NoCode"))
            txt_path = txt_output_dir / f"{code_for_file}.txt"

            # âœ… ç»Ÿä¸€å†™å…¥ï¼šèµ° camper/clarks_jingya åŒä¸€å†™å…¥å™¨
            format_txt(info, txt_path, brand="barbour")
            print(f"âœ… [{idx}/{len(urls)}] å†™å…¥æˆåŠŸï¼š{txt_path.name}")

        except Exception as e:
            print(f"âŒ [{idx}/{len(urls)}] å¤±è´¥ï¼š{url}ï¼Œé”™è¯¯ï¼š{e}")

if __name__ == "__main__":
    fetch_and_write_txt()
