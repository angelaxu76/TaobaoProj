# barbour/supplier/allweathers_fetch_info.py
# -*- coding: utf-8 -*-

import re
import time
import json
import tempfile
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed

import demjson3
from bs4 import BeautifulSoup
from selenium import webdriver

from config import BARBOUR

# ç»Ÿä¸€å†™å…¥ï¼šå¤ç”¨ä½ ç°æœ‰çš„é²¸èŠ½å†™å…¥å™¨ï¼ˆå’Œ camper / clarks_jingya å®Œå…¨ä¸€è‡´ï¼‰
# å¦‚æœä½ çš„å†™å…¥å™¨åœ¨é¡¹ç›®æ ¹ç›®å½•ï¼šfrom txt_writer import format_txt
from common_taobao.txt_writer import format_txt

# è®© selenium-stealth å¯é€‰ï¼ˆæ²¡è£…ä¹Ÿèƒ½è·‘ï¼‰
try:
    from selenium_stealth import stealth
except ImportError:
    def stealth(*args, **kwargs):
        return

# -------- å…¨å±€é…ç½® --------
LINK_FILE = BARBOUR["LINKS_FILES"]["allweathers"]
TXT_DIR = BARBOUR["TXT_DIRS"]["allweathers"]
TXT_DIR.mkdir(parents=True, exist_ok=True)

MAX_WORKERS = 6


def get_driver():
    temp_profile = tempfile.mkdtemp()
    options = webdriver.ChromeOptions()
    options.add_argument("--headless=new")
    options.add_argument(f"--user-data-dir={temp_profile}")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1920,1080")

    driver = webdriver.Chrome(options=options)
    stealth(
        driver,
        languages=["en-US", "en"],
        vendor="Google Inc.",
        platform="Win32",
        webgl_vendor="Intel Inc.",
        renderer="Intel Iris OpenGL Engine",
        fix_hairline=True,
    )
    return driver


# ============ æŠ½å–è¾…åŠ©å‡½æ•°ï¼ˆä¸æˆ·å¤–ç«™å®é™…é¡µé¢é€‚é…ï¼‰ ============

def _clean_text(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "").strip())


def _infer_gender_from_title(title_or_name: str) -> str:
    t = (title_or_name or "").lower()
    if re.search(r"\b(women|woman|women's|ladies)\b", t):
        return "å¥³æ¬¾"
    if re.search(r"\b(men|men's|man)\b", t):
        return "ç”·æ¬¾"
    if re.search(r"\b(kids?|boys?|girls?)\b", t):
        return "ç«¥æ¬¾"
    return "æœªçŸ¥"


def _extract_name_and_color(soup: BeautifulSoup) -> tuple[str, str]:
    # ä¼˜å…ˆ og:titleï¼šå½¢å¦‚ "Barbour Acorn Women's Waxed Jacket | Olive"
    og = soup.find("meta", {"property": "og:title"})
    if og and og.get("content"):
        txt = og["content"].strip()
        if "|" in txt:
            name, color = map(str.strip, txt.split("|", 1))
            return name, color
        return txt, "Unknown"

    # å…¶æ¬¡ document.title
    if soup.title and soup.title.string:
        t = soup.title.string.strip()
        t = t.split("|", 1)[0].strip()
        if "â€“" in t:
            name, color = map(str.strip, t.split("â€“", 1))
            return name, color
        return t, "Unknown"

    return "Unknown", "Unknown"


def _extract_description(soup: BeautifulSoup) -> str:
    # 1) twitter:description
    m = soup.find("meta", attrs={"name": "twitter:description"})
    if m and m.get("content"):
        desc = _clean_text(m["content"])
        # å»æ‰ â€œKey Features â€¦â€ ç­‰å°¾æ³¨
        desc = re.split(r"(Key\s*Features|Materials\s*&\s*Technical)", desc, flags=re.I)[0].strip(" -â€“|,")
        return desc

    # 2) og:description
    m = soup.find("meta", attrs={"property": "og:description"})
    if m and m.get("content"):
        return _clean_text(m["content"])

    # 3) JSON-LD ProductGroup.description
    for s in soup.find_all("script", {"type": "application/ld+json"}):
        try:
            j = demjson3.decode(s.string)
        except Exception:
            continue
        if isinstance(j, dict) and j.get("@type") in ("ProductGroup", "Product"):
            desc = _clean_text(j.get("description") or "")
            if desc:
                desc = re.split(r"(Key\s*Features|Materials\s*&\s*Technical)", desc, flags=re.I)[0].strip(" -â€“|,")
                return desc
    return "No Data"


def _extract_features(soup: BeautifulSoup) -> str:
    # å¯»æ‰¾ â€œKey Features & Benefitsâ€ æ ‡é¢˜åçš„åˆ—è¡¨
    h = soup.find(["h2", "h3"], string=re.compile(r"Key\s*Features", re.I))
    if h:
        ul = h.find_next("ul")
        if ul:
            items = []
            for li in ul.find_all("li"):
                txt = _clean_text(li.get_text(" ", strip=True))
                if txt:
                    items.append(txt)
            if items:
                return " | ".join(items)

    # é€€å› JSON-LD description é‡Œçš„ â€œKey Features â€¦ï¼ˆåˆ° Materials ä¹‹å‰ï¼‰â€
    for s in soup.find_all("script", {"type": "application/ld+json"}):
        try:
            j = demjson3.decode(s.string)
        except Exception:
            continue
        if isinstance(j, dict) and j.get("@type") in ("ProductGroup", "Product"):
            desc = j.get("description") or ""
            if "Key" in desc:
                m = re.search(
                    r"Key\s*Features.*?:\s*(.+?)\s*(Materials\s*&\s*Technical|Frequently|$)",
                    desc, flags=re.I | re.S
                )
                if m:
                    block = m.group(1)
                    parts = [_clean_text(p) for p in re.split(r"[\r\n]+|â€¢|- ", block)]
                    parts = [p for p in parts if p]
                    if parts:
                        return " | ".join(parts)
    return "No Data"


def _extract_material_outer(soup: BeautifulSoup) -> str:
    # é¡µé¢ H2 â€œMaterials & Technical Specificationsâ€ åˆ—è¡¨ä¸­çš„ Outer
    h = soup.find(["h2", "h3"], string=re.compile(r"Materials\s*&\s*Technical", re.I))
    if h:
        ul = h.find_next("ul")
        if ul:
            for li in ul.find_all("li"):
                txt = _clean_text(li.get_text(" ", strip=True))
                m = re.match(r"Outer:\s*(.+)", txt, flags=re.I)
                if m:
                    return m.group(1)

    # é€€å› JSON-LD description
    for s in soup.find_all("script", {"type": "application/ld+json"}):
        try:
            j = demjson3.decode(s.string)
        except Exception:
            continue
        if isinstance(j, dict) and j.get("@type") in ("ProductGroup", "Product"):
            desc = j.get("description") or ""
            m = re.search(r"Outer:\s*(.+)", desc, flags=re.I)
            if m:
                outer_line = _clean_text(m.group(1))
                outer_line = re.split(r"[\r\n;]+", outer_line)[0].strip()
                return outer_line
    return "No Data"


def _extract_header_price(soup: BeautifulSoup) -> float | None:
    # Shopify å¸¸è§çš„ meta ä»·æ ¼
    m = soup.find("meta", {"property": "product:price:amount"})
    if m and m.get("content"):
        try:
            return float(m["content"])
        except Exception:
            pass
    return None


# ============ è§£æè¯¦æƒ…é¡µä¸ºç»Ÿä¸€ info ============

def parse_detail_page(html: str, url: str) -> dict:
    soup = BeautifulSoup(html, "html.parser")

    # åç§° & é¢œè‰²
    name, color = _extract_name_and_color(soup)

    # JSON-LDï¼ˆShopify ProductGroupï¼‰
    script = soup.find("script", {"type": "application/ld+json"})
    if not script:
        raise ValueError("æœªæ‰¾åˆ° JSON-LD æ•°æ®æ®µ")

    data = demjson3.decode(script.string)
    variants = data.get("hasVariant", []) if isinstance(data, dict) else []
    if not variants:
        raise ValueError("âŒ æœªæ‰¾åˆ°å°ºç å˜ä½“")

    # å•†å“ç¼–ç ï¼ˆè‰²ç å«åœ¨ç¼–ç æœ«å°¾ï¼‰ï¼šå¦‚ LWX0752OL51-16 â†’ LWX0752OL51
    first_sku = (variants[0].get("sku") or "")
    base_sku = first_sku.split("-")[0] if first_sku else "Unknown"

    # å°ºç /ä»·æ ¼/åº“å­˜
    size_detail = {}
    for item in variants:
        sku = item.get("sku", "")
        offer = item.get("offers") or {}
        try:
            price = float(offer.get("price", 0.0))
        except Exception:
            price = 0.0
        availability = (offer.get("availability") or "").lower()
        can_order = "instock" in availability
        # UK å°ºç åœ¨ sku å°¾éƒ¨ï¼šLWX0752OL51-16 â†’ UK 16
        size_tail = sku.split("-")[-1] if "-" in sku else "Unknown"
        size = f"UK {re.sub(r'\\s+', ' ', size_tail)}"
        size_detail[size] = {
            "stock_count": 3 if can_order else 0,  # ç»Ÿä¸€ä¸Šæ¶é‡ç­–ç•¥
            "ean": "0000000000000",                # æ²¡æœ‰å°±ç”¨å ä½ï¼Œæ–¹ä¾¿ä¸‹æ¸¸è§£æ
            # ä¹Ÿå¯åŠ  per-size ä»·ï¼šä½†é²¸ç‰™æ¨¡æ¿æŒ‰å•†å“å±‚é¢ä»·å†™å…¥å³å¯
        }

    gender = _infer_gender_from_title(name)
    description = _extract_description(soup)
    features = _extract_features(soup)
    material_outer = _extract_material_outer(soup)
    price_header = _extract_header_price(soup)

    info = {
        # â€”â€” ç»Ÿä¸€â€œé²¸èŠ½â€æ¨¡æ¿é”®å â€”â€”
        "Product Code": base_sku,                 # ä½œä¸ºæ–‡ä»¶åä¸å”¯ä¸€æ ‡è¯†
        "Product Name": name,
        "Product Description": description,
        "Product Gender": gender,
        "Product Color": color,
        "Product Price": price_header,            # åŸä»·
        "Adjusted Price": price_header,           # æ²¡æœ‰ä¿ƒé”€æ—¶ä¸åŸä»·ä¸€è‡´ï¼›æœ‰ä¿ƒé”€å†æ›¿æ¢
        "Product Material": material_outer,       # Outer
        # "Style Category": ç•™ç©ºè®© txt_writer è‡ªåŠ¨ infer
        "Feature": features,
        "SizeDetail": size_detail,                # æ¯ç åº“å­˜/å ä½ EAN
        "Source URL": url,
        "Site Name": "Allweathers",
    }
    return info


# ============ æŠ“å–å¹¶å†™å…¥ TXT ============

def fetch_one_product(url: str, idx: int, total: int):
    print(f"[{idx}/{total}] æŠ“å–: {url}")
    try:
        driver = get_driver()
        driver.get(url)
        time.sleep(2.5)
        html = driver.page_source
        driver.quit()

        info = parse_detail_page(html, url)

        code = info.get("Product Code") or "Unknown"
        txt_path = TXT_DIR / f"{re.sub(r'[^A-Za-z0-9_-]+', '_', code)}.txt"
        # âœ… ç»Ÿä¸€å†™å…¥ï¼ˆä¸ camper / clarks_jingya å®Œå…¨ä¸€è‡´ï¼‰
        format_txt(info, txt_path, brand="barbour")
        return (url, "âœ… æˆåŠŸ")
    except Exception as e:
        return (url, f"âŒ å¤±è´¥: {e}")


def fetch_allweathers_products(max_workers: int = MAX_WORKERS):
    print(f"ğŸš€ å¯åŠ¨ Allweathers å¤šçº¿ç¨‹å•†å“è¯¦æƒ…æŠ“å–ï¼ˆçº¿ç¨‹æ•°: {max_workers}ï¼‰")
    links = LINK_FILE.read_text(encoding="utf-8").splitlines()
    links = [u.strip() for u in links if u.strip()]
    total = len(links)

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [
            executor.submit(fetch_one_product, url, idx + 1, total)
            for idx, url in enumerate(links)
        ]
        for future in as_completed(futures):
            url, status = future.result()
            print(f"{status} - {url}")

    print("\nâœ… æ‰€æœ‰å•†å“æŠ“å–å®Œæˆ")


if __name__ == "__main__":
    fetch_allweathers_products()
