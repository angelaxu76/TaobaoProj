# barbour/supplier/allweathers_fetch_info.py
# -*- coding: utf-8 -*-

import re
import time
import json
import tempfile
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed

import demjson3
from bs4 import BeautifulSoup
from selenium import webdriver

from config import BARBOUR

# âœ… ç»Ÿä¸€å†™å…¥ï¼šä½¿ç”¨ä½ çš„ txt_writerï¼Œä¿è¯ä¸å…¶å®ƒç«™ç‚¹åŒæ¨¡æ¿
from common_taobao.txt_writer import format_txt  # ä¸é¡¹ç›®å½“å‰ç”¨æ³•ä¿æŒä¸€è‡´

# å¯é€‰çš„ selenium_stealthï¼ˆæ— åˆ™è·³è¿‡ï¼‰
try:
    from selenium_stealth import stealth
except ImportError:
    def stealth(*args, **kwargs):
        return

# â€”â€” æ–°å¢ï¼šæ€§åˆ«ä¿®æ­£ï¼ˆBarbour ç¼–ç å‰ç¼€ä¼˜å…ˆï¼‰â€”â€”
try:
    from common_taobao.core.size_normalizer import infer_gender_for_barbour
except Exception:
    infer_gender_for_barbour = None  # è‹¥æœªæä¾›å…±äº«æ¨¡å—ï¼Œä¸‹é¢ä¼šç”¨æœ¬åœ°å…œåº•

# -------- å…¨å±€é…ç½® --------
LINK_FILE = BARBOUR["LINKS_FILES"]["allweathers"]
TXT_DIR = BARBOUR["TXT_DIRS"]["allweathers"]
TXT_DIR.mkdir(parents=True, exist_ok=True)

MAX_WORKERS = 6


def get_driver():
    temp_profile = tempfile.mkdtemp()
    options = webdriver.ChromeOptions()
    options.add_argument("--headless=new")
    options.add_argument(f"--user-data-dir={temp_profile}")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1920,1080")

    driver = webdriver.Chrome(options=options)
    stealth(
        driver,
        languages=["en-US", "en"],
        vendor="Google Inc.",
        platform="Win32",
        webgl_vendor="Intel Inc.",
        renderer="Intel Iris OpenGL Engine",
        fix_hairline=True,
    )
    return driver


# ============ æŠ½å–è¾…åŠ©å‡½æ•°ï¼ˆä¸æˆ·å¤–ç«™å®é™…é¡µé¢é€‚é…ï¼‰ ============

def _clean_text(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "").strip())


def _infer_gender_from_title(title_or_name: str) -> str:
    t = (title_or_name or "").lower()
    if re.search(r"\b(women|woman|women's|ladies)\b", t):
        return "å¥³æ¬¾"
    if re.search(r"\b(men|men's|man)\b", t):
        return "ç”·æ¬¾"
    if re.search(r"\b(kids?|boys?|girls?)\b", t):
        return "ç«¥æ¬¾"
    return "æœªçŸ¥"


def _extract_name_and_color(soup: BeautifulSoup) -> tuple[str, str]:
    # ä¼˜å…ˆ og:titleï¼šå½¢å¦‚ "Barbour Acorn Women's Waxed Jacket | Olive"
    og = soup.find("meta", {"property": "og:title"})
    if og and og.get("content"):
        txt = og["content"].strip()
        if "|" in txt:
            name, color = map(str.strip, txt.split("|", 1))
            return name, color
        return txt, "Unknown"

    # å…¶æ¬¡ document.title
    if soup.title and soup.title.string:
        t = soup.title.string.strip()
        t = t.split("|", 1)[0].strip()
        if "â€“" in t:
            name, color = map(str.strip, t.split("â€“", 1))
            return name, color
        return t, "Unknown"

    return "Unknown", "Unknown"


def _extract_description(soup: BeautifulSoup) -> str:
    # 1) twitter:description
    m = soup.find("meta", attrs={"name": "twitter:description"})
    if m and m.get("content"):
        desc = _clean_text(m["content"])
        # å»æ‰ â€œKey Features â€¦â€ ç­‰å°¾æ³¨
        desc = re.split(r"(Key\s*Features|Materials\s*&\s*Technical)", desc, flags=re.I)[0].strip(" -â€“|,")
        return desc

    # 2) og:description
    m = soup.find("meta", attrs={"property": "og:description"})
    if m and m.get("content"):
        return _clean_text(m["content"])

    # 3) JSON-LD ProductGroup.description
    for s in soup.find_all("script", {"type": "application/ld+json"}):
        try:
            j = demjson3.decode(s.string)
        except Exception:
            continue
        if isinstance(j, dict) and j.get("@type") in ("ProductGroup", "Product"):
            desc = _clean_text(j.get("description") or "")
            if desc:
                desc = re.split(r"(Key\s*Features|Materials\s*&\s*Technical)", desc, flags=re.I)[0].strip(" -â€“|,")
                return desc
    return "No Data"


def _extract_features(soup: BeautifulSoup) -> str:
    # å¯»æ‰¾ â€œKey Features & Benefitsâ€ æ ‡é¢˜åçš„åˆ—è¡¨
    h = soup.find(["h2", "h3"], string=re.compile(r"Key\s*Features", re.I))
    if h:
        ul = h.find_next("ul")
        if ul:
            items = []
            for li in ul.find_all("li"):
                txt = _clean_text(li.get_text(" ", strip=True))
                if txt:
                    items.append(txt)
            if items:
                return " | ".join(items)

    # é€€å› JSON-LD description é‡Œçš„ â€œKey Features â€¦ï¼ˆåˆ° Materials ä¹‹å‰ï¼‰â€
    for s in soup.find_all("script", {"type": "application/ld+json"}):
        try:
            j = demjson3.decode(s.string)
        except Exception:
            continue
        if isinstance(j, dict) and j.get("@type") in ("ProductGroup", "Product"):
            desc = j.get("description") or ""
            if "Key" in desc:
                m = re.search(
                    r"Key\s*Features.*?:\s*(.+?)\s*(Materials\s*&\s*Technical|Frequently|$)",
                    desc, flags=re.I | re.S
                )
                if m:
                    block = m.group(1)
                    parts = [_clean_text(p) for p in re.split(r"[\r\n]+|â€¢|- ", block)]
                    parts = [p for p in parts if p]
                    if parts:
                        return " | ".join(parts)
    return "No Data"


def _extract_material_outer(soup: BeautifulSoup) -> str:
    # é¡µé¢ H2 â€œMaterials & Technical Specificationsâ€ åˆ—è¡¨ä¸­çš„ Outer
    h = soup.find(["h2", "h3"], string=re.compile(r"Materials\s*&\s*Technical", re.I))
    if h:
        ul = h.find_next("ul")
        if ul:
            for li in ul.find_all("li"):
                txt = _clean_text(li.get_text(" ", strip=True))
                m = re.match(r"Outer:\s*(.+)", txt, flags=re.I)
                if m:
                    return m.group(1)

    # é€€å› JSON-LD description
    for s in soup.find_all("script", {"type": "application/ld+json"}):
        try:
            j = demjson3.decode(s.string)
        except Exception:
            continue
        if isinstance(j, dict) and j.get("@type") in ("ProductGroup", "Product"):
            desc = j.get("description") or ""
            m = re.search(r"Outer:\s*(.+)", desc, flags=re.I)
            if m:
                outer_line = _clean_text(m.group(1))
                outer_line = re.split(r"[\r\n;]+", outer_line)[0].strip()
                return outer_line
    return "No Data"


def _extract_header_price(soup: BeautifulSoup) -> float | None:
    # Shopify å¸¸è§çš„ meta ä»·æ ¼
    m = soup.find("meta", {"property": "product:price:amount"})
    if m and m.get("content"):
        try:
            return float(m["content"])
        except Exception:
            pass
    return None


# ============ è§£æè¯¦æƒ…é¡µä¸ºç»Ÿä¸€ infoï¼ˆçˆ¬å–é€»è¾‘ä¿æŒä¸å˜ï¼‰ ============

def parse_detail_page(html: str, url: str) -> dict:
    soup = BeautifulSoup(html, "html.parser")

    # åç§° & é¢œè‰²
    name, color = _extract_name_and_color(soup)

    # JSON-LDï¼ˆShopify ProductGroupï¼‰
    script = soup.find("script", {"type": "application/ld+json"})
    if not script:
        raise ValueError("æœªæ‰¾åˆ° JSON-LD æ•°æ®æ®µ")

    data = demjson3.decode(script.string)
    variants = data.get("hasVariant", []) if isinstance(data, dict) else []
    if not variants:
        raise ValueError("âŒ æœªæ‰¾åˆ°å°ºç å˜ä½“")

    # å•†å“ç¼–ç ï¼ˆè‰²ç å«åœ¨ç¼–ç æœ«å°¾ï¼‰ï¼šå¦‚ LWX0752OL51-16 â†’ LWX0752OL51
    first_sku = (variants[0].get("sku") or "")
    base_sku = first_sku.split("-")[0] if first_sku else "Unknown"

    # å°ºç /ä»·æ ¼/åº“å­˜
    size_detail = {}
    for item in variants:
        sku = item.get("sku", "")
        offer = item.get("offers") or {}
        try:
            price = float(offer.get("price", 0.0))
        except Exception:
            price = 0.0
        availability = (offer.get("availability") or "").lower()
        can_order = "instock" in availability
        # UK å°ºç åœ¨ sku å°¾éƒ¨ï¼šLWX0752OL51-16 â†’ UK 16
        size_tail = sku.split("-")[-1] if "-" in sku else "Unknown"
        size = f"UK {re.sub(r'\\s+', ' ', size_tail)}"
        size_detail[size] = {
            "stock_count": 3 if can_order else 0,  # ç»Ÿä¸€ä¸Šæ¶é‡ç­–ç•¥
            "ean": "0000000000000",                # å ä½ EAN
        }

    gender = _infer_gender_from_title(name)
    description = _extract_description(soup)
    features = _extract_features(soup)
    material_outer = _extract_material_outer(soup)
    price_header = _extract_header_price(soup)

    info = {
        "Product Code": base_sku,
        "Product Name": name,
        "Product Description": description,
        "Product Gender": gender,
        "Product Color": color,
        "Product Price": price_header,
        "Adjusted Price": price_header,
        "Product Material": material_outer,
        # "Style Category": ç•™ç©ºè®©å†™å…¥å™¨è‡ªåŠ¨ inferï¼ˆå¦‚ä½ å·²å‡çº§åˆ†ç±»å™¨ï¼‰
        "Feature": features,
        "SizeDetail": size_detail,       # æ¯ç åº“å­˜/å ä½ EANï¼ˆå†™å…¥å‰å†è½¬ä¸¤è¡Œï¼‰
        "Source URL": url,
        "Site Name": "Allweathers",
    }
    return info


# ============ åå¤„ç†ï¼ˆä¸æ”¹çˆ¬å–ï¼Œåªæ•´ç†å†™å…¥å­—æ®µï¼‰ ============

WOMEN_ORDER = ["4","6","8","10","12","14","16","18","20"]
MEN_ALPHA_ORDER = ["2XS","XS","S","M","L","XL","2XL","3XL"]
MEN_NUM_ORDER = [str(n) for n in range(30, 52, 2)]  # 30..50ï¼ˆæŒ‰ä½ è¦æ±‚ï¼šä¸åŒ…å« 52ï¼‰

ALPHA_MAP = {
    "XXXS": "2XS", "2XS": "2XS",
    "XXS": "XS", "XS": "XS",
    "S": "S", "SMALL": "S",
    "M": "M", "MEDIUM": "M",
    "L": "L", "LARGE": "L",
    "XL": "XL", "X-LARGE": "XL",
    "XXL": "2XL", "2XL": "2XL",
    "XXXL": "3XL", "3XL": "3XL",
}

def _normalize_size(token: str, gender: str) -> str | None:
    """å°† 'UK 36' / '36' / 'XL' å½’ä¸€åˆ°ä½ çš„æ ‡å‡†ï¼Œå¹¶è¿‡æ»¤ç”·æ¬¾ 52ã€‚"""
    s = (token or "").strip().upper()
    s = s.replace("UK ", "").replace("EU ", "").replace("US ", "")
    s = re.sub(r"\s*\(.*?\)\s*", "", s)
    s = re.sub(r"\s+", " ", s)

    # å…ˆæ•°å­—
    m = re.findall(r"\d{1,3}", s)
    if m:
        n = int(m[0])
        if gender == "å¥³æ¬¾" and n in {4,6,8,10,12,14,16,18,20}:
            return str(n)
        if gender == "ç”·æ¬¾":
            # ç”·æ•°å­—ï¼š30..50ï¼ˆå¶æ•°ï¼‰ï¼Œä¸”æ˜¾å¼æ’é™¤ 52
            if 30 <= n <= 50 and n % 2 == 0:
                return str(n)
            # å®¹é”™è´´è¿‘ï¼šå¯¹ 28..54 å–å°±è¿‘å¶æ•°å¹¶è£å‰ªåˆ° 30..50
            if 28 <= n <= 54:
                cand = n if n % 2 == 0 else n-1
                cand = max(30, min(50, cand))
                return str(cand)
        # å…¶å®ƒåœºæ™¯ï¼šä¸è¿”å›
        return None

    # å†å­—æ¯
    key = s.replace("-", "").replace(" ", "")
    return ALPHA_MAP.get(key)

def _sort_sizes(keys: list[str], gender: str) -> list[str]:
    if gender == "å¥³æ¬¾":
        return [k for k in WOMEN_ORDER if k in keys]
    # ç”·æ¬¾ï¼šå­—æ¯ä¼˜å…ˆï¼Œå†æ•°å­—
    ordered = [k for k in MEN_ALPHA_ORDER if k in keys] + [k for k in MEN_NUM_ORDER if k in keys]
    return ordered

def _build_size_lines_from_sizedetail(size_detail: dict, gender: str) -> tuple[str, str]:
    """
    è¾“å…¥ SizeDetail(dict) â†’ è¾“å‡ºï¼š
      Product Sizeï¼ˆå½¢å¦‚ 34:æœ‰è´§;...ï¼‰
      Product Size Detailï¼ˆå½¢å¦‚ 34:1:000...;...ï¼‰
    ä¸å†™ SizeMapï¼›åŒæ—¶è¿‡æ»¤ç”·æ¬¾ 52ã€‚
    """
    bucket_status: dict[str, str] = {}
    bucket_stock: dict[str, int] = {}

    for raw_size, meta in (size_detail or {}).items():
        norm = _normalize_size(raw_size, gender)
        if not norm:
            continue
        stock = int(meta.get("stock_count", 0) or 0)
        status = "æœ‰è´§" if stock > 0 else "æ— è´§"
        # åŒå°ºç å¤šæ¬¡å‡ºç°ï¼šæœ‰è´§ä¼˜å…ˆ
        prev = bucket_status.get(norm)
        if prev is None or (prev == "æ— è´§" and status == "æœ‰è´§"):
            bucket_status[norm] = status
            bucket_stock[norm] = 1 if stock > 0 else 0

    ordered = _sort_sizes(list(bucket_status.keys()), gender)

    ps = ";".join(f"{k}:{bucket_status[k]}" for k in ordered)
    psd = ";".join(f"{k}:{bucket_stock[k]}:0000000000000" for k in ordered)
    return ps, psd


# ============ æŠ“å–å¹¶å†™å…¥ TXTï¼ˆpipeline ç­¾åä¿æŒä¸å˜ï¼‰ ============

def fetch_one_product(url: str, idx: int, total: int):
    print(f"[{idx}/{total}] æŠ“å–: {url}")
    try:
        driver = get_driver()
        driver.get(url)
        time.sleep(2.5)
        html = driver.page_source
        driver.quit()

        # â€”â€” ä¸æ”¹çˆ¬å–é€»è¾‘ï¼šä¿ç•™ä½ åŸæœ‰çš„è§£æ â€”â€”
        info = parse_detail_page(html, url)

        # â€”â€” å†™å…¥å‰çš„è§„èŒƒåŒ–ï¼šåªåšå­—æ®µæ¸…æ´—ï¼Œä¸è§¦ç¢°æŠ“å–æµç¨‹ â€”â€”
        # å“ç‰Œä¸ç«™ç‚¹ä¿¡æ¯
        info.setdefault("Brand", "Barbour")
        info.setdefault("Site Name", "Allweathers")
        info.setdefault("Source URL", url)

        # æ€§åˆ«ä¿®æ­£ï¼ˆä¼˜å…ˆ Barbour ç¼–ç å‰ç¼€ï¼›å†çœ‹æ ‡é¢˜/æè¿°ï¼›å¦åˆ™ç”¨åŸå€¼ï¼‰
        if infer_gender_for_barbour:
            info["Product Gender"] = infer_gender_for_barbour(
                product_code=info.get("Product Code"),
                title=info.get("Product Name"),
                description=info.get("Product Description"),
                given_gender=info.get("Product Gender"),
            ) or info.get("Product Gender") or "ç”·æ¬¾"

        # ç”± SizeDetail ç”Ÿæˆä¸¤è¡Œï¼ˆä¸è¾“å‡º SizeMapï¼›å¹¶è¿‡æ»¤ç”·æ¬¾ 52ï¼‰
        if info.get("SizeDetail") and (not info.get("Product Size") or not info.get("Product Size Detail")):
            ps, psd = _build_size_lines_from_sizedetail(info["SizeDetail"], info.get("Product Gender", "ç”·æ¬¾"))
            info["Product Size"] = info.get("Product Size") or ps
            info["Product Size Detail"] = info.get("Product Size Detail") or psd

        # æ–‡ä»¶åï¼šç”¨ Product Code
        code = info.get("Product Code") or "Unknown"
        safe_code = re.sub(r"[^A-Za-z0-9_-]+", "_", code)
        txt_path = TXT_DIR / f"{safe_code}.txt"

        # âœ… ç»Ÿä¸€å†™å…¥ï¼ˆåŒå…¶å®ƒç«™ç‚¹ï¼‰
        format_txt(info, txt_path, brand="Barbour")
        return (url, "âœ… æˆåŠŸ")
    except Exception as e:
        return (url, f"âŒ å¤±è´¥: {e}")


def fetch_allweathers_products(max_workers: int = MAX_WORKERS):
    print(f"ğŸš€ å¯åŠ¨ Allweathers å¤šçº¿ç¨‹å•†å“è¯¦æƒ…æŠ“å–ï¼ˆçº¿ç¨‹æ•°: {max_workers}ï¼‰")
    links = LINK_FILE.read_text(encoding="utf-8").splitlines()
    links = [u.strip() for u in links if u.strip()]
    total = len(links)

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [
            executor.submit(fetch_one_product, url, idx + 1, total)
            for idx, url in enumerate(links)
        ]
        for future in as_completed(futures):
            url, status = future.result()
            print(f"{status} - {url}")

    print("\nâœ… æ‰€æœ‰å•†å“æŠ“å–å®Œæˆ")


if __name__ == "__main__":
    fetch_allweathers_products()
