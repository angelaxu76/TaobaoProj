# barbour/supplier/allweathers_fetch_info.py

import demjson3
import time
import re
from pathlib import Path
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import undetected_chromedriver as uc
from bs4 import BeautifulSoup
from config import BARBOUR
from barbour.write_offer_txt import write_offer_txt

LINK_FILE = BARBOUR["LINKS_FILES"]["allweathers"]
TXT_DIR = BARBOUR["TXT_DIRS"]["allweathers"]
TXT_DIR.mkdir(parents=True, exist_ok=True)

MAX_WORKERS = 6  # âœ… çº¿ç¨‹æ•°å»ºè®® 4~8ï¼Œæ ¹æ®æ€§èƒ½è°ƒæ•´

def get_driver():
    options = uc.ChromeOptions()
    # options.add_argument("--headless=new")  # å¯åˆ‡æ¢ä¸ºé™é»˜è¿è¡Œ
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-blink-features=AutomationControlled")
    return uc.Chrome(options=options, use_subprocess=True)

def parse_detail_page(html, url):
    soup = BeautifulSoup(html, "html.parser")
    title = soup.title.text.strip()
    clean_title = title.split("|")[0].strip()
    if "â€“" in clean_title:
        name, color = map(str.strip, clean_title.split("â€“"))
    else:
        name, color = clean_title, "Unknown"

    script = soup.find("script", {"type": "application/ld+json"})
    if not script:
        raise ValueError("æœªæ‰¾åˆ° JSON æ•°æ®æ®µ")

    data = demjson3.decode(script.string)
    variants = data.get("hasVariant", [])
    if not variants:
        raise ValueError("âŒ æœªæ‰¾åˆ°å°ºç å˜ä½“")

    offer_list = []
    base_sku = variants[0]["sku"].split("-")[0]

    for item in variants:
        sku = item.get("sku", "")
        price = float(item["offers"].get("price", 0.0))
        availability = item["offers"].get("availability", "")
        stock_status = "æœ‰è´§" if "InStock" in availability else "æ— è´§"
        can_order = stock_status == "æœ‰è´§"
        size = f"UK {sku.split('-')[-1]}" if "-" in sku else "Unknown"
        offer_list.append((size, price, stock_status, can_order))

    return {
        "Product Name": name,
        "Product Color": color,
        "Product Color Code": base_sku,
        "Site Name": "Allweathers",
        "Product URL": url,
        "Offers": offer_list
    }

# âœ… æ¯ä¸ªçº¿ç¨‹æ‰§è¡Œçš„ä»»åŠ¡
def fetch_one_product(url, idx, total):
    print(f"[{idx}/{total}] æŠ“å–: {url}")
    try:
        driver = get_driver()
        driver.get(url)
        time.sleep(1.0)
        html = driver.page_source
        driver.quit()

        data = parse_detail_page(html, url)
        code = data["Product Color Code"]
        txt_path = TXT_DIR / f"{code}.txt"
        write_offer_txt(data, txt_path)
        return (url, "âœ… æˆåŠŸ")
    except Exception as e:
        return (url, f"âŒ å¤±è´¥: {e}")

def fetch_allweathers_products(max_workers=6):  # âœ… è®¾ç½®é»˜è®¤çº¿ç¨‹æ•°
    print(f"ğŸš€ å¯åŠ¨ Allweathers å¤šçº¿ç¨‹å•†å“è¯¦æƒ…æŠ“å–ï¼ˆçº¿ç¨‹æ•°: {max_workers}ï¼‰")
    links = LINK_FILE.read_text(encoding="utf-8").splitlines()
    total = len(links)

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [
            executor.submit(fetch_one_product, url, idx + 1, total)
            for idx, url in enumerate(links)
        ]

        for future in as_completed(futures):
            url, status = future.result()
            print(f"âœ… {status} - {url}")

    print("\nâœ… æ‰€æœ‰å•†å“æŠ“å–å®Œæˆ")

# âœ… æœ€å‰é¢é¢„çƒ­ï¼Œç¡®ä¿é©±åŠ¨å·²è§£å‹ï¼Œä¸å†é‡å¤å†™æ–‡ä»¶
def warm_up_chromedriver():
    try:
        driver = get_driver()
        driver.quit()
    except Exception:
        pass


if __name__ == "__main__":
    warm_up_chromedriver()  # âœ… æå‰åˆå§‹åŒ–
    fetch_allweathers_products(5)
