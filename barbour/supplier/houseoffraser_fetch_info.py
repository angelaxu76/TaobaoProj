# -*- coding: utf-8 -*-
"""
House of Fraser | Barbour å•†å“æŠ“å–ï¼ˆç»Ÿä¸€ TXT æ¨¡æ¿ç‰ˆï¼‰
- ä¸å…¶å®ƒç«™ç‚¹ä¿æŒä¸€è‡´ï¼šé›¶å‚æ•°ã€ä» config è¯»å–è·¯å¾„ä¸é“¾æ¥
- çˆ¬å–æŠ€æœ¯ï¼šSelenium + BeautifulSoupï¼ˆä¸ç°æœ‰ allweathers/outdoorandcountry ä¸€è‡´ï¼‰
- å†™å…¥ï¼šcommon_taobao.txt_writer.format_txtï¼ˆä¸ç°æœ‰ç«™ç‚¹ä¸€è‡´ï¼‰

è¾“å‡ºå­—æ®µè¡¥é½ï¼š
- Product Descriptionï¼šmeta[property="og:description"]
- Featureï¼š#DisplayAttributes li â†’ "key: value; ..."
- Product Materialï¼šFeature ä¸­ Fabric/Material/Shell çš„å€¼ï¼ˆæ— åˆ™ No Dataï¼‰
- Product Sizeï¼šä» #sizeDdl > option è§£æï¼ˆgreyOut=æ— è´§ï¼‰â†’ "6:æ— è´§;8:æœ‰è´§;..."
- Product Size Detailï¼šä¸ä¸Šå¯¹åº”ï¼Œ"size:1/0:0000000000000"ï¼ˆæœ‰è´§=1ï¼Œæ— è´§=0ï¼‰
- Product Priceï¼šä¼˜å…ˆ DOM æå–çš„ç°ä»·ï¼›è‹¥æœªå–åˆ°ä¿ç•™ "No Data"
- Adjusted Priceï¼šç•™ç©ºï¼Œç”±ä¸‹æ¸¸ price_utils è®¡ç®—
- Style Categoryï¼šåŸºäºæ ‡é¢˜å…³é”®å­—çš„ç®€å•æ¨æ–­ï¼ˆjacket / quilted jacket / wax jacketï¼‰
- Product Codeï¼šHOF é¡µé¢æ— ï¼Œå›ºå®š "No Data"
"""

import re
import time
from pathlib import Path
from typing import Optional, List, Tuple

import undetected_chromedriver as uc
from bs4 import BeautifulSoup

from config import BARBOUR
from common_taobao.txt_writer import format_txt  # ç»Ÿä¸€å†™å…¥æ¨¡æ¿ï¼ˆä¸ä½ ç°æœ‰ç«™ç‚¹ä¸€è‡´ï¼‰
from barbour.core.site_utils import assert_site_or_raise as canon

# ========== ç«™ç‚¹çº§å¸¸é‡ ==========
SITE_NAME = canon("houseoffraser")
EAN_PLACEHOLDER = "0000000000000"

LINKS_FILE = BARBOUR["LINKS_FILES"]["houseoffraser"]
TXT_DIR: Path = BARBOUR["TXT_DIRS"]["houseoffraser"]
TXT_DIR.mkdir(parents=True, exist_ok=True)  # ç¡®ä¿ç›®å½•å­˜åœ¨


# ========== Selenium é©±åŠ¨ï¼ˆä¸ç°æœ‰é£æ ¼ä¸€è‡´ï¼‰ ==========
def get_driver():
    options = uc.ChromeOptions()
    # å¦‚éœ€æ— å¤´ï¼šoptions.add_argument("--headless=new")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("--start-maximized")
    driver = uc.Chrome(options=options)
    return driver


# ========== å·¥å…·å‡½æ•° ==========
def _extract_gender(title: str, soup: BeautifulSoup) -> str:
    """
    ä»æ ‡é¢˜/meta/breadcrumb æ¨æ–­æ€§åˆ«
    """
    t = (title or "").lower()
    if "women" in t:
        return "å¥³æ¬¾"
    if "men" in t:
        return "ç”·æ¬¾"
    if "kids" in t or "girls" in t or "boys" in t:
        return "ç«¥æ¬¾"
    # å…œåº•ï¼šä» meta og:title åˆ¤æ–­
    m = soup.find("meta", attrs={"property": "og:title"})
    if m and "women" in m.get("content", "").lower():
        return "å¥³æ¬¾"
    return "No Data"


def _extract_color(soup: BeautifulSoup) -> str:
    """
    ä»é¢œè‰²é€‰æ‹©å™¨ä¸­æå–å½“å‰é¢œè‰²
    """
    ul = soup.find("ul", id="ulColourImages")
    if ul:
        li = ul.find("li", attrs={"aria-checked": "true"})
        if li:
            # data-text å±æ€§ä¼˜å…ˆ
            txt = li.get("data-text") or ""
            if txt.strip():
                return _clean_text(txt)
            # å†å°è¯• <img alt>
            img = li.find("img")
            if img and img.get("alt"):
                return _clean_text(img["alt"])
    return "No Data"


def _clean_text(s: Optional[str]) -> str:
    return re.sub(r"\s+", " ", (s or "").strip())

def _price_from_text_block(text: str) -> Tuple[Optional[float], Optional[float]]:
    """
    ä»ä¸€æ®µå¹¶æ’ä»·æ ¼å­—ç¬¦ä¸²é‡Œæå– (current, original)
    ä¾‹å¦‚ "Â£95.00 Â£189.00" â†’ (95.00, 189.00)
    ç»éªŒç­–ç•¥ï¼šç¬¬ä¸€ä¸ªè§†ä¸ºç°ä»·ï¼›æœ€å¤§å€¼è§†ä¸ºåŸä»·ï¼ˆè‹¥æ¯”ç°ä»·å¤§ï¼‰
    """
    vals = re.findall(r"Â£?\s*([0-9]+(?:\.[0-9]{1,2})?)", text or "")
    nums = [float(v) for v in vals]
    if not nums:
        return (None, None)
    curr = nums[0]
    orig = None
    if len(nums) >= 2:
        mx = max(nums)
        if mx > curr:
            orig = mx
        else:
            orig = nums[-1]
    return (curr, orig)

def _extract_title(soup: BeautifulSoup) -> str:
    # å– <title>ï¼Œå»æ‰ç«™ç‚¹åç¼€
    t = _clean_text(soup.title.get_text()) if soup.title else "No Data"
    t = re.sub(r"\s*\|\s*House of Fraser\s*$", "", t, flags=re.I)
    return t or "No Data"

def _extract_og_description(soup: BeautifulSoup) -> str:
    m = soup.find("meta", attrs={"property": "og:description"})
    return _clean_text(m["content"]) if (m and m.get("content")) else "No Data"

def _extract_features_and_material(soup: BeautifulSoup) -> Tuple[str, str]:
    """
    ä» #DisplayAttributes li æ‹¿ç‰¹å¾åˆ—è¡¨ï¼Œå¹¶ä»ä¸­æŠ½å– Fabric/Material/Shell ç­‰ä½œä¸º Product Material
    """
    ul = soup.find("ul", id="DisplayAttributes")
    features = []
    material = ""
    if ul:
        for li in ul.find_all("li"):
            k = li.find("span", class_="feature-name")
            v = li.find("span", class_="feature-value")
            key = _clean_text(k.get_text() if k else "")
            val = _clean_text(v.get_text() if v else "")
            if key and val:
                features.append(f"{key}: {val}")
                if not material and key.lower() in {"fabric", "material", "shell", "outer"}:
                    material = val
    feat_str = "; ".join(features) if features else "No Data"
    return feat_str, (material or "No Data")

def _extract_prices(soup: BeautifulSoup) -> Tuple[Optional[float], Optional[float]]:
    """
    ä»å¤šä¸ªå¸¸è§èŠ‚ç‚¹é‡Œå‡‘ä¸€æ®µä»·æ ¼æ–‡æœ¬ï¼Œç„¶åè°ƒç”¨ _price_from_text_block
    """
    blocks = []
    # å¸¸è§ç±»å/IDï¼ˆHOF ä¸»é¢˜å¯èƒ½å˜åŒ–ï¼Œå°½é‡å¤šè·¯æ”¶é›†ï¼‰
    for sel in [
        {"id": "lblSellingPrice"},
        {"class_": re.compile(r"(product-price|price-now|now-price|current|selling)", re.I)},
        {"class_": re.compile(r"(prices?|productPrices?)", re.I)},
    ]:
        node = soup.find(attrs=sel)
        if node:
            blocks.append(node.get_text(" ", strip=True))
    # WAS/Was èŠ‚ç‚¹
    was_nodes = soup.find_all(string=re.compile(r"\bwas\b", re.I))
    for n in was_nodes:
        blocks.append(str(n))
        if getattr(n, "parent", None):
            blocks.append(n.parent.get_text(" ", strip=True))
    merged = " | ".join({b for b in blocks if b})
    return _price_from_text_block(merged)

def _extract_size_offers(soup: BeautifulSoup) -> List[Tuple[str, str]]:
    """
    è§£æ #sizeDdl > option
    è¿”å› [(norm_size, stock_status)]ï¼›greyOut æˆ– title å« 'out of stock' â†’ æ— è´§ï¼Œå¦åˆ™ æœ‰è´§
    """
    sel = soup.find("select", id="sizeDdl")
    results: List[Tuple[str, str]] = []
    if not sel:
        return results
    for opt in sel.find_all("option"):
        txt = _clean_text(opt.get_text())
        if not txt or txt.lower().startswith("select"):
            continue
        cls = opt.get("class") or []
        title = _clean_text(opt.get("title") or "")
        oos = ("greyOut" in cls) or ("out of stock" in title.lower())
        status = "æ— è´§" if oos else "æœ‰è´§"
        # å½’ä¸€ï¼šæŠŠ "8 (XS)" â†’ "8"
        norm = re.sub(r"\s*\(.*?\)\s*", "", txt).strip()
        norm = re.sub(r"^(UK|EU|US)\s+", "", norm, flags=re.I)
        results.append((norm, status))
    return results

def _build_size_lines(pairs: List[Tuple[str, str]]) -> Tuple[str, str]:
    """
    - Product Size: "8:æœ‰è´§;10:æœ‰è´§;..."
    - Product Size Detail: "8:1:000...;10:1:000...;..."ï¼ˆæœ‰è´§=1ï¼Œæ— è´§=0ï¼‰
    å¯¹åŒå°ºç å¤šæ¬¡å‡ºç°ï¼šæœ‰è´§ä¼˜å…ˆè¦†ç›–
    """
    bucket = {}
    for size, status in pairs or []:
        prev = bucket.get(size)
        if prev is None or (prev == "æ— è´§" and status == "æœ‰è´§"):
            bucket[size] = status
    # æ’åºï¼šå¥³æ¬¾ä¼˜å…ˆç”¨ 6,8,10,12... çš„è‡ªç„¶æ¬¡åºï¼›å¦åˆ™æŒ‰æ•°å­—ä¼˜å…ˆã€å†å­—æ¯
    def _key(k: str):
        m = re.fullmatch(r"\d{1,3}", k)
        return (0, int(k)) if m else (1, k)
    ordered = sorted(bucket.keys(), key=_key)
    ps = ";".join(f"{k}:{bucket[k]}" for k in ordered)
    psd = ";".join(f"{k}:{3 if bucket[k]=='æœ‰è´§' else 0}:{EAN_PLACEHOLDER}" for k in ordered)
    return ps or "No Data", psd or "No Data"

def _infer_style_category(name: str) -> str:
    n = (name or "").lower()
    if "jacket" in n and "quilt" in n:
        return "quilted jacket"
    if "jacket" in n and "wax" in n:
        return "wax jacket"
    if "jacket" in n:
        return "jacket"
    return "casual wear"


# ========== è§£æä¸å†™ç›˜ ==========
def parse_and_build_info(html: str, url: str) -> dict:
    soup = BeautifulSoup(html, "html.parser")

    title = _extract_title(soup)
    description = _extract_og_description(soup)
    features, material = _extract_features_and_material(soup)
    curr_price, orig_price = _extract_prices(soup)
    pairs = _extract_size_offers(soup)
    product_size, product_size_detail = _build_size_lines(pairs)

    info = {
        "Product Code": "No Data",               # HOF æ— ç¼–ç 
        "Product Name": title,
        "Product Description": description or "No Data",
        "Product Gender": _extract_gender(title, soup),
        "Product Color": _extract_color(soup),
        "Product Price": f"{curr_price:.2f}" if curr_price is not None else "No Data",
        "Adjusted Price": "",                    # ç”±ä¸‹æ¸¸è®¡ç®—
        "Product Material": material or "No Data",
        "Style Category": _infer_style_category(title),
        "Feature": features or "No Data",
        "Product Size": product_size,
        "Product Size Detail": product_size_detail,
        "Source URL": url,
        "Site Name": SITE_NAME,
    }

    # è‹¥åŸä»·å­˜åœ¨ï¼ŒæŠŠå®ƒé™„åŠ åˆ° Feature æœ«å°¾ï¼Œä¸å¢å­—æ®µåï¼ˆä¸ä½ å…¶å®ƒç«™ç‚¹çš„å†™æ³•ä¸€è‡´ï¼‰
    if orig_price is not None:
        extra = f"Original Price: {orig_price:.2f}"
        info["Feature"] = (info["Feature"] + "; " + extra) if info["Feature"] != "No Data" else extra

    return info


def process_url(url: str):
    print(f"\nğŸŒ æ­£åœ¨æŠ“å–: {url}")
    driver = get_driver()
    try:
        driver.get(url)
        time.sleep(2.5)  # è½»ç­‰å¾…ï¼Œè§†é¡µé¢å¤æ‚åº¦å¯é€‚å½“å¢åŠ 
        html = driver.page_source
    finally:
        try:
            driver.quit()
        except Exception:
            pass

    info = parse_and_build_info(html, url)

    # æ–‡ä»¶åï¼šHOF æ— ç¼–ç  â†’ ç”¨æ ‡é¢˜å®‰å…¨åŒ–
    safe_name = re.sub(r"[\\/:*?\"<>|'\s]+", "_", info.get("Product Name") or "NoName")
    out_path = TXT_DIR / f"{safe_name}.txt"
    format_txt(info, out_path, brand="Barbour")  # ä¸ barbour_fetch_info ç­‰ä¿æŒä¸€è‡´
    print(f"âœ… å†™å…¥: {out_path.name}")
    return out_path


# ========== ä¸»å…¥å£ï¼šé›¶å‚æ•°ï¼Œè¯»å– config é“¾æ¥æ–‡ä»¶ ==========
def houseoffraser_fetch_info():
    links_file = Path(LINKS_FILE)
    if not links_file.exists():
        print(f"âš  æ‰¾ä¸åˆ°é“¾æ¥æ–‡ä»¶ï¼š{links_file}")
        return

    urls = [line.strip() for line in links_file.read_text(encoding="utf-8", errors="ignore").splitlines()
            if line.strip() and not line.strip().startswith("#")]
    print(f"ğŸ“„ å…± {len(urls)} ä¸ªå•†å“é¡µé¢å¾…è§£æ...")

    for idx, url in enumerate(urls, 1):
        try:
            print(f"[{idx}/{len(urls)}] å¤„ç†ä¸­...")
            process_url(url)
        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥: {url}\n    {e}")


if __name__ == "__main__":
    houseoffraser_fetch_info()
