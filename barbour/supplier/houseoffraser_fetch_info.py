# -*- coding: utf-8 -*-
"""
House of Fraser | Barbour å•†å“æŠ“å–ï¼ˆç²¾ç®€ç‰ˆï¼‰
- è§£æå•†å“ï¼šæ ‡é¢˜ã€é¢œè‰²ã€ä»·æ ¼ã€å°ºç 
- ç”¨ç¬¬ä¸‰æ–¹ç›¸ä¼¼åº¦ï¼ˆRapidFuzzï¼‰åœ¨æ•°æ®åº“ barbour_products ä¸Šåšâ€œæ ‡é¢˜+é¢œè‰²â€åŒ¹é…ï¼Œæ‹¿åˆ° product_code
- TXT æ–‡ä»¶å = product_code.txtï¼›å¦‚æœæœªå‘½ä¸­ï¼Œåˆ™ç”¨å®‰å…¨æ ‡é¢˜å‘½å
"""

from __future__ import annotations

import re
import os
import json
import time
import tempfile
import threading
from pathlib import Path
from typing import Optional, Tuple, List, Dict, Any
from concurrent.futures import ThreadPoolExecutor, as_completed

# ===== ç¬¬ä¸‰æ–¹ä¸è§£æ =====
import undetected_chromedriver as uc
from bs4 import BeautifulSoup
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# ===== DB ä¸é¡¹ç›®é…ç½® =====
from sqlalchemy import create_engine
from sqlalchemy.engine import Connection
from config import BARBOUR, BRAND_CONFIG
from barbour.core.site_utils import assert_site_or_raise as canon
from barbour.core.sim_matcher import match_product, choose_best, explain_results

# ================== ç«™ç‚¹ä¸ç›®å½• ==================
SITE_NAME = canon("houseoffraser")
LINKS_FILE: str = BARBOUR["LINKS_FILES"]["houseoffraser"]
TXT_DIR: Path = BARBOUR["TXT_DIRS"]["houseoffraser"]
TXT_DIR.mkdir(parents=True, exist_ok=True)

PRODUCTS_TABLE: str = BRAND_CONFIG.get("barbour", {}).get("PRODUCTS_TABLE", "barbour_products")
PG = BRAND_CONFIG["barbour"]["PGSQL_CONFIG"]

# ================== å¯è°ƒå‚æ•° ==================
WAIT_PRICE_SECONDS = 8           # ç­‰ä»·é¢ä»·æ¨¡å—çš„æœ€é•¿ç­‰å¾…ï¼ˆç§’ï¼‰
DEFAULT_DELAY = 2.0              # æ‰“å¼€é¡µé¢åçš„ç¼“å†²ç­‰å¾…ï¼ˆç§’ï¼‰
MAX_WORKERS_DEFAULT = 4          # å¹¶å‘æ•°
MIN_SCORE = 0.72                 # ç›¸ä¼¼åº¦é˜ˆå€¼
MIN_LEAD = 0.04                  # é¢†å…ˆå¹…åº¦é˜ˆå€¼ï¼ˆTop1 ä¸ Top2 å·®å€¼ï¼‰
NAME_WEIGHT = 0.75               # åç§°æƒé‡
COLOR_WEIGHT = 0.25              # é¢œè‰²æƒé‡

# ================== å¹¶å‘å»é‡ + åŸå­å†™ ==================
_WRITTEN: set[str] = set()
_WRITTEN_LOCK = threading.Lock()

def _atomic_write_bytes(data: bytes, dst: Path, retries: int = 6, backoff: float = 0.25) -> bool:
    """
    æ›´å¼ºå¥çš„åŸå­å†™ï¼šä¸ºå¹¶å‘ä¸ Windows å¥æŸ„å ç”¨åšå®¹é”™ã€‚
    - å”¯ä¸€ tmp æ–‡ä»¶åï¼Œé¿å…è·¨çº¿ç¨‹å†²çª
    - PermissionError/FileExistsError é€€é¿é‡è¯•
    - è‹¥é‡è¯•åç›®æ ‡å·²å­˜åœ¨ï¼ˆå¤šçº¿ç¨‹å·²å†™å…¥ï¼‰ï¼ŒæŒ‰æˆåŠŸå¤„ç†
    """
    dst.parent.mkdir(parents=True, exist_ok=True)
    for i in range(retries):
        tmp = None
        try:
            # å”¯ä¸€ tmpï¼šæ”¾åœ¨åŒç›®å½•ï¼Œé™ä½è·¨ç›˜ replace é£é™©
            with tempfile.NamedTemporaryFile(
                delete=False, dir=str(dst.parent), prefix=".tmp_", suffix=f".{os.getpid()}.{threading.get_ident()}"
            ) as tf:
                tmp = Path(tf.name)
                tf.write(data)
                tf.flush()
                os.fsync(tf.fileno())
            try:
                # è‹¥ç›®æ ‡å·²å­˜åœ¨ä¸”è¢«å ç”¨ï¼Œå¯èƒ½æŠ› PermissionErrorï¼›é€€é¿é‡è¯•
                os.replace(tmp, dst)  # åŸå­æ›¿æ¢
            finally:
                if tmp and tmp.exists():
                    try:
                        tmp.unlink(missing_ok=True)
                    except Exception:
                        pass
            return True
        except (PermissionError, FileExistsError, OSError) as e:
            # è‹¥ç›®æ ‡å·²ç»å­˜åœ¨ï¼ˆå¯èƒ½å…¶ä»–çº¿ç¨‹å…ˆå®Œæˆï¼‰ï¼Œå½“ä½œæˆåŠŸ
            if dst.exists():
                return True
            # é€€é¿åé‡è¯•
            time.sleep(backoff * (i + 1))
            # æœ€åä¸€è½®å‰ï¼Œå°è¯•æ¸…ç†æ®‹ç•™ tmp
            try:
                if tmp and tmp.exists():
                    tmp.unlink(missing_ok=True)
            except Exception:
                pass
        except Exception:
            # å…¶ä»–å¼‚å¸¸ä¹Ÿåšé€€é¿
            time.sleep(backoff * (i + 1))
    # åˆ°æ­¤ä»å¤±è´¥ï¼Œä½†è‹¥ç›®æ ‡å·²å­˜åœ¨ï¼ˆå¹¶å‘å·²æˆåŠŸï¼‰ï¼Œä¹Ÿç®—æˆåŠŸ
    return dst.exists()


def _kv_txt_bytes(info: Dict[str, Any]) -> bytes:
    fields = [
        "Product Code","Product Name","Product Description","Product Gender",
        "Product Color","Product Price","Adjusted Price","Product Material",
        "Style Category","Feature","Product Size","Product Size Detail",
        "Source URL","Site Name"
    ]
    lines = [f"{k}: {info.get(k, 'No Data')}" for k in fields]
    return ("\n".join(lines) + "\n").encode("utf-8", errors="ignore")

def _safe_name(s: str) -> str:
    return re.sub(r"[\\/:*?\"<>|'\s]+", "_", s or "NoName")

def get_dbapi_connection(conn_or_engine):
    """æŠŠ SQLAlchemy Engine/Connection å‰¥æˆ DBAPIï¼ˆæœ‰ .cursor()ï¼‰ä¾› sim_matcher ä½¿ç”¨"""
    if hasattr(conn_or_engine, "cursor"):
        return conn_or_engine
    if hasattr(conn_or_engine, "raw_connection"):
        return conn_or_engine.raw_connection()
    c = getattr(conn_or_engine, "connection", None)
    if c is not None:
        dbapi = getattr(c, "dbapi_connection", None)
        if dbapi is not None and hasattr(dbapi, "cursor"):
            return dbapi
        inner = getattr(c, "connection", None)
        if inner is not None and hasattr(inner, "cursor"):
            return inner
        if hasattr(c, "cursor"):
            return c
    return conn_or_engine

# ================== è§£æå‡½æ•°ï¼ˆå¤Ÿç”¨ä¸”ç¨³ï¼‰ ==================
def _clean(s: Optional[str]) -> str:
    return re.sub(r"\s+", " ", (s or "").strip())

def _extract_title(soup: BeautifulSoup) -> str:
    t = _clean(soup.title.get_text()) if soup.title else "No Data"
    # å»æ‰ç«™ååç¼€
    t = re.sub(r"\s*\|\s*House of Fraser\s*$", "", t, flags=re.I)
    return t or "No Data"

def _extract_desc(soup: BeautifulSoup) -> str:
    m = soup.find("meta", attrs={"property": "og:description"})
    return _clean(m["content"]) if (m and m.get("content")) else "No Data"

def _extract_color(soup: BeautifulSoup) -> str:
    c = soup.select_one("#colourName")
    if c:
        name = _clean(c.get_text())
        if name:
            return name
    ul = soup.find("ul", id="ulColourImages")
    if ul:
        li = ul.find("li", attrs={"aria-checked": "true"})
        if li:
            txt = (li.get("data-text") or "").strip()
            if txt:
                return _clean(txt)
            img = li.find("img")
            if img and img.get("alt"):
                return _clean(img["alt"])
    return "No Data"

def _extract_gender(title: str, soup: BeautifulSoup) -> str:
    t = (title or "").lower()
    if "women" in t: return "å¥³æ¬¾"
    if "men" in t:   return "ç”·æ¬¾"
    if any(k in t for k in ["kids","girls","boys"]): return "ç«¥æ¬¾"
    return "No Data"

def _to_num(s: Optional[str]) -> Optional[float]:
    if not s: return None
    m = re.search(r"([0-9]+(?:\.[0-9]{1,2})?)", s.replace(",", ""))
    return float(m.group(1)) if m else None

def _extract_prices(soup: BeautifulSoup) -> Tuple[Optional[float], Optional[float]]:
    # ç°ä»· / ç¥¨é¢ä»·
    sp = soup.select_one("#lblSellingPrice")
    tp = soup.select_one("#lblTicketPrice")
    if sp:
        curr = _to_num(sp.get_text(" ", strip=True))
        orig = _to_num(tp.get_text(" ", strip=True)) if tp else None
        if curr is not None:
            return curr, (orig if orig is not None else curr)
    # ç»“æ„åŒ–æ•°æ®å…œåº•
    ld = soup.select_one("#structuredDataLdJson")
    if ld:
        try:
            data = json.loads(ld.get_text())
            if isinstance(data, list) and data:
                offers = (data[0] or {}).get("offers") or []
                if offers:
                    curr = _to_num(str(offers[0].get("price")))
                    if curr is not None:
                        return curr, curr
        except Exception:
            pass
    return (None, None)

def _extract_size_pairs(soup: BeautifulSoup) -> List[Tuple[str, str]]:
    sel = soup.find("select", id="sizeDdl")
    results: List[Tuple[str, str]] = []
    if not sel: return results
    for opt in sel.find_all("option"):
        txt = _clean(opt.get_text())
        if not txt or txt.lower().startswith("select"): continue
        cls = opt.get("class") or []
        title = _clean(opt.get("title") or "")
        oos = ("greyOut" in cls) or ("out of stock" in title.lower())
        status = "æ— è´§" if oos else "æœ‰è´§"
        norm = re.sub(r"\s*\(.*?\)\s*", "", txt).strip()
        norm = re.sub(r"^(UK|EU|US)\s+", "", norm, flags=re.I)
        results.append((norm, status))
    return results

def _build_size_lines(pairs: List[Tuple[str, str]]) -> Tuple[str, str]:
    by_size: Dict[str, str] = {}
    for size, status in pairs:
        prev = by_size.get(size)
        if prev is None or (prev == "æ— è´§" and status == "æœ‰è´§"):
            by_size[size] = status
    def _key(k: str):
        m = re.fullmatch(r"\d{1,3}", k)
        return (0, int(k)) if m else (1, k)
    ordered = sorted(by_size.keys(), key=_key)
    ps = ";".join(f"{k}:{by_size[k]}" for k in ordered) or "No Data"
    EAN = "0000000000000"
    psd = ";".join(f"{k}:{3 if by_size[k]=='æœ‰è´§' else 0}:{EAN}" for k in ordered) or "No Data"
    return ps, psd

def parse_info(html: str, url: str) -> Dict[str, Any]:
    soup = BeautifulSoup(html, "html.parser")

    title = _extract_title(soup)
    desc  = _extract_desc(soup)
    color = _extract_color(soup)
    gender = _extract_gender(title, soup)
    curr, orig = _extract_prices(soup)
    if curr is None and orig is not None: curr = orig
    if orig is None and curr is not None: orig = curr

    size_pairs = _extract_size_pairs(soup)
    product_size, product_size_detail = _build_size_lines(size_pairs)

    info = {
        "Product Code": "No Data",
        "Product Name": title or "No Data",
        "Product Description": desc or "No Data",
        "Product Gender": gender,
        "Product Color": color or "No Data",
        "Product Price": f"{orig:.2f}" if orig else "No Data",
        "Adjusted Price": f"{curr:.2f}" if curr else "No Data",
        "Product Material": "No Data",
        "Style Category": "casual wear",
        "Feature": "No Data",
        "Product Size": product_size,
        "Product Size Detail": product_size_detail,
        "Source URL": url,
        "Site Name": SITE_NAME,
    }
    return info

# ================== Selenium & æŠ“å–æµç¨‹ ==================
def get_driver(headless: bool = False):
    options = uc.ChromeOptions()
    if headless:
        options.add_argument("--headless=new")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("--start-maximized")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    return uc.Chrome(options=options)

def process_url(url: str, conn: Connection, delay: float = DEFAULT_DELAY, headless: bool = False) -> Path:
    print(f"\nğŸŒ æ­£åœ¨æŠ“å–: {url}")
    driver = get_driver(headless=headless)
    try:
        driver.get(url)
        if WAIT_PRICE_SECONDS > 0:
            try:
                WebDriverWait(driver, WAIT_PRICE_SECONDS).until(
                    EC.presence_of_element_located((By.ID, "lblSellingPrice"))
                )
            except Exception:
                pass
        if delay > 0:
            time.sleep(delay)
        html = driver.page_source
    finally:
        try: driver.quit()
        except Exception: pass

    info = parse_info(html, url)

    # ============= ç¬¬ä¸‰æ–¹ç›¸ä¼¼åº¦åŒ¹é…ï¼ˆä¸ä¾èµ– keyword å­—æ®µï¼‰ =============
    raw_conn = get_dbapi_connection(conn)
    title = info.get("Product Name") or ""
    color = info.get("Product Color") or ""

    results = match_product(
        raw_conn,
        scraped_title=title,
        scraped_color=color,
        table=PRODUCTS_TABLE,
        name_weight=0.72,
        color_weight=0.18,
        type_weight=0.10,
        topk=5,
        recall_limit=2000,
        # â†“ æ–°å¢ï¼šåç§°/é¢œè‰²ç¡¬é—¨æ§›ï¼ˆä¸¥æ ¼ï¼‰
        min_name=0.92,
        min_color=0.85,
        # é€‰é…ï¼šåªæ¥å—é¢œè‰²â€œç­‰å€¼/åŒä¹‰â€
        require_color_exact=False,   # è®¾ True æ›´ä¸¥
        # é€‰é…ï¼šè¦æ±‚ç±»å‹ä¸€è‡´ï¼ˆjacket/gilet/coat ç­‰ï¼‰
        require_type=False,
    )
    code = choose_best(results, min_score=MIN_SCORE, min_lead=MIN_LEAD)
    if not code and results:
        st = results[0].get("type_scraped")
        if st:
            for r in results:
                if r.get("type_db") == st:
                    code = r["product_code"]
                    print(f"ğŸ¯ tie-break by type â†’ {code}")
                    break

    print("ğŸ” match debug")
    print(f"  raw_title: {title}")
    print(f"  raw_color: {color}")
    if results:
        print(f"  cleaned : {results[0]['title_clean']}  | color_norm: {results[0].get('style_clean','') and results[0]['title_clean'].split(' ')[-1] if False else (results[0]['color_score']>=0)}")
    txt, why = explain_results(results, min_score=MIN_SCORE, min_lead=MIN_LEAD)
    print(txt)
    if code:
        print(f"  â‡’ âœ… choose_best = {code}")
    else:
        print(f"  â‡’ âŒ no match ({why})")

    if not code and results:
        # è®°å½•ä¸€ä¸‹å‰ä¸‰ï¼Œæ–¹ä¾¿ä½ åœ¨æ§åˆ¶å°è§‚å¯Ÿï¼›æ— éœ€ä»»ä½•è°ƒè¯•æ–‡ä»¶
        print("ğŸ§ª top:", " | ".join(f"{r['product_code']}[{r['score']:.3f}]" for r in results[:3]))

    # å‘½åï¼šä¼˜å…ˆç”¨ç¼–ç 
    if code:
        info["Product Code"] = code
        out_name = f"{code}.txt"
    else:
        # ä¸ºé¿å…ä¸åŒé¡µé¢åŒæ ‡é¢˜é€ æˆåŒåï¼ˆå¹¶å‘å†™å†²çªï¼‰ï¼Œè¿½åŠ  URL çŸ­å“ˆå¸Œ
        short = f"{abs(hash(url)) & 0xFFFF:04x}"
        out_name = f"{_safe_name(title)}_{short}.txt"

    out_path = TXT_DIR / out_name
    
    # å¹¶å‘å»é‡
    with _WRITTEN_LOCK:
        if out_name in _WRITTEN:
            print(f"â†©ï¸  è·³è¿‡é‡å¤å†™å…¥ï¼š{out_name}")
            return out_path
        _WRITTEN.add(out_name)

    # åŸå­å†™å…¥
    payload = _kv_txt_bytes(info)
    ok = _atomic_write_bytes(payload, out_path)
    if ok:
        print(f"âœ… å†™å…¥: {out_path.name} (code={info.get('Product Code')})")
    else:
        print(f"â— æ”¾å¼ƒå†™å…¥: {out_path.name}")
    return out_path

def houseoffraser_fetch_info(max_workers: int = MAX_WORKERS_DEFAULT, delay: float = DEFAULT_DELAY, headless: bool = False):
    links_file = Path(LINKS_FILE)
    if not links_file.exists():
        print(f"âš  æ‰¾ä¸åˆ°é“¾æ¥æ–‡ä»¶ï¼š{links_file}")
        return

    urls = [line.strip() for line in links_file.read_text(encoding="utf-8", errors="ignore").splitlines()
            if line.strip() and not line.strip().startswith("#")]
    total = len(urls)
    print(f"ğŸ“„ å…± {total} ä¸ªå•†å“é¡µé¢å¾…è§£æ...ï¼ˆå¹¶å‘ {max_workers}ï¼‰")
    if total == 0:
        return

    engine_url = f"postgresql+psycopg2://{PG['user']}:{PG['password']}@{PG['host']}:{PG['port']}/{PG['dbname']}"
    engine = create_engine(engine_url)

    indexed = list(enumerate(urls, start=1))

    def _worker(idx_url):
        idx, u = idx_url
        print(f"[å¯åŠ¨] [{idx}/{total}] {u}")
        try:
            with engine.begin() as conn:
                path = process_url(u, conn=conn, delay=delay, headless=headless)
            return (idx, u, str(path), None)
        except Exception as e:
            return (idx, u, None, e)

    ok, fail = 0, 0
    with ThreadPoolExecutor(max_workers=max_workers, thread_name_prefix="hof") as ex:
        futures = [ex.submit(_worker, iu) for iu in indexed]
        for fut in as_completed(futures):
            idx, u, path, err = fut.result()
            if err is None:
                ok += 1
                print(f"[å®Œæˆ] [{idx}/{total}] âœ… {u} -> {path}")
            else:
                fail += 1
                print(f"[å¤±è´¥] [{idx}/{total}] âŒ {u}\n    {repr(err)}")

    print(f"\nğŸ“¦ ä»»åŠ¡ç»“æŸï¼šæˆåŠŸ {ok}ï¼Œå¤±è´¥ {fail}ï¼Œæ€»è®¡ {total}")

if __name__ == "__main__":
    houseoffraser_fetch_info()
