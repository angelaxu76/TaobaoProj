# -*- coding: utf-8 -*-
"""
House of Fraser | Barbour - æ–°ç‰ˆ Next.js PDP è§£æ
- ä¸å†å°è¯•æ—§ç‰ˆï¼›ç»Ÿä¸€æŒ‰æ–°æ ˆ(JSON-LD + DOM)è§£æ
- å•å®ä¾‹ Seleniumï¼ˆundetected-chromedriverï¼‰ï¼Œé¦–ä¸ªå•†å“é¡µç­‰å¾…10ç§’æ‰‹åŠ¨ç‚¹ Cookie
- è¾“å‡ºæ²¿ç”¨æ—§æœ‰ KV æ–‡æœ¬æ¨¡æ¿ï¼Œä¸æ”¹å­—æ®µå/é¡ºåºï¼Œä¿è¯ä¸‹æ¸¸å…¼å®¹
"""

from __future__ import annotations

from logging import info
import os, re, json, time, tempfile, threading, html as ihtml
from pathlib import Path
from typing import Optional, Tuple, List, Dict, Any
from urllib.parse import urlparse, parse_qsl, urlunparse, urlencode
from collections import OrderedDict

# ---- ä¾èµ– ----
import undetected_chromedriver as uc
from bs4 import BeautifulSoup
from selenium.webdriver.common.by import By
from sqlalchemy import create_engine
from sqlalchemy.engine import Connection

# ---- é¡¹ç›®å†…æ¨¡å—ï¼ˆä¿æŒä¸å˜ï¼‰----
from config import BARBOUR, BRAND_CONFIG
from barbour.core.site_utils import assert_site_or_raise as canon
from barbour.core.sim_matcher import match_product, choose_best, explain_results
from common_taobao.size_utils import clean_size_for_barbour as _norm_size  # å°ºç æ¸…æ´—

# ================== å¸¸é‡/è·¯å¾„ ==================
SITE_NAME = canon("houseoffraser")
LINKS_FILE: str = BARBOUR["LINKS_FILES"]["houseoffraser"]
TXT_DIR: Path = BARBOUR["TXT_DIRS"]["houseoffraser"]
TXT_DIR.mkdir(parents=True, exist_ok=True)
DEBUG_DIR: Path = TXT_DIR / "_debug"
DEBUG_DIR.mkdir(parents=True, exist_ok=True)

PRODUCTS_TABLE: str = BRAND_CONFIG.get("barbour", {}).get("PRODUCTS_TABLE", "barbour_products")
OFFERS_TABLE: Optional[str] = BRAND_CONFIG.get("barbour", {}).get("OFFERS_TABLE")
PG = BRAND_CONFIG["barbour"]["PGSQL_CONFIG"]

WAIT_HYDRATE_SECONDS = 22
DEFAULT_DELAY = 0.0
MAX_WORKERS_DEFAULT = 1  # å»ºè®®ä¸²è¡Œæœ€ç¨³ï¼›å¹¶å‘è¯·æ”¹ä¸ºâ€œæ¯çº¿ç¨‹1ä¸ªdriverâ€æ–¹æ¡ˆ
MIN_SCORE = 0.72
MIN_LEAD = 0.04
NAME_WEIGHT = 0.75
COLOR_WEIGHT = 0.25

# ================== URLâ†’Code ç¼“å­˜ ==================
URL_CODE_CACHE: Dict[str, str] = {}
_URL_CODE_CACHE_READY = False


import json, re
from bs4 import BeautifulSoup

# ---------- helpers: æå– next/å†…åµŒ JSON æ–‡æœ¬ ----------
def _extract_next_json_chunks(html: str) -> str:
    # ç›´æ¥ç”¨æ­£åˆ™åœ¨ HTML æ–‡æœ¬é‡Œæ‰¾åŒ…å«å…³é”®å­—æ®µçš„å¤§å— JSON
    # å°½é‡ä¿å®ˆï¼šç”¨æœ€å°åŒ¹é… + å…³é”®é”šç‚¹
    m = re.search(r'"gender":"(?:Mens|Womens|Girls|Boys)".{0,2000}?"variants":\[(.*?)\]\}', html, re.S)
    if m: 
        return m.group(0)
    # å…œåº•ï¼šæ‰¾åŒ…å« sizes.allSizes çš„å—
    m = re.search(r'"sizes":\{"allSizes":\[(.*?)\]\}', html, re.S)
    return m.group(0) if m else ""

def _extract_color_from_og_alt(soup: BeautifulSoup) -> str:
    # å–ç¬¬ä¸€ä¸ª og:image:altï¼Œå½¢å¦‚ "Black BK11 - Barbour International - xxx"
    for tag in soup.find_all("meta", {"property": "og:image:alt"}):
        content = (tag.get("content") or "").strip()
        if content:
            # é¢œè‰²å = ç¬¬ä¸€ä¸ª " - " ä¹‹å‰ï¼Œå†å»æ‰æœ«å°¾çš„è‰²ç 
            first = content.split(" - ", 1)[0]  # e.g. "Black BK11"
            # å»æ‰ç±»ä¼¼ BK11/NY91 è¿™ç±»è‰²ç 
            first = re.sub(r"\b[A-Z]{2}\d{2,}\b", "", first).strip()
            if first:
                return first
    return ""

def _extract_all_sizes(html: str):
    """
    è¿”å› list[str]ï¼Œæ¥è‡ª sizes.allSizes[].sizeã€‚
    """
    sizes = []
    ms = re.search(r'"sizes"\s*:\s*\{\s*"allSizes"\s*:\s*(\[[^\]]*\])', html, re.S | re.I)
    if ms:
        try:
            arr = json.loads(ms.group(1))
            for it in arr:
                s = (it.get("size") or "").strip()
                if s:
                    sizes.append(s)
        except Exception:
            pass
    return sizes


def _extract_sizes_dom_fallback(soup: BeautifulSoup):
    """
    ä»æ¸²æŸ“åçš„å°ºå¯¸æŒ‰é’®å®¹å™¨æŠ“å–ï¼š
    - data-testid="swatch-button-enabled" => æœ‰è´§
    - data-testid="swatch-button-disabled" => æ— è´§
    - æœ‰äº›ç«™ä¸æ¸²æŸ“ disabled æŒ‰é’®ï¼Œæ­¤æ—¶åªæ‹¿åˆ° enabled çš„é‚£éƒ¨åˆ†
    """
    enabled = set()
    disabled = set()

    # å¯ç”¨æŒ‰é’®
    for btn in soup.select('[data-testid="swatch-button-enabled"]'):
        val = (btn.get("value") or btn.get_text() or "").strip()
        if val:
            enabled.add(re.sub(r"\s+", " ", val))

    # ç¦ç”¨æŒ‰é’®ï¼ˆå¦‚æœæ¸²æŸ“ï¼‰
    for btn in soup.select('[data-testid="swatch-button-disabled"], button[disabled][data-testid*="swatch"]'):
        val = (btn.get("value") or btn.get_text() or "").strip()
        if val:
            disabled.add(re.sub(r"\s+", " ", val))

    if not enabled and not disabled:
        return []

    # å¦‚æœç¦ç”¨æ²¡æ¸²æŸ“å‡ºæ¥ï¼Œå°±åªè¿”å› enabledï¼Œå…¶ä»–äº¤ç»™ allSizes åˆå¹¶é€»è¾‘è¡¥æ— è´§
    entries = [(s, "æœ‰è´§") for s in sorted(enabled, key=lambda x: (len(x), x))]
    entries += [(s, "æ— è´§") for s in sorted(disabled, key=lambda x: (len(x), x))]
    return entries



def _extract_color_gender_from_json(html: str) -> (str, str):
    block = _extract_next_json_chunks(html)
    color = ""
    gender = ""
    if block:
        mc = re.search(r'"color"\s*:\s*"([^"]+)"', block)
        if mc:
            raw = mc.group(1)  # e.g. "Black BK11"
            color = re.sub(r"\b[A-Z]{2}\d{2,}\b", "", raw).strip()
        mg = re.search(r'"gender"\s*:\s*"(Mens|Womens|Girls|Boys)"', block)
        if mg:
            g = mg.group(1).lower()
            # ç»Ÿä¸€åˆ°ä½ åŸæ¥ TXT ä¹ æƒ¯ï¼ˆmen / womenï¼‰ï¼Œä¹Ÿå¯è¾“å‡º mens/womens
            mapping = {"mens":"men", "womens":"women", "girls":"women", "boys":"men"}
            gender = mapping.get(g, g)
    return color, gender

def _extract_color_new(soup: BeautifulSoup, html: str) -> str:
    color, _ = _extract_color_gender_from_json(html)
    if color:
        return color
    alt_color = _extract_color_from_og_alt(soup)
    return alt_color or "No Data"

def _infer_gender_from_code(code: Optional[str]) -> str:
    """
    ä»…åœ¨å·²æ‹¿åˆ°äº§å“ç¼–ç æ—¶å¯ç”¨ã€‚
    å¸¸è§å‰ç¼€ï¼ˆBarbour/Internationalï¼‰ï¼š
      ç”·æ¬¾ï¼šMQU, MWX, MSH, MKN, MGL, MFL, MGI, MLI, MSW, MCA...
      å¥³æ¬¾ï¼šLQU, LWX, LSH, LKN, LGL, LFL, LGI, LLI, LSW, LCA...
    è¿”å›ï¼šmen / women / No Data
    """
    if not code:
        return "No Data"
    c = code.strip().upper()
    # å…ˆçœ‹é¦–å­—æ¯
    if c.startswith("M"):
        return "men"
    if c.startswith("L"):
        return "women"
    # å†çœ‹å¸¸è§ 3 ä½å‰ç¼€ï¼ˆæ›´ç²¾ç¡®ï¼‰
    male3  = ("MQU", "MWX", "MSH", "MKN", "MGL", "MFL", "MGI", "MLI", "MSW", "MCA")
    female3= ("LQU", "LWX", "LSH", "LKN", "LGL", "LFL", "LGI", "LLI", "LSW", "LCA")
    pre3 = c[:3]
    if pre3 in male3:
        return "men"
    if pre3 in female3:
        return "women"
    return "No Data"

def _gender_to_cn(g: str) -> str:
    if not g:
        return "No Data"
    g = g.strip().lower()
    if g in ("men", "mens"):
        return "ç”·æ¬¾"
    if g in ("women", "womens"):
        return "å¥³æ¬¾"
    # å¦‚ä½ ä»¥åæƒ³åŒºåˆ†ç«¥æ¬¾ï¼Œå¯åœ¨è¿™æ‰©å±•ï¼š
    if g in ("boys", "boy"):
        return "ç”·æ¬¾"   # æˆ–è€…è¿”å› "ç«¥æ¬¾-ç”·"
    if g in ("girls", "girl"):
        return "å¥³æ¬¾"   # æˆ–è€…è¿”å› "ç«¥æ¬¾-å¥³"
    return "No Data"


def _extract_gender_new(soup: BeautifulSoup, html: str, url: str) -> str:
    """
    ä¼˜å…ˆä»æ•´é¡µ JSON ä¸­æŠ½å– "gender":"Mens|Womens|Boys|Girls"ï¼›
    è‹¥æ²¡æœ‰ï¼Œå†ä»é¢åŒ…å±‘/æ ‡é¢˜/æè¿°é‡Œæ¨æ–­ï¼›æœ€åç”¨ URL å…œåº•ã€‚
    è¿”å›ï¼šmen / women / No Data
    """
    # 1) JSONï¼ˆæ•´é¡µä»»æ„ä½ç½®ï¼‰
    m = re.search(r'"gender"\s*:\s*"(Mens|Womens|Girls|Boys)"', html, re.I)
    if m:
        g = m.group(1).lower()
        mapping = {"mens": "men", "womens": "women", "girls": "women", "boys": "men"}
        return mapping.get(g, g)

    # 2) é¢åŒ…å±‘/æ ‡é¢˜/æè¿° æ¨æ–­
    # ï¼ˆå°½é‡ä¸ä¾èµ–ä½ å…¶ä»–å‡½æ•°ï¼Œé¿å…å‘½åå†²çªï¼‰
    bc = soup.select("nav[aria-label*=breadcrumb] a, ol[aria-label*=breadcrumb] a")
    bc_txt = " ".join(a.get_text(" ", strip=True) for a in bc) if bc else ""
    title = soup.title.get_text(strip=True) if soup.title else ""
    meta_desc = soup.find("meta", {"name": "description"})
    desc = (meta_desc.get("content") or "") if meta_desc else ""
    blob = " ".join([bc_txt.lower(), title.lower(), desc.lower()])

    if any(w in blob for w in ("womens", "women", "ladies", "women's", "lady")):
        return "women"
    if any(w in blob for w in ("mens", "men", "men's", "man")):
        return "men"

    # 3) URL å…œåº•
    ul = (url or "").lower()
    if "/women" in ul or "womens" in ul:
        return "women"
    if "/men" in ul or "mens" in ul:
        return "men"

    return "No Data"



# ---------- sizes & availability ----------
def _extract_sizes_from_variants(html: str):
    """
    è¿”å› list[(size, status)]ï¼Œä»…åŒ…å« variants é‡Œå‡ºç°çš„å°ºç ï¼›
    status = "æœ‰è´§"/"æ— è´§"ï¼ˆä»¥ isOnStock ä¸ºå‡†ï¼‰ã€‚
    """
    entries = []
    # æ”¾å®½åŒ¹é…èŒƒå›´ï¼Œç¡®ä¿ "size" å’Œ "isOnStock" ä¸åŒå±‚ä¹Ÿèƒ½å‘½ä¸­
    patt = r'"size"\s*:\s*"([^"]+?)".{0,4000}?"isOnStock"\s*:\s*(true|false)'
    for m in re.finditer(patt, html, re.S | re.I):
        size = m.group(1).strip()
        avail = (m.group(2).lower() == "true")
        if size:
            entries.append((size, "æœ‰è´§" if avail else "æ— è´§"))
    return entries



def _extract_sizes_from_allSizes(html: str):
    # å…œåº•ï¼šæ²¡æœ‰ isOnStockï¼Œå°±è®¤ä¸ºé¡µé¢åœ¨å”® â†’ è®°ä¸ºæœ‰è´§
    entries = []
    ms = re.search(r'"sizes"\s*:\s*\{\s*"allSizes"\s*:\s*(\[[^\]]*\])', html, re.S)
    if ms:
        try:
            arr = json.loads(ms.group(1))
            for it in arr:
                size = (it.get("size") or "").strip()
                if size:
                    entries.append((size, "æœ‰è´§"))
        except Exception:
            pass
    return entries

def _extract_sizes_new(soup: BeautifulSoup, html: str):
    """
    ç»Ÿä¸€å‡ºå£ï¼š
    1) allSizes = å…¨é‡å°ºç 
    2) æœ‰è´§é›†åˆ = variants(isOnStock=true) âˆª DOM-enabled
    3) ä¸åœ¨æœ‰è´§é›†åˆä½†åœ¨ allSizes çš„ => æ— è´§
    4) å¦‚æœ allSizes ä¸ºç©ºï¼šä»…ç”¨ DOMï¼ˆæœ‰åˆ™æœ‰ï¼Œæ— åˆ™ No Dataï¼‰
    """
    all_sizes = _extract_all_sizes(html)
    var_entries = _extract_sizes_from_variants(html)  # åªå«å‡ºç°åœ¨ variants çš„å°ºç 
    dom_entries = _extract_sizes_dom_fallback(soup)

    instock_from_variants = {s for s, st in var_entries if st == "æœ‰è´§"}
    instock_from_dom = {s for s, st in dom_entries if st == "æœ‰è´§"}
    oos_from_dom = {s for s, st in dom_entries if st == "æ— è´§"}

    # 1) æœ‰å…¨é‡å°ºç æ—¶ï¼šæŒ‰é›†åˆæ ‡è®°
    if all_sizes:
        in_stock = set()
        in_stock |= instock_from_variants
        in_stock |= instock_from_dom

        # æŒ‰é¡µé¢æ˜¾ç¤ºçš„è‡ªç„¶é¡ºåºè¾“å‡ºï¼ˆä¸å¼ºè¡Œæ’åºï¼‰
        by_size = {}
        ordered = []
        for s in all_sizes:
            s_norm = re.sub(r"\s+", " ", s).strip()
            if not s_norm:
                continue
            status = "æœ‰è´§" if s_norm in in_stock else "æ— è´§"
            by_size[s_norm] = status
            ordered.append(s_norm)

        EAN = "0000000000000"
        product_size        = ";".join(f"{s}:{by_size[s]}" for s in ordered) or "No Data"
        product_size_detail = ";".join(f"{s}:{3 if by_size[s]=='æœ‰è´§' else 0}:{EAN}" for s in ordered) or "No Data"
        return product_size, product_size_detail

    # 2) æ²¡æœ‰ allSizesï¼šé€€å› DOM/variants çš„å¹¶é›†
    if var_entries or dom_entries:
        merged = {}
        order = []
        for s, st in (var_entries + dom_entries):
            s_norm = re.sub(r"\s+", " ", s).strip()
            if not s_norm:
                continue
            # æœ‰è´§ä¼˜å…ˆè¦†ç›–æ— è´§
            if (s_norm not in merged) or (merged[s_norm] == "æ— è´§" and st == "æœ‰è´§"):
                merged[s_norm] = st
                if s_norm not in order:
                    order.append(s_norm)

        EAN = "0000000000000"
        product_size        = ";".join(f"{s}:{merged[s]}" for s in order) or "No Data"
        product_size_detail = ";".join(f"{s}:{3 if merged[s]=='æœ‰è´§' else 0}:{EAN}" for s in order) or "No Data"
        return product_size, product_size_detail

    return "No Data", "No Data"




def _normalize_url(u: str) -> str:
    return u.strip() if u else ""

def get_dbapi_connection(conn_or_engine):
    if hasattr(conn_or_engine, "cursor"): return conn_or_engine
    if hasattr(conn_or_engine, "raw_connection"): return conn_or_engine.raw_connection()
    c = getattr(conn_or_engine, "connection", None)
    if c is not None:
        dbapi = getattr(c, "dbapi_connection", None)
        if dbapi is not None and hasattr(dbapi, "cursor"): return dbapi
        inner = getattr(c, "connection", None)
        if inner is not None and hasattr(inner, "cursor"): return inner
        if hasattr(c, "cursor"): return c
    return conn_or_engine

def _safe_sql_to_cache(raw_conn, sql: str, params=None) -> Dict[str, str]:
    cache = OrderedDict()
    try:
        with raw_conn.cursor() as cur:
            cur.execute(sql, params or {})
            for url, code in cur.fetchall():
                if url and code:
                    cache[_normalize_url(str(url))] = str(code).strip()
    except Exception:
        try: raw_conn.rollback()
        except Exception: pass
    return cache

def build_url_code_cache(raw_conn, products_table: str, offers_table: Optional[str], site_name: str):
    """å¯åŠ¨æ—¶æ„å»ºä¸€æ¬¡ URLâ†’ProductCode æ˜ å°„ç¼“å­˜ã€‚"""
    global URL_CODE_CACHE, _URL_CODE_CACHE_READY
    if _URL_CODE_CACHE_READY:
        return URL_CODE_CACHE

    cache = OrderedDict()

    if offers_table:
        candidates = [
            ("offer_url",   "product_code"),
            ("source_url",  "product_code"),
            ("product_url", "product_code"),
            ("offer_url",   "color_code"),
            ("source_url",  "color_code"),
            ("product_url", "color_code"),
        ]
        for url_col, code_col in candidates:
            sql = f"""
                SELECT {url_col}, {code_col}
                  FROM {offers_table}
                 WHERE site_name = %(site)s
                   AND {url_col} IS NOT NULL
                   AND {code_col} IS NOT NULL
            """
            cache.update(_safe_sql_to_cache(raw_conn, sql, {"site": site_name}))

    for url_col in ("source_url", "offer_url", "product_url"):
        sql = f"""
            SELECT {url_col}, product_code
              FROM {products_table}
             WHERE {url_col} IS NOT NULL
               AND product_code IS NOT NULL
        """
        cache.update(_safe_sql_to_cache(raw_conn, sql))

    URL_CODE_CACHE = dict(cache)
    _URL_CODE_CACHE_READY = True
    print(f"ğŸ§  URLâ†’Code ç¼“å­˜æ„å»ºå®Œæˆï¼š{len(URL_CODE_CACHE)} æ¡")
    return URL_CODE_CACHE

# ================== æ–‡ä»¶å†™å…¥/æ¨¡æ¿ ==================
def _atomic_write_bytes(data: bytes, dst: Path, retries: int = 6, backoff: float = 0.25) -> bool:
    dst.parent.mkdir(parents=True, exist_ok=True)
    for i in range(retries):
        tmp = None
        try:
            with tempfile.NamedTemporaryFile(delete=False, dir=str(dst.parent),
                                             prefix=".tmp_", suffix=f".{os.getpid()}.{threading.get_ident()}") as tf:
                tmp = Path(tf.name)
                tf.write(data); tf.flush(); os.fsync(tf.fileno())
            try:
                os.replace(tmp, dst)
            finally:
                if tmp and tmp.exists():
                    try: tmp.unlink(missing_ok=True)
                    except Exception: pass
            return True
        except Exception:
            if dst.exists(): return True
            time.sleep(backoff * (i + 1))
            try:
                if tmp and tmp.exists(): tmp.unlink(missing_ok=True)
            except Exception:
                pass
    return dst.exists()

def _kv_txt_bytes(info: Dict[str, Any]) -> bytes:
    # âœ¨ ä¿æŒä¸æ—§ç‰ˆå®Œå…¨ä¸€è‡´çš„ KV è¾“å‡ºå­—æ®µé¡ºåº
    fields = [
        "Product Code", "Product Name", "Product Description", "Product Gender",
        "Product Color", "Product Price", "Adjusted Price", "Product Material",
        "Style Category", "Feature", "Product Size", "Product Size Detail",
        "Source URL", "Site Name"
    ]
    lines = [f"{k}: {info.get(k, 'No Data')}" for k in fields]
    return ("\n".join(lines) + "\n").encode("utf-8", errors="ignore")

def _safe_name(s: str) -> str:
    return re.sub(r"[\\/:*?\"<>|'\s]+", "_", (s or "NoName"))

def _dump_debug_html(html: str, url: str, tag: str = "debug1") -> Path:
    short = f"{abs(hash(url)) & 0xFFFFFFFF:08x}"
    out = DEBUG_DIR / f"{tag}_{short}.html"
    _atomic_write_bytes(html.encode("utf-8", errors="ignore"), out)
    print(f"ğŸ§ª HTML dump â†’ {out}")
    return out

def _clean(s: Optional[str]) -> str:
    return re.sub(r"\s+", " ", (s or "").strip())

def _to_num(s: Optional[str]) -> Optional[float]:
    if not s: return None
    m = re.search(r"([0-9]+(?:\.[0-9]{1,2})?)", s.replace(",", ""))
    return float(m.group(1)) if m else None

# ================== ç­‰å¾…æ°´åˆ ==================
def _soft_scroll(driver, steps=6, pause=0.45):
    for _ in range(steps):
        try:
            driver.execute_script("window.scrollBy(0, Math.floor(document.body.scrollHeight * 0.28));")
        except Exception:
            pass
        time.sleep(pause)
    try:
        driver.execute_script("window.scrollTo(0, 0);")
    except Exception:
        pass
    time.sleep(0.3)

def wait_pdp_ready(driver, timeout: int = WAIT_HYDRATE_SECONDS) -> bool:
    key_css = ", ".join([
        # price/title
        "[data-testid*='price']","[data-component*='price']","[itemprop='price']","meta[itemprop='price']",
        "h1","[data-testid*='title']","[data-component*='title']",
        # sizes
        "button[aria-pressed][data-testid*='size']","li[role='option']","div[role='option']",
        "option[data-testid*='drop-down-option']","#sizeDdl option",
        # JSON-LD
        "script[type='application/ld+json']",
    ])
    end = time.time() + timeout
    while time.time() < end:
        try:
            if driver.find_elements(By.CSS_SELECTOR, key_css):
                return True
        except Exception:
            pass
        _soft_scroll(driver, steps=2, pause=0.45)
    return False

# ================== JSON-LD è§£æ ==================
def _pick_first(x):
    if isinstance(x, list) and x: return x[0]
    return x

def parse_jsonld(html: str) -> dict:
    soup = BeautifulSoup(html, "html.parser")
    blocks = []
    for tag in soup.find_all("script", {"type": "application/ld+json"}):
        txt = (tag.string or tag.text or "").strip()
        if not txt: continue
        try:
            data = json.loads(txt)
            blocks.append(data)
        except Exception:
            continue

    def _walk(obj):
        if isinstance(obj, dict):
            yield obj
            for v in obj.values():
                yield from _walk(v)
        elif isinstance(obj, list):
            for it in obj:
                yield from _walk(it)

    product = None
    offers = []
    for b in blocks:
        for node in _walk(b):
            if not isinstance(node, dict): continue
            t = node.get("@type") or node.get("@type".lower())
            if t == "Product":
                product = node
            elif t == "Offer":
                offers.append(node)

    out = {}
    if product:
        out["title"] = _pick_first(product.get("name"))
        out["sku"] = product.get("sku") or product.get("mpn") or product.get("productID")
        brand = product.get("brand")
        if isinstance(brand, dict): out["brand"] = brand.get("name")
        else: out["brand"] = brand
        imgs = product.get("image") or []
        if isinstance(imgs, str): imgs = [imgs]
        out["images"] = imgs
        desc = product.get("description") or ""
        desc = re.sub(r"<[^>]+>", " ", desc)
        out["description"] = re.sub(r"\s+", " ", ihtml.unescape(desc)).strip()

    price = currency = availability = None
    url = None
    for off in offers:
        p = off.get("price") or off.get("priceSpecification", {}).get("price")
        if p:
            price = p
            currency = off.get("priceCurrency") or off.get("priceSpecification", {}).get("priceCurrency")
            availability = off.get("availability")
            url = off.get("url")
            break
    if price: out["price"] = str(price)
    if currency: out["currency"] = currency
    if availability:
        out["availability"] = availability.split("/")[-1] if "/" in availability else availability
    if url: out["url"] = url
    return out

# ================== æ€§åˆ«/é¢œè‰²/å°ºç ï¼ˆåŠ å›ºç‰ˆï¼‰ ==================
GENDER_WORDS = (
    ("Womens", ("womens", "women", "ladies", "women's", "lady")),
    ("Mens",   ("mens", "men", "men's", "man")),
    ("Boys",   ("boys", "boy")),
    ("Girls",  ("girls", "girl")),
)
COLOR_WORDS = [
    "Black","Navy","Green","Olive","Brown","Blue","Red","Cream","Beige","Grey","Gray",
    "White","Pink","Burgundy","Khaki","Stone","Tan","Orange","Yellow","Purple"
]

def _infer_gender_from_text(*texts) -> str:
    blob = " ".join(t.lower() for t in texts if t).strip()
    for label, keys in GENDER_WORDS:
        if any(k in blob for k in keys):
            return label
    return "No Data"

def _extract_gender(html: str, soup: BeautifulSoup, title: str, desc: str, url: str) -> str:
    bc = soup.select("nav[aria-label*=breadcrumb] a, ol[aria-label*=breadcrumb] a")
    bc_txt = " ".join(a.get_text(" ", strip=True) for a in bc) if bc else ""
    g = _infer_gender_from_text(bc_txt, title, desc, url)
    if g != "No Data": return g
    m = re.search(r'"gender"\s*:\s*"(?P<g>Womens|Mens|Boys|Girls)"', html, re.I)
    if m: return m.group("g").capitalize()
    return _infer_gender_from_text(desc, title)

def _extract_color(html: str, soup: BeautifulSoup, title: str) -> str:
    el = soup.select_one("[data-testid*='colour'] [data-testid*='value'], [data-component*='colour'], [aria-label*='Colour'] [aria-live]")
    if el:
        c = el.get_text(strip=True)
        if c: return c
    m = re.search(r'"color"\s*:\s*"([A-Za-z /-]{3,20})"', html)
    if m: return m.group(1).strip()
    for w in COLOR_WORDS:
        if re.search(rf"\b{re.escape(w)}\b", title, re.I):
            return w
    return "No Data"

def _extract_sizes_legacy_dropdown(soup: BeautifulSoup) -> Tuple[str, str]:
    entries = []

    # 1) æŒ‰é’®
    for btn in soup.select("button[aria-pressed][data-testid*='size'], button[aria-pressed][aria-label*='Size']"):
        lab = (btn.get_text() or btn.get("aria-label") or "").strip()
        if not lab: continue
        disabled = (btn.get("disabled") is not None) or (btn.get("aria-disabled") in ("true", "True"))
        status = "æ— è´§" if disabled else "æœ‰è´§"
        entries.append((lab, status))

    # 2) åˆ—è¡¨/å¯é€‰é¡¹
    for node in soup.select("li[role='option'], div[role='option']"):
        lab = (node.get_text() or node.get("aria-label") or "").strip()
        if not lab: continue
        if lab.lower().startswith(("select size", "choose size")):
            continue
        disabled = node.get("aria-disabled") in ("true", "True") or "disabled" in (node.get("class") or [])
        status = "æ— è´§" if disabled else "æœ‰è´§"
        entries.append((lab, status))

    # 3) ä¸‹æ‹‰
    for opt in soup.select("select option[data-testid*='drop-down-option'], #sizeDdl option"):
        lab = (opt.get_text() or "").strip()
        if not lab or lab.lower().startswith(("select", "choose")):
            continue
        clean = re.sub(r"\s*-\s*Out\s*of\s*stock\s*$", "", lab, flags=re.I).strip(" -/")
        disabled = opt.has_attr("disabled") or (opt.get("aria-disabled") == "true") or "out of stock" in lab.lower()
        status = "æ— è´§" if disabled else "æœ‰è´§"
        entries.append((clean or lab, status))

    # 4) å…œåº•ï¼šå½¢ä¼¼å°ºç çš„æŒ‰é’®
    if not entries:
        for btn in soup.select("button, [role='option']"):
            lab = (btn.get_text() or getattr(btn, "get", lambda *_: None)("aria-label") or "").strip()
            if not lab: continue
            if re.search(r"\b\d{1,2}(\s*\([A-Z0-9]+\))?$", lab):
                disabled = hasattr(btn, "get") and (btn.get("disabled") is not None or btn.get("aria-disabled") == "true")
                status = "æ— è´§" if disabled else "æœ‰è´§"
                entries.append((lab, status))

    if not entries:
        return "No Data", "No Data"

    ordered = []
    seen = {}
    for label, status in entries:
        label = re.sub(r"\s+", " ", label).strip()
        if label not in seen or (seen[label] == "æ— è´§" and status == "æœ‰è´§"):
            seen[label] = status
            if label not in ordered: ordered.append(label)

    EAN = "0000000000000"
    product_size        = ";".join(f"{s}:{seen[s]}" for s in ordered) or "No Data"
    product_size_detail = ";".join(f"{s}:{3 if seen[s]=='æœ‰è´§' else 0}:{EAN}" for s in ordered) or "No Data"
    return product_size, product_size_detail

def _from_jsonld_product_new(soup: BeautifulSoup) -> Dict[str, Any]:
    """
    ä» JSON-LD æå–æœ€æ ¸å¿ƒçš„äº§å“å…ƒæ•°æ®ï¼ˆname/description/skuï¼‰ã€‚
    æ³¨æ„ï¼šæ­¤å‡½æ•°ä¸ parse_jsonld çš„å­—æ®µå‘½åå¯¹é½ï¼šname/description/skuã€‚
    """
    out = {"name": None, "description": None, "sku": None}

    # ç›´æ¥å¤ç”¨å·²å­˜åœ¨çš„ parse_jsonldï¼šæŠŠ soup è½¬æˆ html å†è§£æ
    try:
        html = str(soup)
        jd = parse_jsonld(html) or {}
    except Exception:
        jd = {}

    # parse_jsonld è¿”å›çš„æ˜¯ title/description/skuï¼Œè¿™é‡Œåšä¸€æ¬¡å­—æ®µå¯¹é½
    title = jd.get("title")
    if title:
        out["name"] = title
    if jd.get("description"):
        out["description"] = jd["description"]
    if jd.get("sku"):
        out["sku"] = jd["sku"]

    # å…œåº•ï¼šæ²¡æœ‰æ‹¿åˆ° name æ—¶ï¼Œç”¨ <h1> æˆ– <title>
    if not out["name"]:
        h1 = soup.select_one("h1,[data-testid*='title'],[data-component*='title']")
        out["name"] = h1.get_text(strip=True) if h1 else (soup.title.get_text(strip=True) if soup.title else None)

    return out

def _extract_prices_new(soup: BeautifulSoup, html: str) -> Tuple[Optional[float], Optional[float]]:
    """
    è¿”å› (current_price, original_price)
    - current_priceï¼šå½“å‰å±•ç¤ºä»·ï¼ˆå¸¸ä¸ºæŠ˜åä»·ï¼‰
    - original_priceï¼šåŸä»·ï¼ˆticket/was/rrpï¼‰
    é€»è¾‘ï¼šJSON-LD çš„ price ä½œä¸ºâ€œå½“å‰ä»·â€ä¼˜å…ˆï¼›åŸä»·ä» DOM çš„ ticket/was èŠ‚ç‚¹å…œåº•ã€‚
    """
    current_price = None
    original_price = None

    # 1) JSON-LD çš„ä»·æ ¼ä½œä¸ºå½“å‰ä»·
    try:
        jd = parse_jsonld(html) or {}
        if jd.get("price"):
            current_price = float(str(jd["price"]).replace(",", ""))
    except Exception:
        pass

    # 2) DOM é‡Œæ‰¾åŸä»·ï¼ˆticket/was/rrpï¼‰
    was_el = soup.select_one(
        "[data-testid*='ticket-price'], [data-component*='ticket'], .price-was, .wasPrice, .rrp"
    )
    if was_el:
        original_price = _to_num(was_el.get_text(" ", strip=True))

    # 3) è‹¥ç¼ºå¤±ï¼Œäº’ç›¸å…œåº•
    if original_price is None and current_price is not None:
        original_price = current_price
    if current_price is None:
        # å†å°è¯•ä»å¯è§ä»·æ ¼å—å–ä¸€æ¬¡â€œå½“å‰ä»·â€
        cur_el = soup.select_one("[data-testid*='price'], [data-component*='price'], [itemprop='price'], meta[itemprop='price']")
        if cur_el:
            if getattr(cur_el, "name", "") == "meta":
                current_price = _to_num(cur_el.get("content") or "")
            else:
                current_price = _to_num(cur_el.get_text(" ", strip=True))
        # ä»ç„¶æ²¡æœ‰å°±é€€å›åŸä»·
        if current_price is None and original_price is not None:
            current_price = original_price

    return current_price, original_price

# ================== æ ¸å¿ƒè§£æ ==================
def parse_info_new(html: str, url: str) -> Dict[str, Any]:
    soup = BeautifulSoup(html, "html.parser")
    jd = _from_jsonld_product_new(soup) or {}

    title = jd.get("name") or (soup.title.get_text(strip=True) if soup.title else "No Data")
    desc  = jd.get("description") or "No Data"

    curr, orig = _extract_prices_new(soup, html)
    if curr is None and orig is not None: curr = orig
    if orig is None and curr is not None: orig = curr

    # >>> æ–°å¢ï¼šé¢œè‰²/æ€§åˆ«/å°ºç  <<<
    color  = _extract_color_new(soup, html)
    gender = _extract_gender_new(soup, html, url)
    product_size, product_size_detail = _extract_sizes_new(soup, html)

    info = {
        "Product Code": jd.get("sku") or "No Data",   # è¿™é‡Œä»æ˜¯ç»„åˆ SKUï¼ˆå¦‚ 321534ï¼‰ï¼›ç²¾ç¡®ç¼–ç ä»äº¤ç”±ä½ ç°æœ‰çš„ DB åŒ¹é…é€»è¾‘
        "Product Name": title or "No Data",
        "Product Description": desc or "No Data",
        "Product Gender": gender,
        "Product Color": color or "No Data",
        "Product Price": f"{orig:.2f}" if isinstance(orig, (int, float)) else "No Data",
        "Adjusted Price": f"{curr:.2f}" if isinstance(curr, (int, float)) else "No Data",
        "Product Material": "No Data",
        "Style Category": "casual wear",
        "Feature": "No Data",
        "Product Size": product_size,
        "Product Size Detail": product_size_detail,
        "Source URL": url,
        "Site Name": SITE_NAME,
    }
    return info


# ================== Selenium åŸºç¡€ ==================
def get_driver(headless: bool = False):
    options = uc.ChromeOptions()
    if headless: options.add_argument("--headless=new")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("--start-maximized")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--lang=en-GB")
    options.add_argument("accept-language=en-GB,en-US;q=0.9,en;q=0.8")
    return uc.Chrome(options=options)

def fetch_product_info_with_single_driver(driver, url: str) -> Dict[str, Any]:
    driver.get(url)
    ok = wait_pdp_ready(driver, timeout=WAIT_HYDRATE_SECONDS)
    if not ok:
        html = driver.page_source or ""
        _dump_debug_html(html, url, tag="timeout_debug")
        return parse_info_new(html, url)
    _soft_scroll(driver, steps=6, pause=0.4)
    html = driver.page_source or ""
    _dump_debug_html(html, url, tag="debug_new")
    return parse_info_new(html, url)

# ================== å¤„ç†å•ä¸ª URL ==================
def process_url_with_driver(driver, url: str, conn: Connection, delay: float = DEFAULT_DELAY) -> Path | None:
    print(f"\nğŸŒ æ­£åœ¨æŠ“å–: {url}")
    info = fetch_product_info_with_single_driver(driver, url)

    # å…ˆæŸ¥ URLâ†’Code ç¼“å­˜ï¼ˆå‘½ä¸­åˆ™ä¸åšåŒ¹é…ï¼‰
    norm_url = _normalize_url(url)
    code = URL_CODE_CACHE.get(norm_url)

        # â€¦â€¦ï¼ˆå·²æœ‰åŒ¹é… code çš„é€»è¾‘åœ¨è¿™ä¹‹å‰ï¼‰
    # æ­¤æ—¶ code å˜é‡å·²ç»ç¡®å®šï¼Œinfo ä¹Ÿå·²ç»å¡«å¥½

    # â˜… gender å…œåº•ï¼šæœ‰ code æ‰å…œåº•ï¼›æ—  code åˆ™ä¿æŒ No Dataï¼ˆä¸å‘å¸ƒï¼‰
    if (not info.get("Product Gender")) or (info.get("Product Gender") == "No Data"):
        g_from_code = _infer_gender_from_code(code or info.get("Product Code"))
        if g_from_code != "No Data":
            info["Product Gender"] = g_from_code




    if code:
        print(f"ğŸ”— ç¼“å­˜å‘½ä¸­ URLâ†’{code}")
        info["Product Code"] = code
    else:
        # æ¨¡ç³ŠåŒ¹é…ï¼ˆæ²¿ç”¨æ—§é˜ˆå€¼/é€»è¾‘ï¼‰
        raw_conn = get_dbapi_connection(conn)
        title = info.get("Product Name") or ""
        color = info.get("Product Color") or ""
        results = match_product(
            raw_conn,
            scraped_title=title, scraped_color=color,
            table=PRODUCTS_TABLE,
            name_weight=NAME_WEIGHT, color_weight=COLOR_WEIGHT,
            type_weight=(1.0 - NAME_WEIGHT - COLOR_WEIGHT),
            topk=5, recall_limit=2000, min_name=0.92, min_color=0.85,
            require_color_exact=False, require_type=False,
        )
        code = choose_best(results, min_score=MIN_SCORE, min_lead=MIN_LEAD)
        print("ğŸ” match debug")
        print(f"  raw_title: {title}")
        print(f"  raw_color: {color}")
        txt, why = explain_results(results, min_score=MIN_SCORE, min_lead=MIN_LEAD)
        print(txt)
        if code:
            print(f"  â‡’ âœ… choose_best = {code}")
            info["Product Code"] = code
        else:
            print(f"  â‡’ âŒ no match ({why})")
            if results:
                top3 = " | ".join(f"{r['product_code']}[{r['score']:.3f}]" for r in results[:3])
                print("ğŸ§ª top:", top3)

    # è¾“å‡º TXT
    if code:
        out_path = TXT_DIR / f"{code}.txt"
    else:
        short = f"{abs(hash(url)) & 0xFFFFFFFF:08x}"
        safe_name = _safe_name(info.get("Product Name") or "BARBOUR")
        out_path = TXT_DIR / f"{safe_name}_{short}.txt"

    info["Product Gender"] = _gender_to_cn(info.get("Product Gender"))

    
    payload = _kv_txt_bytes(info)
    ok = _atomic_write_bytes(payload, out_path)
    if ok:
        print(f"âœ… å†™å…¥: {out_path} (code={info.get('Product Code')})")
    else:
        print(f"â— æ”¾å¼ƒå†™å…¥: {out_path.name}")

    if delay > 0:
        time.sleep(delay)
    return out_path

# ================== ä¸»å…¥å£ ==================
def houseoffraser_fetch_info(max_workers: int = MAX_WORKERS_DEFAULT, delay: float = DEFAULT_DELAY, headless: bool = False):
    links_file = Path(LINKS_FILE)
    if not links_file.exists():
        print(f"âš  æ‰¾ä¸åˆ°é“¾æ¥æ–‡ä»¶ï¼š{links_file}")
        return
    raw_urls = [line.strip() for line in links_file.read_text(encoding="utf-8", errors="ignore").splitlines()
                if line.strip() and not line.strip().startswith("#")]

    # è§„èŒƒåŒ–å»é‡ï¼ˆä¿åºï¼‰
    seen = set(); urls = []
    for u in raw_urls:
        nu = _normalize_url(u)
        if nu in seen: continue
        seen.add(nu); urls.append(u)

    total = len(urls)
    print(f"ğŸ“„ å…± {total} ä¸ªå•†å“é¡µé¢å¾…è§£æ...ï¼ˆå¹¶å‘ {max_workers}ï¼‰")
    if total == 0: return

    engine_url = f"postgresql+psycopg2://{PG['user']}:{PG['password']}@{PG['host']}:{PG['port']}/{PG['dbname']}"
    engine = create_engine(engine_url)

    # æ„å»º URLâ†’Code ç¼“å­˜
    with engine.begin() as conn:
        raw = get_dbapi_connection(conn)
        build_url_code_cache(raw, PRODUCTS_TABLE, OFFERS_TABLE, SITE_NAME)

    # å•å®ä¾‹æµè§ˆå™¨ï¼šé¦–ä¸ªå•†å“é¡µå…ˆç»™ä½ 10ç§’ç‚¹ Cookie
    driver = get_driver(headless=headless)
    try:
        if urls:
            print("ğŸ•’ å°†æ‰“å¼€é¦–ä¸ªå•†å“é¡µã€‚è¯·åœ¨ 10 ç§’å†…æ‰‹åŠ¨ç‚¹å‡» Cookie çš„ 'Allow all' æŒ‰é’®...")
            driver.get(urls[0])
            time.sleep(10)
            print("âœ… å·²ç­‰å¾… 10 ç§’ï¼Œå¼€å§‹æ­£å¼æŠ“å–")

        ok, fail = 0, 0
        with engine.begin() as conn:
            for idx, u in enumerate(urls, start=1):
                print(f"[å¯åŠ¨] [{idx}/{total}] {u}")
                try:
                    path = process_url_with_driver(driver, u, conn=conn, delay=delay)
                    ok += 1 if path else 0
                    print(f"[å®Œæˆ] [{idx}/{total}] {u} -> {path}")
                except Exception as e:
                    fail += 1
                    print(f"[å¤±è´¥] [{idx}/{total}] âŒ {u}\n    {repr(e)}")

        print(f"\nğŸ“¦ ä»»åŠ¡ç»“æŸï¼šæˆåŠŸ {ok}ï¼Œå¤±è´¥ {fail}ï¼Œæ€»è®¡ {total}")

    finally:
        try: driver.quit()
        except Exception: pass


if __name__ == "__main__":
    houseoffraser_fetch_info()
