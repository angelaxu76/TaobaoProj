import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from bs4 import BeautifulSoup
from pathlib import Path
import time
from config import BARBOUR
from pathlib import Path
import re
import json
from bs4 import BeautifulSoup

# === é…ç½®è·¯å¾„ ===
from config import BARBOUR

# âœ… é¡µé¢é…ç½®ï¼šä¸Šè¡£ç±»ï¼ˆç”· + å¥³ï¼‰
TARGET_URLS = [
    ("men", "https://www.outdoorandcountry.co.uk/shop-by-brand/barbour/barbour-menswear/all-barbour-mens-clothing-footwear.sub?s=i&pt=Coats+%26+Jackets%2cGilets+%26+Waistcoats%2cKnitwear"),
    ("women", "https://www.outdoorandcountry.co.uk/shop-by-brand/barbour/barbour-womenswear/womens-barbour-clothing-footwear-accessories.sub?s=i&pt=Coats+%26+Jackets%2cGilets+%26+Waistcoats%2cKnitwear"),
    ("international", "https://www.outdoorandcountry.co.uk/shop-by-brand/barbour-international/all-barbour-international.sub")
]

BASE_DOMAIN = "https://www.outdoorandcountry.co.uk"
OUTPUT_FILE = BARBOUR["LINKS_FILES"]["outdoorandcountry"]
DEBUG_DIR = OUTPUT_FILE.parent / "debug_pages"

SCROLL_STEP = 1200
SCROLL_PAUSE = 1.5
STABLE_THRESHOLD = 10



TXT_DIR = BARBOUR["TXT_DIRS"]["outdoorandcountry"]
LINKS_FILE = BARBOUR["LINKS_FILES"]["outdoorandcountry"]


# === æå–å‡½æ•° ===
def extract_js_object(js_text: str, var_name: str):
    pattern = re.compile(rf"window\.{re.escape(var_name)}\s*=\s*(\{{.*?\}});", re.DOTALL)
    match = pattern.search(js_text)
    if match:
        try:
            return json.loads(match.group(1))
        except Exception:
            print(f"âš ï¸ å˜é‡ {var_name} è§£æå¤±è´¥")
            return {}
    return {}

def parse_outdoor_product_page(html: str, url: str) -> list:
    soup = BeautifulSoup(html, "html.parser")

    # ä¿å­˜é¡µé¢è°ƒè¯•ï¼ˆå¯é€‰ï¼‰
    with open("debug.html", "w", encoding="utf-8") as f:
        f.write(html)

    # å•†å“åç§°
    title_tag = soup.find("title")
    if not title_tag:
        return []
    product_name = title_tag.text.strip()

    # URL ä¸­æå–é¢œè‰²
    color = "Unknown"
    if "?c=" in url:
        color = url.split("?c=")[-1].strip().replace("%20", " ").capitalize()

    # æå– JS å˜é‡
    js_text = soup.text
    colours = extract_js_object(js_text, "Colours")
    sizes = extract_js_object(js_text, "Sizes")
    stock_info = extract_js_object(js_text, "stockInfo")

    if not stock_info:
        print(f"âš ï¸ é¡µé¢æ—  stockInfo: {url}")
        return []

    results = []
    for k, v in stock_info.items():
        try:
            size_id, color_id = k.split("-")
            size = sizes.get(size_id, size_id)
            color_name = colours.get(color_id, color)
            stock_status = v.get("stockLevelMessage", "").lower()
            price = v.get("priceGbp", 0)

            results.append({
                "Product Name": product_name,
                "Product Color": color_name,
                "Product Size": size,
                "Product URL": url,
                "Stock Status": stock_status,
                "Price": f"{price:.2f}"
            })
        except Exception as e:
            print(f"âŒ è§£æå•é¡¹å¤±è´¥: {k} -> {e}")

    return results


# === å†™å…¥ TXT ===
def write_txt(filepath: Path, items: list):
    if not items:
        return
    main = items[0]
    lines = [
        f"Product Name: {main['Product Name']}",
        f"Product Color: {main['Product Color']}",
        f"Product URL: {main['Product URL']}",
        f"Product Size: " + "; ".join(f"{i['Product Size']}: {i['Stock Status']}" for i in items),
        f"Product Price: {main['Price']}"
    ]
    with open(filepath, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))


# === æ‰¹é‡å¤„ç†é€»è¾‘ ===
def outdoorandcountry_fetch_info():
    TXT_DIR.mkdir(parents=True, exist_ok=True)
    urls = set()
    with open(LINKS_FILE, "r", encoding="utf-8") as f:
        for line in f:
            url = line.strip()
            if url:
                urls.add(url)

    # âœ… ä½¿ç”¨ä¸æŠ“é“¾æ¥å®Œå…¨ä¸€æ ·çš„ uc.Chrome æ–¹å¼
    options = uc.ChromeOptions()
    options.add_argument("--start-maximized")
    driver = uc.Chrome(options=options)  # âŒ ä¸åŠ  headless

    for url in sorted(urls):
        try:
            print(f"\nğŸŒ æ‰“å¼€å•†å“è¯¦æƒ…é¡µ: {url}")
            driver.get(url)
            accept_cookies(driver)
            time.sleep(3)  # âœ… ç­‰å¾… JS æ¸²æŸ“

            html = driver.page_source

            # âœ… å¯é€‰ï¼šè°ƒè¯•é¡µé¢æˆªå›¾å’Œ HTML
            with open("debug_product.html", "w", encoding="utf-8") as f:
                f.write(html)
            driver.save_screenshot("debug_product.png")

            items = parse_outdoor_product_page(html, url)
            if items:
                product_name = items[0]['Product Name']
                color = items[0]['Product Color']
                filename = f"{product_name.replace(' ', '_')}_{color}.txt"
                filepath = TXT_DIR / filename
                write_txt(filepath, items)
                print(f"âœ… å†™å…¥: {filepath.name}")
            else:
                print(f"âš ï¸ è·³è¿‡ï¼ˆæ— åº“å­˜ä¿¡æ¯ï¼‰: {url}")
        except Exception as e:
            print(f"âŒ é”™è¯¯: {url}\n    {e}")

    driver.quit()


def accept_cookies(driver, timeout=8):
    try:
        button = WebDriverWait(driver, timeout).until(
            EC.element_to_be_clickable((By.ID, "onetrust-accept-btn-handler"))
        )
        button.click()
        print("ğŸª å·²è‡ªåŠ¨ç‚¹å‡» Accept Cookies")
        time.sleep(1)
    except:
        print("âš ï¸ æœªå‡ºç° Cookie æ¥å—æŒ‰é’®ï¼Œå¯èƒ½å·²æ¥å—æˆ–è¢«è·³è¿‡")

def scroll_like_mouse_until_loaded(driver, step=SCROLL_STEP, pause=SCROLL_PAUSE, stable_threshold=STABLE_THRESHOLD):
    print("âš¡ å¼€å§‹åŠ é€Ÿæ»šåŠ¨ç›´åˆ°å•†å“å…¨éƒ¨åŠ è½½...")
    actions = ActionChains(driver)

    last_count = 0
    stable_count = 0
    total_scrolls = 0

    while True:
        actions.scroll_by_amount(0, step).perform()
        time.sleep(pause)

        html = driver.page_source
        soup = BeautifulSoup(html, "html.parser")
        current_count = len(soup.select("a.image"))
        print(f"ğŸŒ€ æ»šåŠ¨ {total_scrolls+1} æ¬¡åï¼Œå•†å“æ•°: {current_count}")

        if current_count == last_count:
            stable_count += 1
        else:
            stable_count = 0
            last_count = current_count

        if stable_count >= stable_threshold:
            print(f"âœ… å•†å“æ•°é‡ç¨³å®šï¼ˆ{current_count}ï¼‰ï¼Œåœæ­¢æ»šåŠ¨")
            break

        total_scrolls += 1

def collect_links_from_html(html):
    soup = BeautifulSoup(html, "html.parser")
    links = set()
    for a in soup.select("a.image"):
        href = a.get("href", "").strip()
        if href:
            full_url = href if href.startswith("http") else BASE_DOMAIN + href
            links.add(full_url)
    return links

def outdoorandcountry_fetch_and_save_links():
    DEBUG_DIR.mkdir(parents=True, exist_ok=True)
    OUTPUT_FILE.parent.mkdir(parents=True, exist_ok=True)

    options = uc.ChromeOptions()
    options.add_argument("--start-maximized")
    driver = uc.Chrome(options=options, headless=False)

    all_links = set()

    for label, url in TARGET_URLS:
        print(f"\nğŸš€ æ‰“å¼€é¡µé¢ [{label}]: {url}")
        driver.get(url)
        time.sleep(3)

        accept_cookies(driver)
        scroll_like_mouse_until_loaded(driver)

        print(f"ğŸŸ¡ è¯·æ£€æŸ¥é¡µé¢ [{label}] æ˜¯å¦åŠ è½½å®Œæ•´ï¼Œå¯æ‰‹åŠ¨å†æ»šåŠ¨å‡ æ¬¡")
        input(f"â¸ï¸ ç¡®è®¤ [{label}] é¡µé¢åŠ è½½å®Œæˆåï¼ŒæŒ‰å›è½¦ç»§ç»­ >>> ")

        html = driver.page_source
        links = collect_links_from_html(html)
        all_links.update(links)

        debug_file = DEBUG_DIR / f"debug_{label}_auto_scroll.html"
        with debug_file.open("w", encoding="utf-8") as f:
            f.write(html)
        print(f"âœ… [{label}] é“¾æ¥æå–: {len(links)} æ¡ï¼Œé¡µé¢å¿«ç…§ä¿å­˜: {debug_file}")

    # ä¿å­˜æ€»é“¾æ¥
    with OUTPUT_FILE.open("w", encoding="utf-8") as f:
        for link in sorted(all_links):
            f.write(link + "\n")

    print(f"\nğŸ‰ å…±æå–å•†å“é“¾æ¥: {len(all_links)} æ¡")
    print(f"ğŸ“„ é“¾æ¥å†™å…¥: {OUTPUT_FILE}")
    driver.quit()
