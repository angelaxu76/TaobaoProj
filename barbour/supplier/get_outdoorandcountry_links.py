import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.common.action_chains import ActionChains
from bs4 import BeautifulSoup
from pathlib import Path
import time
from config import BARBOUR

# âœ… é¡µé¢ä¸æ–‡ä»¶é…ç½®
TARGET_URL = "https://www.outdoorandcountry.co.uk/shop-by-brand/barbour/barbour-menswear/all-barbour-mens-clothing-footwear.sub?s=i&pt=Coats+%26+Jackets%2cGilets+%26+Waistcoats%2cKnitwear"

BASE_DOMAIN = "https://www.outdoorandcountry.co.uk"
OUTPUT_FILE = BARBOUR["LINKS_FILES"]["outdoorandcountry"]
DEBUG_DIR = OUTPUT_FILE.parent / "debug_pages"
DEBUG_FILE = DEBUG_DIR / "debug_auto_scroll_uc_final.html"

# âœ… è‡ªåŠ¨æ»šåŠ¨å‚æ•°
SCROLL_STEP = 1200          # æ¯æ¬¡æ»šåŠ¨è·ç¦»ï¼ˆåƒç´ ï¼‰
SCROLL_PAUSE = 1.5          # æ¯æ¬¡æ»šåŠ¨ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
STABLE_THRESHOLD = 20        # è¿ç»­ N æ¬¡å•†å“æ•°ä¸å˜æ‰ç»“æŸæ»šåŠ¨

# âœ… æ»šåŠ¨å¹¶æ£€æµ‹æ˜¯å¦åŠ è½½å®Œæˆ
def scroll_like_mouse_until_loaded(driver, step=SCROLL_STEP, pause=SCROLL_PAUSE, stable_threshold=STABLE_THRESHOLD):
    print("âš¡ å¼€å§‹åŠ é€Ÿæ»šåŠ¨ç›´åˆ°å•†å“å…¨éƒ¨åŠ è½½...")
    actions = ActionChains(driver)

    last_count = 0
    stable_count = 0
    total_scrolls = 0

    while True:
        actions.scroll_by_amount(0, step).perform()
        time.sleep(pause)

        html = driver.page_source
        soup = BeautifulSoup(html, "html.parser")
        current_count = len(soup.select("a.image"))
        print(f"ğŸŒ€ æ»šåŠ¨ {total_scrolls+1} æ¬¡åï¼Œå•†å“æ•°: {current_count}")

        if current_count == last_count:
            stable_count += 1
        else:
            stable_count = 0
            last_count = current_count

        if stable_count >= stable_threshold:
            print(f"âœ… å•†å“æ•°é‡ç¨³å®šï¼ˆ{current_count}ï¼‰ï¼Œåœæ­¢æ»šåŠ¨")
            break

        total_scrolls += 1

# âœ… æå–å•†å“é“¾æ¥
def collect_links_from_html(html):
    soup = BeautifulSoup(html, "html.parser")
    links = set()
    for a in soup.select("a.image"):
        href = a.get("href", "").strip()
        if href:
            full_url = href if href.startswith("http") else BASE_DOMAIN + href
            links.add(full_url)
    return links

# âœ… ä¸»æµç¨‹
def outdoorandcountry_fetch_and_save_links():
    DEBUG_DIR.mkdir(parents=True, exist_ok=True)
    OUTPUT_FILE.parent.mkdir(parents=True, exist_ok=True)

    options = uc.ChromeOptions()
    options.add_argument("--start-maximized")

    driver = uc.Chrome(options=options, headless=False)
    print(f"ğŸš€ æ­£åœ¨æ‰“å¼€é¡µé¢: {TARGET_URL}")
    driver.get(TARGET_URL)
    time.sleep(5)

    scroll_like_mouse_until_loaded(driver)

    # ç­‰ä½ ç¡®è®¤é¡µé¢åŠ è½½å®Œåç»§ç»­
    print("\nğŸŸ¡ é¡µé¢è‡ªåŠ¨æ»šåŠ¨å®Œæˆï¼Œå¦‚æœä½ è¿˜æƒ³äººå·¥æ»šå‡ ä¸‹ï¼Œè¯·å®ŒæˆåæŒ‰å›è½¦ç»§ç»­")
    input("â¸ï¸ ç¡®è®¤é¡µé¢åŠ è½½å®Œæˆåè¯·æŒ‰å›è½¦ç»§ç»­ >>> ")

    html = driver.page_source
    links = collect_links_from_html(html)

    # å†™å…¥é“¾æ¥
    with OUTPUT_FILE.open("w", encoding="utf-8") as f:
        for link in sorted(links):
            f.write(link + "\n")

    # ä¿å­˜é¡µé¢å¿«ç…§
    with DEBUG_FILE.open("w", encoding="utf-8") as f:
        f.write(html)

    print(f"\nâœ… å…±æå–å•†å“é“¾æ¥: {len(links)} æ¡")
    print(f"ğŸ“„ é“¾æ¥ä¿å­˜è‡³: {OUTPUT_FILE}")
    print(f"ğŸ“ é¡µé¢å¿«ç…§ä¿å­˜è‡³: {DEBUG_FILE}")
    driver.quit()


