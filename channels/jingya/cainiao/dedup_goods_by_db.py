# -*- coding: utf-8 -*-
"""
æŒ‰æ•°æ®åº“ channel_item_id åˆ¤å®šï¼š
åˆ é™¤é²¸èŠ½â€œè´§å“å¯¼å‡º*.xlsxâ€ä¸­é‡å¤çš„ã€è´§å“åç§°ã€‘è¡Œï¼Œä»…ä¿ç•™ä¸DBåŒ¹é…çš„ä¸€æ¡ã€‚

ä¾èµ–ï¼špandas, sqlalchemy, psycopg2, openpyxl
ä» config.PGSQL_CONFIG è¯»å–è¿æ¥ï¼ˆæ”¯æŒé”®åï¼šhost/port/user/password/**dbname**ï¼‰ã€‚
"""

from pathlib import Path
from datetime import datetime
import pandas as pd
from sqlalchemy import create_engine, text

# ======================= âœ…ã€å‚æ•°é…ç½®åŒºã€‘=======================
GOODS_DIR = Path(r"D:\TB\taofenxiao\goods")
INPUT_PREFIX = "è´§å“å¯¼å‡º"
INPUT_SUFFIX = ".xlsx"

DB_TABLE = "clarks_jingya_inventory"
DB_CHANNEL_FIELD = "channel_item_id"

COL_NAME = "è´§å“åç§°"
COL_ID = "è´§å“ID"

# 0 å‘½ä¸­æ—¶çš„å¤„ç†ç­–ç•¥ï¼š "keep_first"ï¼ˆä¿ç•™ç¬¬ä¸€æ¡ï¼‰| "drop_all"ï¼ˆå…¨éƒ¨åˆ é™¤ï¼‰
ON_ZERO_MATCH = "keep_first"
# ============================================================

# è¯»å– DB é…ç½®
from config import PGSQL_CONFIG  # å¿…é¡»åŒ…å« host/port/user/password/dbname

def _pg_url(cfg: dict) -> str:
    """æ„å»º SQLAlchemy è¿æ¥ä¸²ï¼›å…¼å®¹ dbname é”®ã€‚"""
    host = cfg.get("host") or cfg.get("HOST")
    port = cfg.get("port") or cfg.get("PORT", 5432)
    user = cfg.get("user") or cfg.get("USER")
    password = cfg.get("password") or cfg.get("PASSWORD")
    # âš  ä½ çš„ config ç”¨çš„æ˜¯ "dbname"
    database = cfg.get("database") or cfg.get("dbname") or cfg.get("DB") or cfg.get("DATABASE")
    if not all([host, port, user, password, database]):
        raise ValueError("PGSQL_CONFIG ç¼ºå°‘å¿…è¦å­—æ®µï¼ˆhost/port/user/password/dbnameï¼‰ã€‚")
    return f"postgresql+psycopg2://{user}:{password}@{host}:{port}/{database}"

def _find_latest(goods_dir: Path, prefix: str, suffix: str) -> Path:
    files = [p for p in goods_dir.glob(f"{prefix}*{suffix}") if p.is_file()]
    if not files:
        raise FileNotFoundError(f"æœªåœ¨ {goods_dir} æ‰¾åˆ°æ–‡ä»¶ï¼š{prefix}*{suffix}")
    files.sort(key=lambda p: p.stat().st_mtime, reverse=True)
    return files[0]

def _load_channel_id_set(engine, table: str, field: str) -> set:
    sql = text(f"""
        SELECT DISTINCT {field}
        FROM {table}
        WHERE {field} IS NOT NULL AND {field} <> ''
    """)
    with engine.begin() as conn:
        rows = conn.execute(sql).fetchall()
    return {str(r[0]).strip() for r in rows if r and r[0] is not None}

def _norm(s: pd.Series) -> pd.Series:
    return s.astype(str).fillna("").map(lambda x: x.strip())

def dedup_jingya_goods_by_db(
    goods_dir: Path = GOODS_DIR,
    input_prefix: str = INPUT_PREFIX,
    input_suffix: str = INPUT_SUFFIX,
    db_table: str = DB_TABLE,
    db_channel_field: str = DB_CHANNEL_FIELD,
    col_name: str = COL_NAME,
    col_id: str = COL_ID,
    on_zero_match: str = ON_ZERO_MATCH,
) -> tuple[Path, Path]:
    """
    è¿”å› (å»é‡åExcelè·¯å¾„, åˆ é™¤æ¸…å•Excelè·¯å¾„)
    """
    # 1) æ‰¾è¾“å…¥
    src = _find_latest(goods_dir, input_prefix, input_suffix)

    # 2) è¯» Excel
    df = pd.read_excel(src, dtype=str)
    if col_name not in df.columns or col_id not in df.columns:
        raise KeyError(f"Excelç¼ºå°‘å¿…è¦åˆ—ï¼š{col_name} / {col_id}")

    # ä¿ç•™åŸå§‹é¡ºåºå¹¶è§„èŒƒåŒ–
    df["_orig_idx"] = range(len(df))
    df["_name_norm"] = _norm(df[col_name])
    df["_id_norm"] = _norm(df[col_id])

    # 3) è¯» DB ä¸­çš„ channel_item_id é›†åˆ
    engine = create_engine(_pg_url(PGSQL_CONFIG), future=True)
    channel_ids = _load_channel_id_set(engine, db_table, db_channel_field)

    # 4) åˆ†ç»„å¤„ç†
    keep_idx = []
    removed_rows = []

    grouped = df.groupby("_name_norm", sort=False)
    for name_val, group in grouped:
        if len(group) == 1:
            keep_idx.extend(group.index.tolist())
            continue

        g = group.sort_values("_orig_idx", ascending=True)
        in_db_mask = g["_id_norm"].map(lambda x: x in channel_ids)
        matched = g[in_db_mask]

        if len(matched) == 1:
            # âœ… å”¯ä¸€å‘½ä¸­ï¼šä¿ç•™å®ƒï¼Œå…¶ä½™åˆ é™¤
            keep = matched.index[0]
            keep_idx.append(keep)
            for i in g.index.difference([keep]):
                removed_rows.append({**df.loc[i].to_dict(), "åˆ é™¤åŸå› ": "åŒåé‡å¤-éç³»ç»Ÿè®°å½•"})
        elif len(matched) > 1:
            # å¤šå‘½ä¸­ï¼šä¿ç•™é¦–æ¡ï¼Œå…¶ä½™åˆ é™¤
            keep = matched.sort_values("_orig_idx").index[0]
            keep_idx.append(keep)
            for i in g.index.difference([keep]):
                removed_rows.append({**df.loc[i].to_dict(), "åˆ é™¤åŸå› ": "åŒåé‡å¤-å¤šå‘½ä¸­-å·²ä¿ç•™é¦–æ¡"})
        else:
            # 0 å‘½ä¸­ï¼šæŒ‰ç­–ç•¥
            if on_zero_match == "keep_first":
                keep = g.index[0]
                keep_idx.append(keep)
                for i in g.index.difference([keep]):
                    removed_rows.append({**df.loc[i].to_dict(), "åˆ é™¤åŸå› ": "åŒåé‡å¤-0å‘½ä¸­-ä¿ç•™é¦–æ¡"})
            elif on_zero_match == "drop_all":
                for i in g.index:
                    removed_rows.append({**df.loc[i].to_dict(), "åˆ é™¤åŸå› ": "åŒåé‡å¤-0å‘½ä¸­-å…¨éƒ¨åˆ é™¤"})
            else:
                raise ValueError(f"on_zero_match ä¸æ”¯æŒ: {on_zero_match}")

    # 5) è¾“å‡º
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    out_dir = goods_dir
    out_dir.mkdir(parents=True, exist_ok=True)

    kept_df = df.loc[sorted(set(keep_idx))].drop(columns=["_orig_idx", "_name_norm", "_id_norm"], errors="ignore")
    removed_df = pd.DataFrame(removed_rows) if removed_rows else pd.DataFrame(columns=list(df.columns) + ["åˆ é™¤åŸå› "])

    out_ok = out_dir / f"è´§å“å¯¼å‡º_dedup_by_db_{ts}.xlsx"
    out_rm = out_dir / f"è´§å“å¯¼å‡º_deleted_rows_{ts}.xlsx"

    kept_df.to_excel(out_ok, index=False)
    removed_df.to_excel(out_rm, index=False)

    print(f"âœ… è¾“å…¥ï¼š{src}")
    print(f"ğŸ“¦ åŸå§‹è¡Œæ•°ï¼š{len(df)}")
    print(f"ğŸ§¹ åˆ é™¤è¡Œæ•°ï¼š{len(removed_df)}")
    print(f"âœ… ä¿ç•™è¡Œæ•°ï¼š{len(kept_df)}")
    print(f"ğŸ’¾ è¾“å‡ºï¼š{out_ok}")
    print(f"ğŸ“ åˆ é™¤æ¸…å•ï¼š{out_rm}")

    return out_ok, out_rm

def main():
    dedup_jingya_goods_by_db()

if __name__ == "__main__":
    main()
