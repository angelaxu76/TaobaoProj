# -*- coding: utf-8 -*-
"""
é€ SKU å¯¼å‡ºåº—é“ºä»·æ ¼ï¼ˆæ”¯æŒæ‰¹é‡ï¼‰
- å•æ–‡ä»¶ï¼šä¿æŒ generate_price_excel(brand, input_dir, output_path) åŸè¡Œä¸ºä¸å˜ï¼ˆå– input_dir å†…â€œæœ€è¿‘ä¿®æ”¹â€çš„ä¸€ä¸ªExcelï¼‰
- æ‰¹é‡ï¼šæ–°å¢ generate_price_excels_bulk(brand, input_dir, output_dir, suffix)
  * å¤„ç† input_dir ä¸‹æ‰€æœ‰ *.xlsx / *.xls
  * è¾“å‡ºæ–‡ä»¶å = è¾“å…¥æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰ + suffix + ".xlsx"
  * æ¯ä¸ªæ–‡ä»¶ç‹¬ç«‹æŸ¥ä»·ã€ç‹¬ç«‹å¯¼å‡ºï¼Œå•ä¸ªå¤±è´¥ä¸å½±å“å…¶å®ƒæ–‡ä»¶
- åˆ—è¯†åˆ«ï¼šå®è´ID(=item_id)ã€å•†å®¶ç¼–ç (=product_code)ã€skuID(=skuid)
- ä»·æ ¼æ¥æºï¼šå“ç‰Œè¡¨ä¸­çš„ taobao_store_priceï¼ˆä¼˜å…ˆ product_codeï¼›å…œåº• product_nameï¼‰
- è¾“å‡ºä¸‰åˆ—ï¼šå®è´id | skuid | è°ƒæ•´åä»·æ ¼
"""
from pathlib import Path
import unicodedata
import re
from typing import List, Dict, Optional, Iterable, Tuple
import pandas as pd
import psycopg2
import math

from config import BRAND_CONFIG

# ===== åˆ—ååˆ«å =====
COL_ALIASES = {
    "item_id": {"item_id","itemid","ITEM_ID","å®è´id","å®è´ID","å®è´Id","å®è´"},
    "product_code": {
        "product_code","productcode","code",
        "å•†å“ç¼–ç ","å•†å“Code","äº§å“ç¼–ç ","ç¼–ç ","è´§å·",
        "å•†å®¶ç¼–ç ",
        "å•†å®¶è´§å·","å¤–éƒ¨ç¼–ç ","å¤–éƒ¨ä»£ç ",
        "outer_id","outerid","outer code","outercode"
    },
    "skuid": {
        "skuid","sku_id","SkuId","SKU_ID","SKUID",
        "skuID",
        "æ¸ é“è´§å“ID","æ¸ é“skuid","è´§å“id","è´§å“ID"
    },
}

_SPLIT = re.compile(r"[,\uFF0C;ï¼›\s\r\n]+")


import pandas as pd

import pandas as pd
from pathlib import Path

def _load_blacklist_codes(blacklist_excel_file: str | None) -> set[str]:
    """
    ä» blacklist_excel_file è¯»å–é»‘åå•å•†å“ç¼–ç åˆ—è¡¨ã€‚
    å¦‚æœ blacklist_excel_file æ˜¯ None æˆ–æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ™è¿”å›ç©ºé›†åˆï¼Œè¡¨ç¤ºä¸å¯ç”¨é»‘åå•ã€‚
    """
    if not blacklist_excel_file:
        # æ²¡ä¼ ï¼Œç­‰äºä¸å¯ç”¨é»‘åå•
        print("[BLACKLIST] æœªæä¾›é»‘åå•æ–‡ä»¶ï¼Œé»‘åå•ä¸ºç©º")
        return set()

    path_obj = Path(blacklist_excel_file)
    if not path_obj.exists():
        print(f"[BLACKLIST] é»‘åå•æ–‡ä»¶ä¸å­˜åœ¨: {blacklist_excel_file}ï¼Œé»‘åå•ä¸ºç©º")
        return set()

    df_blk = pd.read_excel(path_obj)

    candidate_cols = [
        "å•†å“ç¼–ç ",
        "product_code",
        "å•†å®¶ç¼–ç ",
        "å¤–éƒ¨å•†å“ç¼–ç ",
        "å•†å®¶è´§å·",
        "è´§å·",
        "ç¼–ç ",
    ]

    col_found = None
    for c in candidate_cols:
        if c in df_blk.columns:
            col_found = c
            break

    if col_found is None:
        print("[BLACKLIST] æœªæ‰¾åˆ°å¯è¯†åˆ«çš„é»‘åå•åˆ—ï¼Œé»‘åå•ä¸ºç©º")
        return set()

    codes = (
        df_blk[col_found]
        .astype(str)
        .str.strip()
        .replace({"": pd.NA})
        .dropna()
        .unique()
        .tolist()
    )

    blacklist = {c.upper() for c in codes}
    print(f"[BLACKLIST] è¯»å–åˆ° {len(blacklist)} ä¸ªé»‘åå•å•†å“ç¼–ç ")
    return blacklist

import logging
from pathlib import Path

def _get_logger(name: str = "price_export"):
    logger = logging.getLogger(name)
    if not logger.handlers:
        logger.setLevel(logging.INFO)
        h = logging.StreamHandler()
        fmt = logging.Formatter("[%(levelname)s] %(message)s")
        h.setFormatter(fmt)
        logger.addHandler(h)
    return logger

from datetime import datetime
import pandas as pd
from pathlib import Path

def _flush_filtered(output_dir: Path, input_file: str, logger=None):
    global filtered_rows
    if not filtered_rows:
        return
    temp_dir = Path(output_dir) / "temp"
    temp_dir.mkdir(parents=True, exist_ok=True)

    stem = Path(input_file).stem
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    out_xlsx = temp_dir / f"{stem}_filtered_reasons_{ts}.xlsx"

    df_log = pd.DataFrame(filtered_rows)
    if "filter_reason" in df_log.columns:
        df_log = df_log.sort_values(["filter_reason"])

    with pd.ExcelWriter(out_xlsx, engine="openpyxl") as w:
        df_log.to_excel(w, index=False, sheet_name="filtered_rows")
        summary = df_log.groupby("filter_reason").size().reset_index(name="count")
        summary.to_excel(w, index=False, sheet_name="summary")

    if logger:
        logger.info(f"è¿‡æ»¤åŸå› æ˜ç»†å·²å†™å…¥: {out_xlsx}")
    else:
        print(f"[LOG] è¿‡æ»¤åŸå› æ˜ç»†å·²å†™å…¥: {out_xlsx}")



import psycopg2
from psycopg2.extras import execute_values

from psycopg2.extras import execute_values

def _fetch_prices_by_code_and_size_bulk(
    conn,
    table: str,
    code_size_pairs: list[tuple[str, str]]
) -> pd.DataFrame:
    """
    æ‰¹é‡ä» {table} é‡Œæ‹¿ (product_code, size) å¯¹åº”çš„ taobao_store_priceã€‚
    å…³é”®ç‚¹ï¼š
    - ä¸å†ä½¿ç”¨ execute_valuesï¼ˆä¹‹å‰åªæ‹¿åˆ°æœ€åä¸€æ‰¹ï¼Œå¯¼è‡´ä¸¥é‡ä¸¢ä»·ï¼‰
    - æ‰‹åŠ¨åˆ†æ‰¹ï¼Œæ¯æ‰¹ç”¨ WITH wanted AS (VALUES ...) JOINï¼Œé€æ‰¹æ‰§è¡Œ cur.execute()
    - æ±‡æ€»æ‰€æœ‰æ‰¹æ¬¡è¿”å›ï¼Œå»é‡åè¾“å‡º DataFrame
    """

    # 1. å…ˆæŠŠè¾“å…¥è§„æ ¼æ¸…æ´—æˆç¨³å®šå­—ç¬¦ä¸²é”®ï¼Œé˜²æ­¢ '16 ' / None ç­‰è„å€¼
    cleaned_pairs = []
    for code, sz in code_size_pairs:
        if code is None or sz is None:
            continue
        c = str(code).strip()
        s = str(sz).strip()
        if c and s:
            cleaned_pairs.append((c, s))

    # å»é‡ï¼Œé¿å…åŒä¸€ä¸ª (code,size) é‡å¤æŸ¥
    unique_pairs = sorted(set(cleaned_pairs))
    if not unique_pairs:
        return pd.DataFrame(columns=["product_code", "size", "taobao_store_price"])

    print(f"[PRICEDEBUG] å¾…æŸ¥è¯¢ç»„åˆæ€»æ•°: {len(cleaned_pairs)}ï¼Œå»é‡å: {len(unique_pairs)}")

    # 2. åˆ†æ‰¹ï¼Œæ¯æ‰¹ 50 ä¸ª (code,size)
    CHUNK_SIZE = 50
    all_rows: list[tuple[str, str, float]] = []

    with conn.cursor() as cur:
        total_batches = (len(unique_pairs) + CHUNK_SIZE - 1) // CHUNK_SIZE

        for batch_idx, start in enumerate(range(0, len(unique_pairs), CHUNK_SIZE), start=1):
            batch = unique_pairs[start:start + CHUNK_SIZE]

            # ç”¨ mogrify æŠŠæ¯æ¡ (code,size) å˜æˆå®‰å…¨çš„ SQL ('CODE','SIZE') ç‰‡æ®µ
            values_sql_list = [
                cur.mogrify("(%s,%s)", (code, size)).decode("utf-8")
                for code, size in batch
            ]
            values_sql_block = ",".join(values_sql_list)

            # æ„é€ æœ¬æ‰¹ SQL
            query_sql = (
                f"WITH wanted(code,size) AS (VALUES {values_sql_block}) "
                f"SELECT t.product_code, t.size, t.taobao_store_price "
                f"FROM {table} t "
                "JOIN wanted w ON t.product_code = w.code AND t.size = w.size"
            )

            # Debug: æ‰“å°æœ¬æ‰¹çš„å…³é”®ä¿¡æ¯ï¼ˆä¸ä¼šå¤ªåµï¼‰
            print("\n[DEBUG SQL BATCH]")
            print(f"  æ‰¹æ¬¡ {batch_idx}/{total_batches}, æœ¬æ‰¹ç»„åˆæ•°: {len(batch)}")
            print(f"  ç¤ºä¾‹VALUESå‰5ä¸ª: {', '.join(values_sql_list[:5])}")

            # çœŸæ­£æ‰§è¡Œ
            cur.execute(query_sql)
            batch_rows = cur.fetchall()
            print(f"  -> æœ¬æ‰¹è¿”å› {len(batch_rows)} è¡Œ")

            all_rows.extend(batch_rows)

    print(f"[PRICEDEBUG] å…¨éƒ¨åˆ†æ‰¹åˆè®¡è¿”å› {len(all_rows)} è¡Œ")

    # 3. æ±‡æ€»å¹¶å»é‡ (é˜²æ­¢åŒä¸€ä¸ª (product_code,size) å¤šæ¬¡å‡ºç°)
    merged_unique = {}
    for (product_code, size, price_val) in all_rows:
        key = (str(product_code).strip(), str(size).strip())
        if key not in merged_unique:
            merged_unique[key] = price_val  # ç¬¬ä¸€æ¡ä¸ºå‡†

    rows_for_df = [
        {
            "product_code": pc,
            "size": sz,
            "taobao_store_price": price_val,
        }
        for (pc, sz), price_val in merged_unique.items()
    ]

    df_price = pd.DataFrame(
        rows_for_df,
        columns=["product_code", "size", "taobao_store_price"]
    )

    print(f"[PRICEDEBUG] å»é‡åæœ€ç»ˆä»·æ ¼è¡Œæ•°: {len(df_price)}")
    return df_price




def _canon(s: str) -> str:
    if s is None: return ""
    s = unicodedata.normalize("NFKC", str(s)).strip()
    s = s.replace("\u00A0", " ").replace("\u200B", "")
    return s.lower()

def _normalize_col(df: pd.DataFrame, want: str) -> str:
    canon2raw = {_canon(c): c for c in df.columns}
    for alias in COL_ALIASES[want]:
        key = _canon(alias)
        if key in canon2raw:
            return canon2raw[key]
    raise KeyError(f"Excelä¸­ç¼ºå°‘å¿…è¦åˆ—ï¼š{want}ï¼ˆå¯ç”¨åˆ«åï¼š{COL_ALIASES[want]}ï¼‰ï¼Œå½“å‰è¡¨å¤´ï¼š{list(df.columns)}")

def _list_excels(input_dir: Path) -> List[Path]:
    files = list(input_dir.glob("*.xlsx")) + list(input_dir.glob("*.xls"))
    return sorted(files, key=lambda p: p.stat().st_mtime, reverse=True)

def _find_latest_excel(input_dir: Path) -> Path:
    files = _list_excels(input_dir)
    if not files:
        raise FileNotFoundError(f"ç›®å½•æ²¡æœ‰æ‰¾åˆ° Excelï¼š{input_dir}")
    return files[0]

def _split_skuids(val) -> List[str]:
    if pd.isna(val): return []
    if isinstance(val, (int, float)) and not isinstance(val, bool):
        val = str(int(val))
    s = str(val).strip()
    if not s: return []
    parts = [p.strip() for p in _SPLIT.split(s) if p.strip()]
    parts = [re.sub(r"[^\w\-]", "", p) for p in parts if p]
    return [p for p in parts if p]

def _chunked(it: Iterable, size: int):
    buf = []
    for x in it:
        buf.append(x)
        if len(buf) >= size:
            yield buf; buf = []
    if buf: yield buf

def _fetch_prices(conn, table: str, codes: List[str]) -> Dict[str, Optional[float]]:
    prices: Dict[str, Optional[float]] = {}
    missing = list(dict.fromkeys(codes))
    with conn.cursor() as cur:
        try:
            for chunk in _chunked(missing, 1000):
                cur.execute(
                    f"SELECT product_code, taobao_store_price FROM {table} "
                    f"WHERE product_code = ANY(%s)", (chunk,))
                for code, price in cur.fetchall():
                    prices[str(code)] = None if price is None else float(price)
            missing = [c for c in missing if c not in prices]
        except Exception:
            pass
        if missing:
            try:
                for chunk in _chunked(missing, 1000):
                    cur.execute(
                        f"SELECT product_name, taobao_store_price FROM {table} "
                        f"WHERE product_name = ANY(%s)", (chunk,))
                    for code, price in cur.fetchall():
                        prices[str(code)] = None if price is None else float(price)
            except Exception:
                pass
    return prices

def _to_text(v: object) -> str:
    if v is None or (isinstance(v, float) and (math.isnan(v))):
        return ""
    if isinstance(v, (int,)):
        return str(v)
    if isinstance(v, float):
        if v.is_integer():
            return str(int(v))
        return f"{v:.15g}"
    if isinstance(v, pd._libs.missing.NAType) or (isinstance(v, str) and v.lower() == "nan"):
        return ""
    return str(v)

# ===== å•æ–‡ä»¶ï¼šä¿æŒå…¼å®¹ =====
def generate_price_excel(
    brand: str,
    input_dir: str | Path,
    output_path: str | Path,
    drop_rows_without_price: bool = True
) -> Path:
    brand = brand.lower().strip()
    if brand not in BRAND_CONFIG:
        raise ValueError(f"æœªçŸ¥å“ç‰Œï¼š{brand}")
    cfg = BRAND_CONFIG[brand]
    table = cfg["TABLE_NAME"]
    pg    = cfg["PGSQL_CONFIG"]

    input_dir = Path(input_dir)
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    excel_file = _find_latest_excel(input_dir)
    print(f"ğŸ“„ ä½¿ç”¨è¾“å…¥æ–‡ä»¶ï¼š{excel_file}")
    return _generate_price_excel_from_file(brand, excel_file, output_path, drop_rows_without_price, table, pg)

# ===== æ‰¹é‡ï¼šå¤„ç† input_dir ä¸‹æ‰€æœ‰ Excel =====
def generate_price_excels_bulk(
    brand: str,
    input_dir: str,
    output_dir: str,
    suffix: str = "_ä»·æ ¼",
    drop_rows_without_price: bool = True,
    blacklist_excel_file: str | None = None,
):
    """
    ä» input_dir ä¸­æ‰¹é‡è¯»å–åº—é“º Excelï¼ˆæ¯ä¸ªä»£è¡¨ä¸€ä¸ªåº—é“ºï¼‰ï¼Œ
    è°ƒç”¨ _generate_price_excel_from_file() ç”Ÿæˆå¯¹åº”ä»·æ ¼è¡¨ï¼Œ
    å¹¶è¾“å‡ºåˆ° output_dirã€‚

    æ–°å¢å‚æ•°:
        blacklist_excel_file: é»‘åå• Excel çš„ç»å¯¹è·¯å¾„ã€‚
                              å¦‚æœæä¾›ï¼Œåˆ™ä¼šè¿‡æ»¤æ‰é»‘åå•å•†å“ç¼–ç ã€‚
                              å¦‚æœä¸º Noneï¼Œåˆ™ä¸å¯ç”¨é»‘åå•ã€‚
    """

    input_dir = Path(input_dir)
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # æ•°æ®åº“é…ç½®
    from config import BRAND_CONFIG
    pgsql_config = BRAND_CONFIG[brand]["PGSQL_CONFIG"]
    table_name = BRAND_CONFIG[brand]["TABLE_NAME"]

    # æ‰¾å‡ºç›®å½•ä¸‹æ‰€æœ‰ Excel æ–‡ä»¶
    excel_files = list(input_dir.glob("*.xlsx"))
    if not excel_files:
        print(f"âš  æ²¡æ‰¾åˆ°ä»»ä½• Excel æ–‡ä»¶: {input_dir}")
        return

    print(f"[INFO] å…±å‘ç° {len(excel_files)} ä¸ªè¾“å…¥æ–‡ä»¶ï¼Œå°†ç”Ÿæˆä»·æ ¼è¡¨...")
    print(f"[INFO] å“ç‰Œ: {brand}")
    print(f"[INFO] é»‘åå•æ–‡ä»¶: {blacklist_excel_file or 'æœªå¯ç”¨'}")

    for f in excel_files:
        try:
            print(f"\n[START] å¤„ç†æ–‡ä»¶: {f.name}")
            _generate_price_excel_from_file(
                file_path=str(f),
                output_dir=str(output_dir),
                brand=brand,
                pgsql_config=pgsql_config,
                blacklist_excel_file=blacklist_excel_file,
                table_name=table_name,
            )
        except Exception as e:
            print(f"[ERROR] å¤„ç† {f.name} å¤±è´¥: {e}")

    print(f"\nâœ… æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆã€‚è¾“å‡ºè·¯å¾„: {output_dir}")


def normalize_size_for_taobao(size_raw):
    """
    å°†æ·˜å®å¯¼å‡ºçš„å°ºç æ ¼å¼ç»Ÿä¸€æˆæ•°æ®åº“é‡Œçš„æ ¼å¼ï¼š
    - '38-39' -> '38'
    - '34ç '  -> '34'
    - å…¶ä»–ä¸å« 'ç 'ã€'-' çš„å€¼åŸæ ·è¿”å›ï¼ˆå¦‚ '39'ï¼‰
    """
    if size_raw is None:
        return None

    if not isinstance(size_raw, str):
        size_str = str(size_raw)
    else:
        size_str = size_raw

    size_str = size_str.strip()

    # 1) å»æ‰â€œç â€
    size_str = size_str.replace("ç ", "").strip()

    # 2) å¦‚æœæ˜¯ "38-39" è¿™ç§ï¼Œå–å‰åŠæ®µ
    if "-" in size_str:
        size_str = size_str.split("-", 1)[0].strip()

    # 3) ç©ºä¸²è§†ä¸º None
    return size_str or None



import pandas as pd
from pathlib import Path

def _normalize_input_columns(df: pd.DataFrame) -> pd.DataFrame:
    """
    æˆ‘ä»¬éœ€è¦æ‹¿åˆ°ä¸‰åˆ—ï¼š
    - item_id   (å®è´id)
    - skuid     (SKUå”¯ä¸€æ ‡è¯†ï¼Œæ¸ é“è´§å“ID)
    - sku_spec  (skuè§„æ ¼ï¼Œæ¯”å¦‚ 'MWX0339OL71,M')

    è¿™ä¸ªå‡½æ•°çš„ä½œç”¨æ˜¯ï¼šä»å„ç§å¯èƒ½çš„åˆ—åé‡ŒæŠ½å–å¹¶æ ‡å‡†åŒ–æˆè¿™ä¸‰åˆ—ã€‚
    """

    # å¯èƒ½çš„åˆ—åæ˜ å°„
    item_id_candidates = ["å®è´id", "å®è´ID", "item_id", "itemid", "itemId", "å•†å“ID", "item id"]
    skuid_candidates   = ["skuid", "SKU ID", "skuID", "SKUId", "æ¸ é“è´§å“ID", "æ¸ é“è´§å“id", "è´§å“id", "sku id"]
    spec_candidates    = ["skuè§„æ ¼", "SKUè§„æ ¼", "è§„æ ¼", "sku spec", "é”€å”®å±æ€§"]

    colmap = {}

    # æ‰¾ item_id
    for c in item_id_candidates:
        if c in df.columns:
            colmap["item_id"] = c
            break
    # æ‰¾ skuid
    for c in skuid_candidates:
        if c in df.columns:
            colmap["skuid"] = c
            break
    # æ‰¾ sku_spec
    for c in spec_candidates:
        if c in df.columns:
            colmap["sku_spec"] = c
            break

    missing = [name for name in ["item_id","skuid","sku_spec"] if name not in colmap]
    if missing:
        print("âš  è¾“å…¥Excelç¼ºå°‘å¿…è¦åˆ—:", missing)
        # æˆ‘ä»¬è¿˜æ˜¯å°½é‡è¿”å›ä¸€äº›åˆ—ï¼Œåé¢ä¼š dropna
    # å¤åˆ¶ä¸€ä»½åªä¿ç•™æˆ‘ä»¬å…³å¿ƒçš„åˆ—ï¼Œå¹¶é‡å‘½å
    df2 = df.copy()
    out = pd.DataFrame()
    out["item_id"] = df2[colmap["item_id"]] if "item_id" in colmap else pd.NA
    out["skuid"]   = df2[colmap["skuid"]]   if "skuid"   in colmap else pd.NA
    out["sku_spec"]= df2[colmap["sku_spec"]]if "sku_spec"in colmap else pd.NA

    return out


def _split_spec_value(spec_val: str) -> tuple[str|None, str|None]:
    """
    æŠŠ 'MWX0339OL71,M' è¿™ç§å€¼æ‹†æˆ ('MWX0339OL71','M')
    è¦æ±‚ï¼šç¬¬ä¸€ä¸ªé€—å·å‰ = å•†å“ç¼–ç 
          ç¬¬ä¸€ä¸ªé€—å·å = å°ºç 
    å¦‚æœæ²¡æœ‰é€—å·ï¼Œæˆ–ä¸ºç©ºï¼Œè¿”å› (None, None)
    """
    if not isinstance(spec_val, str):
        return (None, None)
    raw = spec_val.strip()
    if raw == "":
        return (None, None)
    parts = [p.strip() for p in raw.split(",")]
    if len(parts) < 2:
        # æ²¡æœ‰å°ºç ä¿¡æ¯ï¼Œè§†ä¸ºæ— æ•ˆ
        return (None, None)
    code_part = parts[0].strip().upper()
    size_part = parts[1].strip()
    if code_part == "" or size_part == "":
        return (None, None)
    return (code_part, size_part)





def _clean_spec_key(spec_val):
    if not isinstance(spec_val, str):
        return None
    v = spec_val.strip()
    return v if v else None


from datetime import datetime

# === è¿‡æ»¤åŸå› è®°å½•å™¨ ===
filtered_rows = []  # list[dict]

def _add_filtered(df_part, reason: str, extra: dict | None = None, limit: int | None = None):
    """
    df_part: è¢«è¿‡æ»¤æ‰çš„ DataFrameï¼ˆæˆ–ä¸€è¡Œè½¬æˆ dict ä¹Ÿè¡Œï¼‰
    reason: è¿‡æ»¤åŸå› 
    extra: é¢å¤–å­—æ®µ
    limit: é™åˆ¶è®°å½•æ¡æ•°ï¼Œé¿å…æç«¯æƒ…å†µä¸‹æ–‡ä»¶å¤ªå¤§ï¼ˆNone=ä¸é™åˆ¶ï¼‰
    """
    if df_part is None:
        return
    if isinstance(df_part, dict):
        row = dict(df_part)
        row["filter_reason"] = reason
        if extra: row.update(extra)
        filtered_rows.append(row)
        return

    if df_part.empty:
        return

    cols_keep = [c for c in ["item_id", "skuid", "sku_spec", "code_part", "size_part", "spec_key"] if c in df_part.columns]
    sub = df_part[cols_keep].copy()
    sub["filter_reason"] = reason
    if extra:
        for k, v in extra.items():
            sub[k] = v
    if limit is not None:
        sub = sub.head(limit)
    filtered_rows.extend(sub.to_dict("records"))






def _generate_price_excel_from_file(
    file_path: str,
    output_dir: str,
    brand: str,
    pgsql_config: dict,
    blacklist_excel_file: str | None,
    table_name: str = "barbour_inventory",
):
    """
    ç”Ÿæˆä»·æ ¼å¯¼å…¥è¡¨ï¼šå®è´id | skuid | è°ƒæ•´åä»·æ ¼
    é¢å¤–èƒ½åŠ›ï¼š
      - ç”¨ logger æ‰“å…³é”®ç»Ÿè®¡
      - è¿‡æ»¤æ‰çš„è¡Œå†™å…¥ output_dir/temp/*_filtered_reasons_*.xlsx æ–¹ä¾¿æ’æŸ¥
    """
    import pandas as pd
    import psycopg2
    from pathlib import Path

    logger = _get_logger(f"price_export.{brand}")
    logger.info(f"å¼€å§‹å¤„ç†: {file_path}")

    # æ¯ä¸ªæ–‡ä»¶é‡æ–°æ¸…ç©ºè¿‡æ»¤è®°å½•ï¼ˆä½ åŸæœ¬æ˜¯å…¨å±€åˆ—è¡¨ï¼‰
    global filtered_rows
    filtered_rows = []

    out_dir = Path(output_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    # 1) é»‘åå•
    blacklist_codes = _load_blacklist_codes(blacklist_excel_file)
    logger.info(f"é»‘åå•å¯ç”¨: {'æ˜¯' if blacklist_codes else 'å¦'} | count={len(blacklist_codes)}")

    # 2) è¯»è¾“å…¥ Excel + æ ‡å‡†åŒ–åˆ—
    df_raw = pd.read_excel(file_path, dtype=object)
    logger.info(f"åŸå§‹è¡Œæ•° df_raw: {len(df_raw)}")

    df_norm = _normalize_input_columns(df_raw)   # -> item_id, skuid, sku_spec
    logger.info(f"æ ‡å‡†åŒ–å df_norm: {len(df_norm)} | columns={list(df_norm.columns)}")

    # 2.1 å®è´id å‘ä¸‹å¡«å……ï¼ˆæ·˜å®å¯¼å‡ºå¸¸è§ç»“æ„ï¼‰
    df_norm["item_id"] = df_norm["item_id"].ffill()

    # 2.2 item_id å­—ç¬¦ä¸²åŒ–ï¼ˆé˜²æ­¢ç§‘å­¦è®¡æ•°ï¼‰
    def _item_id_to_str(val):
        if pd.isna(val):
            return ""
        if isinstance(val, (int, float)) and not isinstance(val, bool):
            return str(int(val))
        return str(val).strip()

    df_norm["item_id"] = df_norm["item_id"].map(_item_id_to_str)

    # 3) sku_spec -> code_part / size_part
    pairs = df_norm["sku_spec"].map(_split_spec_value)
    df_norm["code_part"] = [p[0] for p in pairs]
    df_norm["size_part"] = [normalize_size_for_taobao(p[1]) for p in pairs]

    # 3.1 drop æ‰ code/size ç¼ºå¤±è¡Œï¼ˆå¹¶è®°å½•åŸå› ï¼‰
    before = len(df_norm)
    dropped = df_norm[df_norm[["code_part", "size_part"]].isna().any(axis=1)].copy()
    _add_filtered(dropped, "dropna_code_or_size")
    df_norm = df_norm.dropna(subset=["code_part", "size_part"])
    logger.info(f"dropna(code/size): {before} -> {len(df_norm)}")

    # 4) é»‘åå•è¿‡æ»¤ï¼ˆæ•´æ¬¾è¿‡æ»¤ï¼‰
    if blacklist_codes:
        before = len(df_norm)
        blocked = df_norm[df_norm["code_part"].isin(blacklist_codes)].copy()
        _add_filtered(blocked, "blacklist_filtered")
        df_norm = df_norm[~df_norm["code_part"].isin(blacklist_codes)]
        logger.info(f"blacklistè¿‡æ»¤: {before} -> {len(df_norm)}")

    # 5) å±•å¼€ skuidï¼ˆå¯èƒ½æ˜¯ä¸€æ ¼å¤šä¸ªï¼‰
    rows = []
    empty_sku_cnt = 0

    for _, r in df_norm.iterrows():
        item_id = r["item_id"]
        code = r["code_part"]
        size = r["size_part"]
        skus_raw = r["skuid"]
        sku_list = _split_skuids(skus_raw)

        if not sku_list:
            empty_sku_cnt += 1
            _add_filtered(
                {
                    "item_id": item_id,
                    "skuid": skus_raw,
                    "sku_spec": r.get("sku_spec", None),
                    "code_part": code,
                    "size_part": size,
                },
                "empty_sku_list"
            )
            continue

        for sid in sku_list:
            if sid:
                rows.append({
                    "item_id": item_id,
                    "skuid": sid,
                    "code_part": code,
                    "size_part": size,
                })

    df_expanded = pd.DataFrame(rows, columns=["item_id", "skuid", "code_part", "size_part"])
    logger.info(f"å±•å¼€SKUå df_expanded: {len(df_expanded)} | empty_sku_rows={empty_sku_cnt}")

    if df_expanded.empty:
        logger.warning("å±•å¼€åæ— æœ‰æ•ˆSKUè¡Œï¼Œç»ˆæ­¢è¯¥æ–‡ä»¶ã€‚")
        _flush_filtered(out_dir, file_path, logger=logger)
        return

    # 6) æ‰¹é‡æŸ¥ä»·
    pairs_for_price = list(zip(df_expanded["code_part"], df_expanded["size_part"]))
    uniq_pairs = len(set(pairs_for_price))
    logger.info(f"å¾…æŸ¥ä»·ç»„åˆ: total={len(pairs_for_price)} unique={uniq_pairs} | table={table_name}")

    conn = psycopg2.connect(**pgsql_config)
    try:
        df_price = _fetch_prices_by_code_and_size_bulk(conn, table_name, pairs_for_price)
    finally:
        conn.close()

    logger.info(f"æ•°æ®åº“è¿”å› df_price è¡Œæ•°: {len(df_price)}")
    if df_price.empty:
        # å…¨éƒ½æ²¡ä»·æ ¼ï¼Œç›´æ¥è¾“å‡ºè¿‡æ»¤åŸå› 
        _add_filtered(df_expanded, "missing_db_price_all_empty_price_table")
        _flush_filtered(out_dir, file_path, logger=logger)
        logger.warning("df_price ä¸ºç©ºï¼šè¯¥æ–‡ä»¶æ— æ³•ç”Ÿæˆä»·æ ¼è¡¨ã€‚")
        return

    # 7) åˆå¹¶ä»·æ ¼
    df_merged = df_expanded.merge(
        df_price,
        left_on=["code_part", "size_part"],
        right_on=["product_code", "size"],
        how="left"
    )
    logger.info(f"åˆå¹¶å df_merged: {len(df_merged)}")

    # 7.1 è®°å½•ç¼ºä»·è¡Œ
    miss = df_merged[df_merged["taobao_store_price"].isna()].copy()
    _add_filtered(miss, "missing_db_price")

    df_merged = df_merged.dropna(subset=["taobao_store_price"])
    logger.info(f"ä¸¢æ‰æ— ä»·å df_merged: {len(df_merged)}")

    if df_merged.empty:
        _flush_filtered(out_dir, file_path, logger=logger)
        logger.warning("å…¨éƒ¨è¡Œéƒ½ç¼ºä»·ï¼Œæœªç”Ÿæˆä»·æ ¼Excelã€‚")
        return

    # 8) å¯¼å‡ºä¸‰åˆ—
    df_final = df_merged[["item_id", "skuid", "taobao_store_price"]].copy()
    df_final = df_final.rename(columns={
        "item_id": "å®è´id",
        "skuid": "skuid",
        "taobao_store_price": "è°ƒæ•´åä»·æ ¼",
    })

    # 9) å»é‡ skuid
    before = len(df_final)
    df_final = df_final.drop_duplicates(subset=["skuid"], keep="first")
    logger.info(f"å»é‡skuid: {before} -> {len(df_final)}")

    # 10) å®è´id å¼ºåˆ¶å­—ç¬¦ä¸²
    df_final["å®è´id"] = df_final["å®è´id"].astype(str)

    # 11) å†™ Excel
    src_name = Path(file_path).stem
    out_path = out_dir / f"{src_name}_price.xlsx"
    df_final.to_excel(out_path, index=False)

    logger.info(f"ç”Ÿæˆå®Œæˆ: {out_path} | rows={len(df_final)}")

    # 12) è¾“å‡ºè¿‡æ»¤åŸå› æ˜ç»†ï¼ˆæœ‰å°±å†™ï¼‰
    _flush_filtered(out_dir, file_path, logger=logger)











# ===== æ–°å¢ï¼šæ‰¹é‡å¯¼å‡º SKU åº“å­˜ =====
def generate_stock_excels_bulk(
    brand: str,
    input_dir: str | Path,
    output_dir: str | Path,
    suffix: str = "_åº“å­˜",
    in_stock_qty: int = 3,
    out_stock_qty: int = 0,
):
    """
    æ‰¹é‡æ ¹æ® input_dir ä¸‹çš„åº—é“ºå¯¼å‡ºè¡¨ç”Ÿæˆâ€œSKUID | è°ƒæ•´ååº“å­˜â€çš„ Excelã€‚
    è§„åˆ™ä¸ generate_price_excels_bulk ä¿æŒä¸€è‡´ï¼šä»…æŒ‰ product_code åˆå¹¶ï¼Œå…¨æ¬¾åŒå€¼ã€‚
    - æ•°æ®æ¥æºï¼š<TABLE_NAME> ä¸­çš„ taobao_store_price å­—æ®µï¼š
        * 'æœ‰è´§'  -> in_stock_qtyï¼ˆé»˜è®¤3ï¼‰
        * å…¶ä»–/ç©º -> out_stock_qtyï¼ˆé»˜è®¤0ï¼‰
    - è¾“å‡ºï¼š<è¾“å…¥æ–‡ä»¶å+suffix>.xlsxï¼Œåªæœ‰ä¸¤åˆ—ï¼šSKUID, è°ƒæ•´ååº“å­˜
    """
    import pandas as pd
    import psycopg2
    from pathlib import Path

    def _to_text(v):
        if v is None:
            return ""
        s = str(v).strip()
        return s

    def _list_excels(input_dir: Path):
        return (list(input_dir.glob("*.xlsx")) + list(input_dir.glob("*.xls")))

    def status_to_qty_from_price(val: str) -> int:
        # æŒ‰ä½ è¦æ±‚ï¼šå¦‚æœ taobao_store_price å­—æ®µæ–‡æœ¬ä¸ºâ€œæœ‰è´§â€ => 3ï¼Œå¦åˆ™ => 0
        #ï¼ˆæ³¨æ„ï¼šå¦‚æœè¯¥å­—æ®µåœ¨æŸäº›å“ç‰Œæ˜¯â€œæ•°å­—ä»·æ ¼â€ï¼Œæ­¤æ˜ å°„ä¼šå¤±çœŸï¼Œéœ€è¦æ”¹å› stock_statusï¼‰
        s = _to_text(val)
        return in_stock_qty if s == "æœ‰è´§" else out_stock_qty

    brand = brand.lower().strip()
    if brand not in BRAND_CONFIG:
        raise ValueError(f"æœªçŸ¥å“ç‰Œï¼š{brand}")
    cfg   = BRAND_CONFIG[brand]
    table = cfg["TABLE_NAME"]
    pg    = cfg["PGSQL_CONFIG"]

    input_dir  = Path(input_dir)
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # 1) æ‹‰å–â€œå¯å”®çŠ¶æ€â€å¹¶åšæ˜ å°„ï¼ˆä»…æŒ‰ product_codeï¼‰
    conn = psycopg2.connect(**pg)
    try:
        df_flag = pd.read_sql(f'SELECT product_code, taobao_store_price FROM {table}', conn)
    finally:
        conn.close()

    if "product_code" not in df_flag.columns:
        raise RuntimeError(f"{table} ç¼ºå°‘ product_code åˆ—")

    df_flag["product_code"] = df_flag["product_code"].astype(str).str.strip()
    # ç»Ÿä¸€æ˜ å°„ä¸ºæ•°é‡
    df_flag["è°ƒæ•´ååº“å­˜"] = df_flag["taobao_store_price"].map(status_to_qty_from_price)
    # æŒ‰æ¬¾èšåˆï¼ˆè‹¥åŒä¸€æ¬¾å¤šè¡Œï¼Œå–æœ€å¤§â€”â€”ç­‰ä»·â€œåªè¦æœ‰ä¸€è¡Œæœ‰è´§åˆ™æœ‰è´§â€ï¼‰
    qty_by_code = (
        df_flag.groupby("product_code")["è°ƒæ•´ååº“å­˜"].max()
               .reset_index()
    )

    # 2) æ‰«æè¾“å…¥ç›®å½•
    files = _list_excels(input_dir)
    if not files:
        raise FileNotFoundError(f"ç›®å½•æ²¡æœ‰æ‰¾åˆ° Excelï¼š{input_dir}")

    results = []
    for f in sorted(files, key=lambda p: p.stat().st_mtime, reverse=True):
        try:
            df0 = pd.read_excel(f, dtype=object)

            # å¤ç”¨ä½ ç°æœ‰çš„åˆ—ååˆ«åè§£æ
            col_item = _normalize_col(df0, "item_id")        # å®è´id
            col_code = _normalize_col(df0, "product_code")   # å•†å®¶ç¼–ç 
            col_sku  = _normalize_col(df0, "skuid")          # skuID

            # å±•å¼€ SKU è¡Œï¼ˆä¸ä»·æ ¼å¯¼å‡ºä¸€è‡´ï¼šåªæŒ‰ product_code åˆå¹¶ï¼Œä¸çœ‹å°ºç ï¼‰
            rows = []
            for _, r in df0.iterrows():
                item_id = _to_text(r.get(col_item))
                code    = _to_text(r.get(col_code))
                skus    = _split_skuids(r.get(col_sku))
                for sid in skus:
                    sid = _to_text(sid)
                    if sid:
                        rows.append((item_id, code, sid))

            if not rows:
                raise ValueError(f"{f.name} æ— æœ‰æ•ˆ SKU è®°å½•ï¼ˆæ£€æŸ¥å®è´ID/å•†å®¶ç¼–ç /skuIDï¼‰ã€‚")

            df_expanded = pd.DataFrame(rows, columns=["å®è´id", "product_code", "skuid"])
            df_expanded["product_code"] = df_expanded["product_code"].astype(str).str.strip()

            # ä»…æŒ‰ product_code åˆå¹¶åº“å­˜æ•°é‡ï¼ˆä¸ä»·æ ¼å¯¼å‡ºç›¸åŒåˆå¹¶ç²’åº¦ï¼‰
            df_tmp = df_expanded.merge(qty_by_code, on="product_code", how="left")
            df_tmp["è°ƒæ•´ååº“å­˜"] = df_tmp["è°ƒæ•´ååº“å­˜"].fillna(out_stock_qty).astype(int)

            # è¾“å‡ºä¸¤åˆ—
            out_df = df_tmp[["skuid", "è°ƒæ•´ååº“å­˜"]]
            out_name = f.stem + (suffix or "")
            if not out_name.endswith(".xlsx"):
                out_name += ".xlsx"
            out_path = output_dir / out_name
            out_df.to_excel(out_path, index=False)

            print(f"âœ… {f.name} -> {out_name} (rows={len(out_df)})")
            results.append((str(f), str(out_path), len(out_df), None))
        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥ï¼š{f} | é”™è¯¯ï¼š{e}")
            results.append((str(f), None, 0, str(e)))

    return results



if __name__ == "__main__":
    # ç”¨æ³•ä¸€ï¼šå•æ–‡ä»¶ï¼ˆä¿æŒå…¼å®¹ï¼‰
    #   python this_script.py <brand> <input_dir> <output_excel_path>
    # ç”¨æ³•äºŒï¼šæ‰¹é‡ï¼ˆæ¨èä½ ç°åœ¨çš„åœºæ™¯ï¼‰
    #   python this_script.py <brand> <input_dir> <output_dir> --bulk [--suffix "_ä»·æ ¼"]
    import sys, traceback
    try:
        if len(sys.argv) >= 5 and sys.argv[4] == "--bulk":
            brand = sys.argv[1]
            input_dir = sys.argv[2]
            output_dir = sys.argv[3]
            suffix = "_ä»·æ ¼"
            if len(sys.argv) >= 7 and sys.argv[5] == "--suffix":
                suffix = sys.argv[6]
            results = generate_price_excels_bulk(brand, input_dir, output_dir, suffix=suffix, drop_rows_without_price=True)
            ok = [r for r in results if r[1] is not None]
            bad = [r for r in results if r[1] is None]
            print(f"ğŸ“¦ æ‰¹é‡å®Œæˆï¼šæˆåŠŸ {len(ok)} ä¸ªï¼Œå¤±è´¥ {len(bad)} ä¸ª")
            if bad:
                print("å¤±è´¥æ¸…å•ï¼š")
                for f, _, err in bad:
                    print(f" - {f} -> é”™è¯¯ï¼š{err}")
        elif len(sys.argv) >= 4:
            generate_price_excel(sys.argv[1], sys.argv[2], sys.argv[3])
        else:
            print("ç”¨æ³•ï¼š")
            print("  å•æ–‡ä»¶ï¼špython this_script.py <brand> <input_dir> <output_excel_path>")
            print('  æ‰¹  é‡ï¼špython this_script.py <brand> <input_dir> <output_dir> --bulk [--suffix "_ä»·æ ¼"]')
    except Exception as e:
        print("âŒ å¤±è´¥ï¼š", e)
        traceback.print_exc()
